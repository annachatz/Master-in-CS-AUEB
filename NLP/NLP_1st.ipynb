{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h07hFvGuxHID"
      },
      "source": [
        "# $n$-Gram language models\n",
        "\n",
        "**Natural Language Processing 2024-2025**\n",
        "\n",
        "|   | Team  Members             |          |                                                          |\n",
        "|---|---------------------------|----------|----------------------------------------------------------|\n",
        "| 1 |   Anna Chatzipapadopoulou | f3322411 | ann.chatzipapadopoul@aueb.gr / annachatzipap@gmail.com   |                   \n",
        "| 2 |   Alviona Mancho          | f3322405 | alv.mantso@aueb.gr  / alviona.mantso@gmail.com           |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDp9KoMTxHIH"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhCNYZazxHII",
        "outputId": "fbea307e-ed6c-443c-8c68-6b76e883f202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'Levenshtein'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8a248a8e5148>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTweetTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'brown'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mLevenshtein\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Levenshtein'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import nltk\n",
        "import evaluate\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import brown\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "nltk.download('brown')\n",
        "from Levenshtein import distance\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyMbR15FxHIK"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0vv--68xHIK"
      },
      "outputs": [],
      "source": [
        "class Tools:\n",
        "    @staticmethod\n",
        "    def format_sequence(sequence):\n",
        "        # Remove all occurrences of '*start*' from the beginning of the sequence\n",
        "        while sequence and sequence[0] == '*start*':\n",
        "            sequence.pop(0)\n",
        "        return ' '.join(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26cMG1fixHIK"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TextProcessor:\n",
        "    def __init__(self):\n",
        "        TextProcessor.tokenizer = TweetTokenizer()\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize_corpus(corpus):\n",
        "        \"\"\"\n",
        "        Tokenizes the corpus by splitting it into sentences, and then tokenizing each sentence into words.\n",
        "        \"\"\"\n",
        "        # Split the corpus into sentences\n",
        "        sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', corpus)\n",
        "\n",
        "        # Split each sentence into words\n",
        "        tokenized_sentences = [TextProcessor.tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "        return tokenized_sentences\n",
        "\n",
        "    @staticmethod\n",
        "    def create_vocabulary(tokenized_corpus, occurrences_threshold=10):\n",
        "        \"\"\"\n",
        "        Creates a vocabulary from the tokenized corpus by counting word occurrences\n",
        "        and selecting words that meet or exceed the occurrences threshold.\n",
        "        \"\"\"\n",
        "        # tokenized_corpus = TextProcessor._remove_punct(tokenized_corpus)\n",
        "        word_counts = Counter(word for sentence in tokenized_corpus for word in sentence)\n",
        "        vocabulary = {word for word, count in word_counts.items() if count >= occurrences_threshold}\n",
        "\n",
        "        return list(vocabulary)\n",
        "\n",
        "    @staticmethod\n",
        "    def process_tokenized_corpus(tokenized_corpus, vocabulary):\n",
        "        \"\"\"\n",
        "        Processes the tokenized corpus by replacing words not in the vocabulary with '*UNK*'.\n",
        "        \"\"\"\n",
        "        # tokenized_corpus = TextProcessor._remove_punct(tokenized_corpus)\n",
        "        def filter_sentence(sentence):\n",
        "            return [word if word in vocabulary else '*UNK*' for word in sentence]\n",
        "\n",
        "        # Apply the filter to each sentence in the tokenized corpus\n",
        "        processed_corpus = [filter_sentence(sentence) for sentence in tokenized_corpus]\n",
        "\n",
        "        return processed_corpus\n",
        "\n",
        "    @staticmethod\n",
        "    def introduce_noise(word, mapping, replace_prob = 0.1, delete_prob = 0.04):\n",
        "        \"\"\"Introduces random noise to a given word by selectively replacing or deleting characters\n",
        "        based on specified probabilities.\"\"\"\n",
        "        noisy_word = [\n",
        "            random.choice(mapping[char.lower()]).upper() if char.isupper() else random.choice(mapping[char.lower()])\n",
        "            if char.lower() in mapping and random.random() < replace_prob\n",
        "            else char\n",
        "            for char in word if random.random() >= delete_prob\n",
        "        ]\n",
        "        return ''.join(noisy_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0l4IHYaxHIL"
      },
      "source": [
        "## $n$-gram Language Model (LM) Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfZONHgVxHIL"
      },
      "source": [
        "* #### Probabilities  (`calculate_ngram_prob`)\n",
        "  $P_{bigram}(w_2|w_1) = \\frac{C(w_1,w_2) + \\alpha}{C(w_1) + \\alpha \\cdot|V|} $\n",
        "\n",
        "  $P_{trigram}(w_3|w_1,w_2) = \\frac{C(w_1,w_2,w_3) + \\alpha}{C(w_1,w_2) + \\alpha \\cdot |V|}$\n",
        "\n",
        "  $P_{inter}(w_3|w_1,w_2) = \\lambda \\cdot P(w_3|w_1,w_2) +(1-\\lambda) \\cdot P(w_3|w_2) $\n",
        "\n",
        "  where\n",
        "\n",
        "  $ C(w_1) $ : unigram count (`unigram_counter`)\n",
        "\n",
        "  $ C(w_1,w_2) $ : bigram count (`bigram_counter`)\n",
        "\n",
        "  $ C(w_1,w_2,w_3) $ : trigram count (`trigram_counter`)\n",
        "\n",
        "  $ 0 \\leq\\alpha \\leq1 $ :  smoothing hyper-parameter (`alpha`)\n",
        "\n",
        "  $ 0 \\leq\\lambda \\leq1 $ (`lamda`)\n",
        "  |$V$|: vocabulary size (`vocabulary_size`)\n",
        "\n",
        "* #### Autocompletion\n",
        "    1. Selecting the most probable next word (`ngram_auto_complete`)\n",
        "    2. Using beam search (`beam_search_autocomplete`)\n",
        "\n",
        "\n",
        "* #### Spelling Correction (`beam_search_spelling_corrector`)\n",
        "  $\\hat{t}_1^k = \\arg\\max_{t_1^k} \\; \\lambda_1 \\cdot log_2 P(t_1^k) + \\lambda_2 \\cdot log_2 P(w_1^k | t_1^k)$\n",
        "  \n",
        "  where\n",
        "\n",
        "  $w_1^k$ : the observed sequence (noisy sentence)\n",
        "\n",
        "  $ 0 \\leq\\lambda_1, \\lambda_2 \\leq1 $ (`lambda1`, `lambda2`)\n",
        "\n",
        "  $ P(w_1^k \\mid t_1^k) \\approx \\prod_{i=1}^k P(w_i \\mid t_i) $ is calculated using the inverse Levenshtein distance, where $ w_1^k $ is the observed sequence (noisy sentence)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwMwQeaFxHIM"
      },
      "outputs": [],
      "source": [
        "class Ngram:\n",
        "    def __init__(self, vocabulary, alpha = 0.01, lamda=0.9,lambda1=0.3,lambda2 = 1.0):\n",
        "        self.alpha = alpha\n",
        "        self.lamda = lamda\n",
        "        self.lambda1 = lambda1\n",
        "        self.lambda2 = lambda2\n",
        "        self.vocabulary = vocabulary\n",
        "        self.vocabulary_size = len(vocabulary)\n",
        "        self.unigram_counter, self.bigram_counter, self.trigram_counter = None, None, None\n",
        "\n",
        "\n",
        "    def train_ngram(self, tokenized_corpus):\n",
        "        unigram_counter = Counter()\n",
        "        bigram_counter = Counter()\n",
        "        trigram_counter = Counter()\n",
        "\n",
        "        for sent in tokenized_corpus:\n",
        "            unigram_counter.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
        "            bigram_counter.update([gram for gram in ngrams(sent, 2, pad_left=True, pad_right=True,\n",
        "                                                        left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
        "            trigram_counter.update([gram for gram in ngrams(sent, 3, pad_left=True, pad_right=True,\n",
        "                                                        left_pad_symbol='*start*',right_pad_symbol='*end*') ])\n",
        "\n",
        "        self.unigram_counter = unigram_counter\n",
        "        self.bigram_counter = bigram_counter\n",
        "        self.trigram_counter = trigram_counter\n",
        "\n",
        "    def fine_tune(self,tokenized_sentences,alphas = np.linspace(0.001,1.0,50),lambdas = np.linspace(0.0,1.0,11)):\n",
        "        best_alpha, best_lambda = None, None\n",
        "        best_perplexity = float('inf')\n",
        "        for alpha in alphas:\n",
        "            for lamb in lambdas:\n",
        "                self.alpha = alpha\n",
        "                self.lamda = lamb\n",
        "                HC, perplexity, _ = self.sequence_ngram_prob(tokenized_sentences)\n",
        "                print(f\"  Alpha: {alpha:.4f}, Lambda: {lamb:.4f}, Cross Entropy: {HC:.4f}, Perplexity: {perplexity:.4f}\")\n",
        "\n",
        "                if perplexity < best_perplexity:\n",
        "                    best_perplexity = perplexity\n",
        "                    best_alpha = alpha\n",
        "                    best_lambda = lamb\n",
        "\n",
        "        print(\"-\" * 45)\n",
        "        print(f\"Optimal Alpha: {best_alpha:.4f}, Optimal Lambda: {best_lambda:.4f}, Best Perplexity: {best_perplexity:.4f}\")\n",
        "        print(\"-\" * 45)\n",
        "        self.alpha = best_alpha\n",
        "        self.lamda = best_lambda\n",
        "        return best_alpha,best_lambda, best_perplexity\n",
        "\n",
        "    def fine_tune_lambdas(self,noisy_sentences,original_sentences,n,beam_width=2,lambda1_values = np.linspace(0.0,1.0,11),lambda2_values = np.linspace(0.0,1.0,11)):\n",
        "        best_lambda1, best_lambda2 = None, None\n",
        "        best_wer, best_cer = float('inf'), float('inf')\n",
        "\n",
        "        for lambda1 in lambda1_values:\n",
        "            for lambda2 in lambda2_values:\n",
        "                corrected_corpus = []\n",
        "                self.lambda1 = lambda1\n",
        "                self.lambda2 = lambda2\n",
        "                for noisy_sentence in noisy_sentences:\n",
        "                    corrected_sentence = self.beam_search_spelling_corrector(noisy_sentence,n,beam_width)\n",
        "                    corrected_corpus.append(corrected_sentence)\n",
        "                wer,cer = self.evaluate_spelling_correction(corrected_corpus,original_sentences)\n",
        "                print(f\"  Lambda1: {lambda1:.4f}, Lambda2: {lambda2:.4f}, WER: {wer:.4f}, CER: {cer:.4f}\")\n",
        "                if wer < best_wer or (wer == best_wer and cer < best_cer):\n",
        "                    best_wer = wer\n",
        "                    best_cer = cer\n",
        "                    best_lambda1 = lambda1\n",
        "                    best_lambda2 = lambda2\n",
        "\n",
        "        self.lambda1 = best_lambda1\n",
        "        self.lambda2 = best_lambda2\n",
        "\n",
        "        print(\"-\" * 45)\n",
        "        print(f\"Optimal Lambda1: {best_lambda1:.4f}, Optimal Lambda2: {best_lambda2:.4f}, Best Avg WER: {best_wer:.4f}, Best Avg CER: {best_cer:.4f}\")\n",
        "        print(\"-\" * 45)\n",
        "\n",
        "        return best_lambda1, best_lambda2, best_wer,best_cer\n",
        "\n",
        "\n",
        "    def calculate_ngram_prob(self, tokens, n):\n",
        "        \"\"\"\n",
        "        Calculates the smoothed probability of an n-gram using Laplace smoothing\n",
        "        \"\"\"\n",
        "        if len(tokens) < n:\n",
        "            raise ValueError(f\"{n}-gram model requires at least {n-1} words as context.\")\n",
        "\n",
        "        counters = {1: self.unigram_counter, 2: self.bigram_counter, 3: self.trigram_counter}\n",
        "\n",
        "        # Calculate numerator and denominator\n",
        "        counter_num = counters[n][tuple(tokens[:n])]\n",
        "        counter_denom = counters[n-1][tuple(tokens[:n-1])]\n",
        "\n",
        "        # Return smoothed probability\n",
        "        return (counter_num + self.alpha) / (counter_denom + self.alpha * self.vocabulary_size)\n",
        "\n",
        "\n",
        "    def sequence_ngram_prob(self, sentences_tokenized, n=None):\n",
        "        \"\"\"\n",
        "        Calculates the probability and perplexity of a sequence of tokenized sentences using n-gram models.\n",
        "        Interpolates between bigram and trigram models if n is not specified, using lambda as the interpolation weight.\n",
        "        \"\"\"\n",
        "        if n == None:\n",
        "            bigram_prob_coeff = 1- self.lamda\n",
        "            trigram_prob_coeff = self.lamda\n",
        "        elif n == 2:\n",
        "            bigram_prob_coeff = 1\n",
        "            trigram_prob_coeff = 0\n",
        "        elif n == 3:\n",
        "            bigram_prob_coeff = 0\n",
        "            trigram_prob_coeff = 1\n",
        "        else:\n",
        "            raise ValueError(\"Only bigrams and trigrams and their interpolation are supported\")\n",
        "\n",
        "        sum_prob = 0\n",
        "        ngram_cnt = 0\n",
        "\n",
        "        n = 3\n",
        "        start_tokens = (n-1)*['*start*']\n",
        "        end_tokens = ['*end*']\n",
        "\n",
        "        for sent in sentences_tokenized:\n",
        "            sent = start_tokens + sent + end_tokens\n",
        "\n",
        "            for i in range(len(sent)-n+1):\n",
        "                tokens = sent[i:i + n]\n",
        "                trigram_prob = self.calculate_ngram_prob(tokens, n=3)\n",
        "                bigram_prob = self.calculate_ngram_prob(tokens[-2:], n=2)\n",
        "\n",
        "                sum_prob += math.log2(trigram_prob_coeff * trigram_prob + bigram_prob_coeff * bigram_prob)\n",
        "                ngram_cnt += 1\n",
        "\n",
        "        HC = -sum_prob / ngram_cnt\n",
        "        perpl = math.pow(2, HC)\n",
        "        return HC, perpl, sum_prob\n",
        "\n",
        "\n",
        "    def ngram_auto_complete(self, start_sentence, n, max_length=20):\n",
        "        \"\"\"\n",
        "        Autocompletes a given start sentence using an n-gram model up to a specified maximum length.\n",
        "        The model selects each subsequent word by choosing the one with the highest probability.\n",
        "        \"\"\"\n",
        "        sequence = start_sentence[:]\n",
        "        if len(sequence) < n - 1:\n",
        "            raise ValueError(f\"{n}-gram model requires at least {n - 1} words as context.\")\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            tokens = sequence[-(n-1):]\n",
        "            most_probable = max(self.vocabulary + ['*end*'], key=lambda word: self.calculate_ngram_prob(tokens+[word], n=n))\n",
        "            sequence.append(most_probable)\n",
        "\n",
        "            if most_probable == '*end*':\n",
        "                break\n",
        "\n",
        "        return sequence\n",
        "\n",
        "    def beam_search_autocomplete(self, initial_state, n, max_depth = 10, beam_width = 2):\n",
        "        \"\"\"\n",
        "        Autocompletes a sentence using beam search with n-gram scoring.\n",
        "        Expands each sequence by selecting the top `beam_width` candidates with the highest cumulative probability.\n",
        "        \"\"\"\n",
        "        candidates = [(initial_state, 0.0)]\n",
        "        if n != 2 and n != 3:\n",
        "            raise ValueError(\"Only bigrams and trigrams are supported\")\n",
        "\n",
        "        for _ in range(max_depth):\n",
        "            new_candidates = []\n",
        "            for candidate, prob in candidates:\n",
        "                if candidate[-1] == '*end*':\n",
        "                    new_candidates.append((candidate, prob))\n",
        "                    continue\n",
        "                for next_state in self._generate_candidates(candidate):\n",
        "                    new_prob = prob + math.log2(self._score_ngram(next_state,n))\n",
        "                    new_candidates.append((next_state, new_prob))\n",
        "\n",
        "            new_candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
        "            candidates = new_candidates[:beam_width]\n",
        "\n",
        "        best_sequence, best_prob = max(candidates, key=lambda x: x[1])\n",
        "        return best_sequence\n",
        "\n",
        "\n",
        "    def beam_search_spelling_corrector(self, w_sentence, n, beam_width=2):\n",
        "        \"\"\"\n",
        "        Performs beam search to find the most probable correction for a noisy sentence.\n",
        "        Combines n-gram probabilities and inverse Levenshtein distances.\n",
        "        \"\"\"\n",
        "        candidates = [(['*start*']*(n-1), 0.0)]\n",
        "        if n != 2 and n != 3:\n",
        "            raise ValueError(\"Only bigrams and trigrams are supported\")\n",
        "\n",
        "        for i in range(len(w_sentence)):\n",
        "            new_candidates = []\n",
        "            word = w_sentence[i]  # Current noisy word in sentence\n",
        "            spelling_candidates = [(word_cand, self.calculate_inverse_levenshtein(word, word_cand))\n",
        "                for word_cand in self.vocabulary]\n",
        "\n",
        "            for candidate, prob in candidates:\n",
        "                for word_cand, p_w_t in spelling_candidates:\n",
        "                    next_state = candidate + [word_cand]\n",
        "                    p_t = self.calculate_ngram_prob(candidate[-(n-1):] + [word_cand], n=n)\n",
        "                    new_prob = prob + self.lambda1*math.log2(p_t) + self.lambda2*math.log2(p_w_t)\n",
        "                    new_candidates.append((next_state, new_prob))\n",
        "\n",
        "            new_candidates = sorted(new_candidates, key=lambda x: x[1], reverse=True)\n",
        "            candidates = new_candidates[:beam_width]\n",
        "        best_sequence, best_prob = max(candidates, key=lambda x: x[1])\n",
        "        return best_sequence[n-1:]\n",
        "\n",
        "\n",
        "    def _generate_candidates(self, state):\n",
        "        return [state + [next_word] for next_word in self.vocabulary + ['*end*']]\n",
        "\n",
        "    def _score_ngram(self, state, n):\n",
        "        prob = 1\n",
        "        for i in range(n - 1, len(state)):\n",
        "            ngram = state[i - (n - 1): i + 1]\n",
        "            prob *= self.calculate_ngram_prob(ngram, n=n)\n",
        "        return prob\n",
        "\n",
        "    # Setters\n",
        "    def set_lambda1(self,value):\n",
        "        self.lambda1 = value\n",
        "\n",
        "    def set_lambda2(self,value):\n",
        "        self.lambda2 = value\n",
        "\n",
        "    def set_alpha(self,value):\n",
        "        self.alpha = value\n",
        "\n",
        "    def set_lamda(self,value):\n",
        "        self.lamda = value\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_inverse_levenshtein(w, t):\n",
        "        weights = (1, 1, 2) # Costs insert: 1, delete: 1, replace: 2\n",
        "        return 1 / (distance(w, t, weights=weights) + 1)\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate_spelling_correction(corrected_corpus,original_corpus):\n",
        "        wer_metric = evaluate.load(\"wer\")\n",
        "        cer_metric = evaluate.load(\"cer\")\n",
        "\n",
        "        predictions = [' '.join(sentence) for sentence in corrected_corpus]\n",
        "        references = [' '.join(sentence) for sentence in original_corpus]\n",
        "\n",
        "        wer_score = wer_metric.compute(predictions=predictions,references=references)\n",
        "        cer_score = cer_metric.compute(predictions=predictions,references=references)\n",
        "\n",
        "        return wer_score,cer_score\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate_ngram_model(ngram_model, corpus, dataset_name, n=None):\n",
        "        if n == None:\n",
        "            print(f\"Evaluating Interpolation Model on {dataset_name} Set:\")\n",
        "            HC, perplexity, _ = ngram_model.sequence_ngram_prob(corpus)\n",
        "            print(f\"  Cross Entropy: {HC:.4f}\")\n",
        "            print(f\"  Perplexity: {perplexity:.4f}\")\n",
        "            print(\"-\" * 45)\n",
        "        else:\n",
        "            print(f\"Evaluating {n}-Gram Model on {dataset_name} Set:\")\n",
        "            HC, perplexity, _ = ngram_model.sequence_ngram_prob(corpus, n=n)\n",
        "            print(f\"  Cross Entropy: {HC:.4f}\")\n",
        "            print(f\"  Perplexity: {perplexity:.4f}\")\n",
        "            print(\"-\" * 45)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bA_32uExHIN"
      },
      "source": [
        "## Fetch and Prepocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH6_41gUxHIN"
      },
      "source": [
        "From the diverse categories available in the Brown Corpus, we select the 'romance' category. This dataset will be divided into a 80% training set, along with 10% for testing and 10% for validation (development)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-yvYzTBxHIN",
        "outputId": "7951159c-ec9f-41a7-bb03-f26c12a2af8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "> Categories available in the Brown Corpus:\n",
            " adventure\n",
            " belles_lettres\n",
            " editorial\n",
            " fiction\n",
            " government\n",
            " hobbies\n",
            " humor\n",
            " learned\n",
            " lore\n",
            " mystery\n",
            " news\n",
            " religion\n",
            " reviews\n",
            " romance\n",
            " science_fiction\n"
          ]
        }
      ],
      "source": [
        "print('> Categories available in the Brown Corpus:\\n', '\\n '.join(brown.categories()))\n",
        "tokenized_corpus = brown.sents(categories='romance')\n",
        "\n",
        "train_corpus, temp_corpus = train_test_split(tokenized_corpus, test_size=0.2, random_state=40)\n",
        "dev_corpus, test_corpus = train_test_split(temp_corpus, test_size=0.5, random_state=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImF9FFmVxHIN"
      },
      "source": [
        "Generate a vocabulary from the training corpus and preprocess the training, development, and test corpora using the created vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsdcdvVyxHIO"
      },
      "outputs": [],
      "source": [
        "textProcessor = TextProcessor()\n",
        "# tokenized_corpus = TextProcessor.tokenize_corpus(corpus)\n",
        "\n",
        "vocabulary = textProcessor.create_vocabulary(train_corpus)\n",
        "processed_train_corpus = textProcessor.process_tokenized_corpus(train_corpus,vocabulary)\n",
        "processed_dev_corpus = textProcessor.process_tokenized_corpus(dev_corpus,vocabulary)\n",
        "processed_test_corpus = textProcessor.process_tokenized_corpus(test_corpus,vocabulary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zu1JWtSxHIO"
      },
      "source": [
        "## Training and Evaluation of the bi-gram and tri-gram LM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxYw51lQxHIO"
      },
      "source": [
        "* $ \\text{CrossEntropy}: \\quad HC_{bigrams} = -\\frac{1}{N}\\sum_{bigrams}{log_2(P(w_2|w_1))} $, where $N$ is the number of bigrams\n",
        "* $ \\text{Perplexity}: \\quad P = 2^{HC_{bigrams}} $\n",
        "\n",
        "---\n",
        "\n",
        "* $ \\text{CrossEntropy}: \\quad HC_{trigrams} = -\\frac{1}{N}\\sum_{trigrams}{log_2(P(w_3|w_1, w_2))} $, where $N$ is the number of trigrams\n",
        "* $ \\text{Perplexity}: \\quad P = 2^{HC_{trigrams}} $\n",
        "\n",
        "---\n",
        "\n",
        "* $ \\text{CrossEntropy}: \\quad HC_{inter} = -\\frac{1}{N}\\sum_{trigrams}{log_2(\\lambda \\cdot P(w_3|w_1,w_2) +(1-\\lambda) \\cdot P(w_3|w_2))} $, where $N$ is the number of trigrams\n",
        "* $ \\text{Perplexity}: \\quad P = 2^{HC_{inter}} $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1LBiOYcxHIO",
        "outputId": "b6d09676-573d-467b-a4c3-74496f9a89d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Ngram Model with alpha = 0.01 and lambda = 0.9\n",
            "============================================================\n",
            "Evaluating 2-Gram Model on Train Set:\n",
            "  Cross Entropy: 3.6767\n",
            "  Perplexity: 12.7881\n",
            "---------------------------------------------\n",
            "Evaluating 3-Gram Model on Train Set:\n",
            "  Cross Entropy: 2.8925\n",
            "  Perplexity: 7.4256\n",
            "---------------------------------------------\n",
            "Evaluating Interpolation Model on Train Set:\n",
            "  Cross Entropy: 2.9136\n",
            "  Perplexity: 7.5349\n",
            "---------------------------------------------\n",
            "Evaluating 2-Gram Model on Dev Set:\n",
            "  Cross Entropy: 4.4258\n",
            "  Perplexity: 21.4937\n",
            "---------------------------------------------\n",
            "Evaluating 3-Gram Model on Dev Set:\n",
            "  Cross Entropy: 5.2970\n",
            "  Perplexity: 39.3135\n",
            "---------------------------------------------\n",
            "Evaluating Interpolation Model on Dev Set:\n",
            "  Cross Entropy: 4.7158\n",
            "  Perplexity: 26.2785\n",
            "---------------------------------------------\n",
            "Evaluating 2-Gram Model on Test Set:\n",
            "  Cross Entropy: 4.5772\n",
            "  Perplexity: 23.8713\n",
            "---------------------------------------------\n",
            "Evaluating 3-Gram Model on Test Set:\n",
            "  Cross Entropy: 5.5637\n",
            "  Perplexity: 47.2977\n",
            "---------------------------------------------\n",
            "Evaluating Interpolation Model on Test Set:\n",
            "  Cross Entropy: 4.9277\n",
            "  Perplexity: 30.4355\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "ngram_model = Ngram(vocabulary)\n",
        "\n",
        "# Train\n",
        "ngram_model.train_ngram(processed_train_corpus)\n",
        "\n",
        "print(f\"Evaluating Ngram Model with alpha = {ngram_model.alpha} and lambda = {ngram_model.lamda}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Evaluate train dev and test sets for bigram, trigram, and interpolated ngram\n",
        "for dataset_name, corpus in [(\"Train\", processed_train_corpus),(\"Dev\", processed_dev_corpus), (\"Test\", processed_test_corpus)]:\n",
        "    for n in [2, 3]:\n",
        "        Ngram.evaluate_ngram_model(ngram_model, corpus, dataset_name, n)\n",
        "    Ngram.evaluate_ngram_model(ngram_model, corpus, dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpsW3UWnxHIO"
      },
      "source": [
        "### Fine-Tuning the $n$-gram Model\n",
        "\n",
        "In this section, we fine-tune the **`alpha`** and **`lambda`** hyperparameters to optimize our Ngram language model. The goal is to find values for these parameters that minimize **perplexity** on the development set.\n",
        "\n",
        "- **Alpha (α)**: The smoothing parameter for Laplace smoothing, which adjusts for unseen words in the N-gram model.\n",
        "- **Lambda (λ)**: The interpolation weight for combining bigram and trigram models. Higher `lambda` values favor the trigram model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O5u_qlsxHIO",
        "outputId": "a94d3d8f-0190-4e32-807d-8c2067a9267b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Alpha: 0.0010, Lambda: 0.0000, Cross Entropy: 4.5054, Perplexity: 22.7123\n",
            "  Alpha: 0.0010, Lambda: 0.1000, Cross Entropy: 4.3073, Perplexity: 19.7979\n",
            "  Alpha: 0.0010, Lambda: 0.2000, Cross Entropy: 4.2481, Perplexity: 19.0018\n",
            "  Alpha: 0.0010, Lambda: 0.3000, Cross Entropy: 4.2228, Perplexity: 18.6718\n",
            "  Alpha: 0.0010, Lambda: 0.4000, Cross Entropy: 4.2193, Perplexity: 18.6266\n",
            "  Alpha: 0.0010, Lambda: 0.5000, Cross Entropy: 4.2347, Perplexity: 18.8263\n",
            "  Alpha: 0.0010, Lambda: 0.6000, Cross Entropy: 4.2702, Perplexity: 19.2963\n",
            "  Alpha: 0.0010, Lambda: 0.7000, Cross Entropy: 4.3318, Perplexity: 20.1370\n",
            "  Alpha: 0.0010, Lambda: 0.8000, Cross Entropy: 4.4342, Perplexity: 21.6193\n",
            "  Alpha: 0.0010, Lambda: 0.9000, Cross Entropy: 4.6250, Perplexity: 24.6753\n",
            "  Alpha: 0.0010, Lambda: 1.0000, Cross Entropy: 5.5364, Perplexity: 46.4124\n",
            "  Alpha: 0.0214, Lambda: 0.0000, Cross Entropy: 4.4422, Perplexity: 21.7392\n",
            "  Alpha: 0.0214, Lambda: 0.1000, Cross Entropy: 4.3848, Perplexity: 20.8913\n",
            "  Alpha: 0.0214, Lambda: 0.2000, Cross Entropy: 4.3710, Perplexity: 20.6920\n",
            "  Alpha: 0.0214, Lambda: 0.3000, Cross Entropy: 4.3759, Perplexity: 20.7630\n",
            "  Alpha: 0.0214, Lambda: 0.4000, Cross Entropy: 4.3956, Perplexity: 21.0473\n",
            "  Alpha: 0.0214, Lambda: 0.5000, Cross Entropy: 4.4295, Perplexity: 21.5485\n",
            "  Alpha: 0.0214, Lambda: 0.6000, Cross Entropy: 4.4798, Perplexity: 22.3134\n",
            "  Alpha: 0.0214, Lambda: 0.7000, Cross Entropy: 4.5517, Perplexity: 23.4529\n",
            "  Alpha: 0.0214, Lambda: 0.8000, Cross Entropy: 4.6569, Perplexity: 25.2266\n",
            "  Alpha: 0.0214, Lambda: 0.9000, Cross Entropy: 4.8290, Perplexity: 28.4233\n",
            "  Alpha: 0.0214, Lambda: 1.0000, Cross Entropy: 5.3498, Perplexity: 40.7807\n",
            "  Alpha: 0.0418, Lambda: 0.0000, Cross Entropy: 4.4914, Perplexity: 22.4924\n",
            "  Alpha: 0.0418, Lambda: 0.1000, Cross Entropy: 4.4606, Perplexity: 22.0171\n",
            "  Alpha: 0.0418, Lambda: 0.2000, Cross Entropy: 4.4610, Perplexity: 22.0243\n",
            "  Alpha: 0.0418, Lambda: 0.3000, Cross Entropy: 4.4772, Perplexity: 22.2725\n",
            "  Alpha: 0.0418, Lambda: 0.4000, Cross Entropy: 4.5065, Perplexity: 22.7299\n",
            "  Alpha: 0.0418, Lambda: 0.5000, Cross Entropy: 4.5493, Perplexity: 23.4141\n",
            "  Alpha: 0.0418, Lambda: 0.6000, Cross Entropy: 4.6079, Perplexity: 24.3845\n",
            "  Alpha: 0.0418, Lambda: 0.7000, Cross Entropy: 4.6875, Perplexity: 25.7681\n",
            "  Alpha: 0.0418, Lambda: 0.8000, Cross Entropy: 4.7996, Perplexity: 27.8505\n",
            "  Alpha: 0.0418, Lambda: 0.9000, Cross Entropy: 4.9758, Perplexity: 31.4674\n",
            "  Alpha: 0.0418, Lambda: 1.0000, Cross Entropy: 5.4598, Perplexity: 44.0123\n",
            "  Alpha: 0.0622, Lambda: 0.0000, Cross Entropy: 4.5399, Perplexity: 23.2625\n",
            "  Alpha: 0.0622, Lambda: 0.1000, Cross Entropy: 4.5231, Perplexity: 22.9928\n",
            "  Alpha: 0.0622, Lambda: 0.2000, Cross Entropy: 4.5319, Perplexity: 23.1338\n",
            "  Alpha: 0.0622, Lambda: 0.3000, Cross Entropy: 4.5549, Perplexity: 23.5053\n",
            "  Alpha: 0.0622, Lambda: 0.4000, Cross Entropy: 4.5903, Perplexity: 24.0891\n",
            "  Alpha: 0.0622, Lambda: 0.5000, Cross Entropy: 4.6387, Perplexity: 24.9117\n",
            "  Alpha: 0.0622, Lambda: 0.6000, Cross Entropy: 4.7028, Perplexity: 26.0419\n",
            "  Alpha: 0.0622, Lambda: 0.7000, Cross Entropy: 4.7877, Perplexity: 27.6203\n",
            "  Alpha: 0.0622, Lambda: 0.8000, Cross Entropy: 4.9048, Perplexity: 29.9567\n",
            "  Alpha: 0.0622, Lambda: 0.9000, Cross Entropy: 5.0848, Perplexity: 33.9382\n",
            "  Alpha: 0.0622, Lambda: 1.0000, Cross Entropy: 5.5533, Perplexity: 46.9572\n",
            "  Alpha: 0.0826, Lambda: 0.0000, Cross Entropy: 4.5846, Perplexity: 23.9932\n",
            "  Alpha: 0.0826, Lambda: 0.1000, Cross Entropy: 4.5768, Perplexity: 23.8654\n",
            "  Alpha: 0.0826, Lambda: 0.2000, Cross Entropy: 4.5914, Perplexity: 24.1081\n",
            "  Alpha: 0.0826, Lambda: 0.3000, Cross Entropy: 4.6192, Perplexity: 24.5772\n",
            "  Alpha: 0.0826, Lambda: 0.4000, Cross Entropy: 4.6590, Perplexity: 25.2637\n",
            "  Alpha: 0.0826, Lambda: 0.5000, Cross Entropy: 4.7115, Perplexity: 26.2007\n",
            "  Alpha: 0.0826, Lambda: 0.6000, Cross Entropy: 4.7795, Perplexity: 27.4651\n",
            "  Alpha: 0.0826, Lambda: 0.7000, Cross Entropy: 4.8684, Perplexity: 29.2093\n",
            "  Alpha: 0.0826, Lambda: 0.8000, Cross Entropy: 4.9893, Perplexity: 31.7645\n",
            "  Alpha: 0.0826, Lambda: 0.9000, Cross Entropy: 5.1726, Perplexity: 36.0672\n",
            "  Alpha: 0.0826, Lambda: 1.0000, Cross Entropy: 5.6325, Perplexity: 49.6063\n",
            "  Alpha: 0.1029, Lambda: 0.0000, Cross Entropy: 4.6253, Perplexity: 24.6812\n",
            "  Alpha: 0.1029, Lambda: 0.1000, Cross Entropy: 4.6242, Perplexity: 24.6622\n",
            "  Alpha: 0.1029, Lambda: 0.2000, Cross Entropy: 4.6432, Perplexity: 24.9883\n",
            "  Alpha: 0.1029, Lambda: 0.3000, Cross Entropy: 4.6747, Perplexity: 25.5396\n",
            "  Alpha: 0.1029, Lambda: 0.4000, Cross Entropy: 4.7178, Perplexity: 26.3141\n",
            "  Alpha: 0.1029, Lambda: 0.5000, Cross Entropy: 4.7735, Perplexity: 27.3504\n",
            "  Alpha: 0.1029, Lambda: 0.6000, Cross Entropy: 4.8446, Perplexity: 28.7322\n",
            "  Alpha: 0.1029, Lambda: 0.7000, Cross Entropy: 4.9365, Perplexity: 30.6223\n",
            "  Alpha: 0.1029, Lambda: 0.8000, Cross Entropy: 5.0606, Perplexity: 33.3719\n",
            "  Alpha: 0.1029, Lambda: 0.9000, Cross Entropy: 5.2465, Perplexity: 37.9633\n",
            "  Alpha: 0.1029, Lambda: 1.0000, Cross Entropy: 5.7010, Perplexity: 52.0217\n",
            "  Alpha: 0.1233, Lambda: 0.0000, Cross Entropy: 4.6628, Perplexity: 25.3308\n",
            "  Alpha: 0.1233, Lambda: 0.1000, Cross Entropy: 4.6668, Perplexity: 25.4003\n",
            "  Alpha: 0.1233, Lambda: 0.2000, Cross Entropy: 4.6892, Perplexity: 25.7979\n",
            "  Alpha: 0.1233, Lambda: 0.3000, Cross Entropy: 4.7236, Perplexity: 26.4211\n",
            "  Alpha: 0.1233, Lambda: 0.4000, Cross Entropy: 4.7694, Perplexity: 27.2733\n",
            "  Alpha: 0.1233, Lambda: 0.5000, Cross Entropy: 4.8277, Perplexity: 28.3982\n",
            "  Alpha: 0.1233, Lambda: 0.6000, Cross Entropy: 4.9014, Perplexity: 29.8853\n",
            "  Alpha: 0.1233, Lambda: 0.7000, Cross Entropy: 4.9958, Perplexity: 31.9070\n",
            "  Alpha: 0.1233, Lambda: 0.8000, Cross Entropy: 5.1224, Perplexity: 34.8326\n",
            "  Alpha: 0.1233, Lambda: 0.9000, Cross Entropy: 5.3106, Perplexity: 39.6876\n",
            "  Alpha: 0.1233, Lambda: 1.0000, Cross Entropy: 5.7616, Perplexity: 54.2519\n",
            "  Alpha: 0.1437, Lambda: 0.0000, Cross Entropy: 4.6975, Perplexity: 25.9467\n",
            "  Alpha: 0.1437, Lambda: 0.1000, Cross Entropy: 4.7055, Perplexity: 26.0913\n",
            "  Alpha: 0.1437, Lambda: 0.2000, Cross Entropy: 4.7307, Perplexity: 26.5519\n",
            "  Alpha: 0.1437, Lambda: 0.3000, Cross Entropy: 4.7676, Perplexity: 27.2394\n",
            "  Alpha: 0.1437, Lambda: 0.4000, Cross Entropy: 4.8157, Perplexity: 28.1620\n",
            "  Alpha: 0.1437, Lambda: 0.5000, Cross Entropy: 4.8761, Perplexity: 29.3673\n",
            "  Alpha: 0.1437, Lambda: 0.6000, Cross Entropy: 4.9519, Perplexity: 30.9505\n",
            "  Alpha: 0.1437, Lambda: 0.7000, Cross Entropy: 5.0484, Perplexity: 33.0927\n",
            "  Alpha: 0.1437, Lambda: 0.8000, Cross Entropy: 5.1771, Perplexity: 36.1800\n",
            "  Alpha: 0.1437, Lambda: 0.9000, Cross Entropy: 5.3673, Perplexity: 41.2785\n",
            "  Alpha: 0.1437, Lambda: 1.0000, Cross Entropy: 5.8159, Perplexity: 56.3321\n",
            "  Alpha: 0.1641, Lambda: 0.0000, Cross Entropy: 4.7297, Perplexity: 26.5333\n",
            "  Alpha: 0.1641, Lambda: 0.1000, Cross Entropy: 4.7411, Perplexity: 26.7433\n",
            "  Alpha: 0.1641, Lambda: 0.2000, Cross Entropy: 4.7687, Perplexity: 27.2606\n",
            "  Alpha: 0.1641, Lambda: 0.3000, Cross Entropy: 4.8077, Perplexity: 28.0067\n",
            "  Alpha: 0.1641, Lambda: 0.4000, Cross Entropy: 4.8577, Perplexity: 28.9937\n",
            "  Alpha: 0.1641, Lambda: 0.5000, Cross Entropy: 4.9200, Perplexity: 30.2732\n",
            "  Alpha: 0.1641, Lambda: 0.6000, Cross Entropy: 4.9975, Perplexity: 31.9451\n",
            "  Alpha: 0.1641, Lambda: 0.7000, Cross Entropy: 5.0959, Perplexity: 34.1990\n",
            "  Alpha: 0.1641, Lambda: 0.8000, Cross Entropy: 5.2264, Perplexity: 37.4364\n",
            "  Alpha: 0.1641, Lambda: 0.9000, Cross Entropy: 5.4183, Perplexity: 42.7619\n",
            "  Alpha: 0.1641, Lambda: 1.0000, Cross Entropy: 5.8651, Perplexity: 58.2876\n",
            "  Alpha: 0.1845, Lambda: 0.0000, Cross Entropy: 4.7599, Perplexity: 27.0941\n",
            "  Alpha: 0.1845, Lambda: 0.1000, Cross Entropy: 4.7741, Perplexity: 27.3624\n",
            "  Alpha: 0.1845, Lambda: 0.2000, Cross Entropy: 4.8038, Perplexity: 27.9314\n",
            "  Alpha: 0.1845, Lambda: 0.3000, Cross Entropy: 4.8446, Perplexity: 28.7315\n",
            "  Alpha: 0.1845, Lambda: 0.4000, Cross Entropy: 4.8962, Perplexity: 29.7783\n",
            "  Alpha: 0.1845, Lambda: 0.5000, Cross Entropy: 4.9601, Perplexity: 31.1267\n",
            "  Alpha: 0.1845, Lambda: 0.6000, Cross Entropy: 5.0392, Perplexity: 32.8815\n",
            "  Alpha: 0.1845, Lambda: 0.7000, Cross Entropy: 5.1391, Perplexity: 35.2397\n",
            "  Alpha: 0.1845, Lambda: 0.8000, Cross Entropy: 5.2712, Perplexity: 38.6176\n",
            "  Alpha: 0.1845, Lambda: 0.9000, Cross Entropy: 5.4646, Perplexity: 44.1564\n",
            "  Alpha: 0.1845, Lambda: 1.0000, Cross Entropy: 5.9102, Perplexity: 60.1376\n",
            "  Alpha: 0.2049, Lambda: 0.0000, Cross Entropy: 4.7883, Perplexity: 27.6321\n",
            "  Alpha: 0.2049, Lambda: 0.1000, Cross Entropy: 4.8049, Perplexity: 27.9531\n",
            "  Alpha: 0.2049, Lambda: 0.2000, Cross Entropy: 4.8364, Perplexity: 28.5699\n",
            "  Alpha: 0.2049, Lambda: 0.3000, Cross Entropy: 4.8787, Perplexity: 29.4203\n",
            "  Alpha: 0.2049, Lambda: 0.4000, Cross Entropy: 4.9318, Perplexity: 30.5230\n",
            "  Alpha: 0.2049, Lambda: 0.5000, Cross Entropy: 4.9971, Perplexity: 31.9361\n",
            "  Alpha: 0.2049, Lambda: 0.6000, Cross Entropy: 5.0776, Perplexity: 33.7687\n",
            "  Alpha: 0.2049, Lambda: 0.7000, Cross Entropy: 5.1789, Perplexity: 36.2250\n",
            "  Alpha: 0.2049, Lambda: 0.8000, Cross Entropy: 5.3124, Perplexity: 39.7354\n",
            "  Alpha: 0.2049, Lambda: 0.9000, Cross Entropy: 5.5070, Perplexity: 45.4757\n",
            "  Alpha: 0.2049, Lambda: 1.0000, Cross Entropy: 5.9518, Perplexity: 61.8968\n",
            "  Alpha: 0.2253, Lambda: 0.0000, Cross Entropy: 4.8151, Perplexity: 28.1500\n",
            "  Alpha: 0.2253, Lambda: 0.1000, Cross Entropy: 4.8339, Perplexity: 28.5191\n",
            "  Alpha: 0.2253, Lambda: 0.2000, Cross Entropy: 4.8669, Perplexity: 29.1804\n",
            "  Alpha: 0.2253, Lambda: 0.3000, Cross Entropy: 4.9106, Perplexity: 30.0780\n",
            "  Alpha: 0.2253, Lambda: 0.4000, Cross Entropy: 4.9650, Perplexity: 31.2333\n",
            "  Alpha: 0.2253, Lambda: 0.5000, Cross Entropy: 5.0315, Perplexity: 32.7075\n",
            "  Alpha: 0.2253, Lambda: 0.6000, Cross Entropy: 5.1133, Perplexity: 34.6136\n",
            "  Alpha: 0.2253, Lambda: 0.7000, Cross Entropy: 5.2158, Perplexity: 37.1629\n",
            "  Alpha: 0.2253, Lambda: 0.8000, Cross Entropy: 5.3505, Perplexity: 40.7987\n",
            "  Alpha: 0.2253, Lambda: 0.9000, Cross Entropy: 5.5463, Perplexity: 46.7302\n",
            "  Alpha: 0.2253, Lambda: 1.0000, Cross Entropy: 5.9904, Perplexity: 63.5768\n",
            "  Alpha: 0.2457, Lambda: 0.0000, Cross Entropy: 4.8404, Perplexity: 28.6497\n",
            "  Alpha: 0.2457, Lambda: 0.1000, Cross Entropy: 4.8611, Perplexity: 29.0633\n",
            "  Alpha: 0.2457, Lambda: 0.2000, Cross Entropy: 4.8956, Perplexity: 29.7664\n",
            "  Alpha: 0.2457, Lambda: 0.3000, Cross Entropy: 4.9406, Perplexity: 30.7085\n",
            "  Alpha: 0.2457, Lambda: 0.4000, Cross Entropy: 4.9961, Perplexity: 31.9136\n",
            "  Alpha: 0.2457, Lambda: 0.5000, Cross Entropy: 5.0637, Perplexity: 33.4457\n",
            "  Alpha: 0.2457, Lambda: 0.6000, Cross Entropy: 5.1466, Perplexity: 35.4217\n",
            "  Alpha: 0.2457, Lambda: 0.7000, Cross Entropy: 5.2502, Perplexity: 38.0593\n",
            "  Alpha: 0.2457, Lambda: 0.8000, Cross Entropy: 5.3859, Perplexity: 41.8146\n",
            "  Alpha: 0.2457, Lambda: 0.9000, Cross Entropy: 5.5828, Perplexity: 47.9284\n",
            "  Alpha: 0.2457, Lambda: 1.0000, Cross Entropy: 6.0265, Perplexity: 65.1869\n",
            "  Alpha: 0.2660, Lambda: 0.0000, Cross Entropy: 4.8646, Perplexity: 29.1330\n",
            "  Alpha: 0.2660, Lambda: 0.1000, Cross Entropy: 4.8869, Perplexity: 29.5880\n",
            "  Alpha: 0.2660, Lambda: 0.2000, Cross Entropy: 4.9227, Perplexity: 30.3305\n",
            "  Alpha: 0.2660, Lambda: 0.3000, Cross Entropy: 4.9688, Perplexity: 31.3149\n",
            "  Alpha: 0.2660, Lambda: 0.4000, Cross Entropy: 5.0254, Perplexity: 32.5674\n",
            "  Alpha: 0.2660, Lambda: 0.5000, Cross Entropy: 5.0940, Perplexity: 34.1546\n",
            "  Alpha: 0.2660, Lambda: 0.6000, Cross Entropy: 5.1778, Perplexity: 36.1973\n",
            "  Alpha: 0.2660, Lambda: 0.7000, Cross Entropy: 5.2824, Perplexity: 38.9192\n",
            "  Alpha: 0.2660, Lambda: 0.8000, Cross Entropy: 5.4192, Perplexity: 42.7886\n",
            "  Alpha: 0.2660, Lambda: 0.9000, Cross Entropy: 5.6170, Perplexity: 49.0768\n",
            "  Alpha: 0.2660, Lambda: 1.0000, Cross Entropy: 6.0604, Perplexity: 66.7347\n",
            "  Alpha: 0.2864, Lambda: 0.0000, Cross Entropy: 4.8876, Perplexity: 29.6014\n",
            "  Alpha: 0.2864, Lambda: 0.1000, Cross Entropy: 4.9115, Perplexity: 30.0953\n",
            "  Alpha: 0.2864, Lambda: 0.2000, Cross Entropy: 4.9484, Perplexity: 30.8752\n",
            "  Alpha: 0.2864, Lambda: 0.3000, Cross Entropy: 4.9955, Perplexity: 31.8998\n",
            "  Alpha: 0.2864, Lambda: 0.4000, Cross Entropy: 5.0530, Perplexity: 33.1976\n",
            "  Alpha: 0.2864, Lambda: 0.5000, Cross Entropy: 5.1226, Perplexity: 34.8376\n",
            "  Alpha: 0.2864, Lambda: 0.6000, Cross Entropy: 5.2073, Perplexity: 36.9440\n",
            "  Alpha: 0.2864, Lambda: 0.7000, Cross Entropy: 5.3128, Perplexity: 39.7467\n",
            "  Alpha: 0.2864, Lambda: 0.8000, Cross Entropy: 5.4504, Perplexity: 43.7253\n",
            "  Alpha: 0.2864, Lambda: 0.9000, Cross Entropy: 5.6491, Perplexity: 50.1808\n",
            "  Alpha: 0.2864, Lambda: 1.0000, Cross Entropy: 6.0923, Perplexity: 68.2265\n",
            "  Alpha: 0.3068, Lambda: 0.0000, Cross Entropy: 4.9096, Perplexity: 30.0562\n",
            "  Alpha: 0.3068, Lambda: 0.1000, Cross Entropy: 4.9348, Perplexity: 30.5868\n",
            "  Alpha: 0.3068, Lambda: 0.2000, Cross Entropy: 4.9728, Perplexity: 31.4023\n",
            "  Alpha: 0.3068, Lambda: 0.3000, Cross Entropy: 5.0208, Perplexity: 32.4653\n",
            "  Alpha: 0.3068, Lambda: 0.4000, Cross Entropy: 5.0792, Perplexity: 33.8065\n",
            "  Alpha: 0.3068, Lambda: 0.5000, Cross Entropy: 5.1496, Perplexity: 35.4971\n",
            "  Alpha: 0.3068, Lambda: 0.6000, Cross Entropy: 5.2351, Perplexity: 37.6648\n",
            "  Alpha: 0.3068, Lambda: 0.7000, Cross Entropy: 5.3415, Perplexity: 40.5450\n",
            "  Alpha: 0.3068, Lambda: 0.8000, Cross Entropy: 5.4799, Perplexity: 44.6287\n",
            "  Alpha: 0.3068, Lambda: 0.9000, Cross Entropy: 5.6793, Perplexity: 51.2451\n",
            "  Alpha: 0.3068, Lambda: 1.0000, Cross Entropy: 6.1224, Perplexity: 69.6676\n",
            "  Alpha: 0.3272, Lambda: 0.0000, Cross Entropy: 4.9307, Perplexity: 30.4985\n",
            "  Alpha: 0.3272, Lambda: 0.1000, Cross Entropy: 4.9572, Perplexity: 31.0638\n",
            "  Alpha: 0.3272, Lambda: 0.2000, Cross Entropy: 4.9961, Perplexity: 31.9134\n",
            "  Alpha: 0.3272, Lambda: 0.3000, Cross Entropy: 5.0450, Perplexity: 33.0133\n",
            "  Alpha: 0.3272, Lambda: 0.4000, Cross Entropy: 5.1042, Perplexity: 34.3961\n",
            "  Alpha: 0.3272, Lambda: 0.5000, Cross Entropy: 5.1753, Perplexity: 36.1354\n",
            "  Alpha: 0.3272, Lambda: 0.6000, Cross Entropy: 5.2616, Perplexity: 38.3621\n",
            "  Alpha: 0.3272, Lambda: 0.7000, Cross Entropy: 5.3687, Perplexity: 41.3170\n",
            "  Alpha: 0.3272, Lambda: 0.8000, Cross Entropy: 5.5079, Perplexity: 45.5018\n",
            "  Alpha: 0.3272, Lambda: 0.9000, Cross Entropy: 5.7080, Perplexity: 52.2734\n",
            "  Alpha: 0.3272, Lambda: 1.0000, Cross Entropy: 6.1510, Perplexity: 71.0625\n",
            "  Alpha: 0.3476, Lambda: 0.0000, Cross Entropy: 4.9509, Perplexity: 30.9294\n",
            "  Alpha: 0.3476, Lambda: 0.1000, Cross Entropy: 4.9785, Perplexity: 31.5276\n",
            "  Alpha: 0.3476, Lambda: 0.2000, Cross Entropy: 5.0184, Perplexity: 32.4098\n",
            "  Alpha: 0.3476, Lambda: 0.3000, Cross Entropy: 5.0680, Perplexity: 33.5452\n",
            "  Alpha: 0.3476, Lambda: 0.4000, Cross Entropy: 5.1280, Perplexity: 34.9682\n",
            "  Alpha: 0.3476, Lambda: 0.5000, Cross Entropy: 5.1998, Perplexity: 36.7544\n",
            "  Alpha: 0.3476, Lambda: 0.6000, Cross Entropy: 5.2868, Perplexity: 39.0380\n",
            "  Alpha: 0.3476, Lambda: 0.7000, Cross Entropy: 5.3945, Perplexity: 42.0649\n",
            "  Alpha: 0.3476, Lambda: 0.8000, Cross Entropy: 5.5344, Perplexity: 46.3475\n",
            "  Alpha: 0.3476, Lambda: 0.9000, Cross Entropy: 5.7352, Perplexity: 53.2690\n",
            "  Alpha: 0.3476, Lambda: 1.0000, Cross Entropy: 6.1782, Perplexity: 72.4151\n",
            "  Alpha: 0.3680, Lambda: 0.0000, Cross Entropy: 4.9704, Perplexity: 31.3495\n",
            "  Alpha: 0.3680, Lambda: 0.1000, Cross Entropy: 4.9991, Perplexity: 31.9793\n",
            "  Alpha: 0.3680, Lambda: 0.2000, Cross Entropy: 5.0397, Perplexity: 32.8929\n",
            "  Alpha: 0.3680, Lambda: 0.3000, Cross Entropy: 5.0901, Perplexity: 34.0624\n",
            "  Alpha: 0.3680, Lambda: 0.4000, Cross Entropy: 5.1507, Perplexity: 35.5242\n",
            "  Alpha: 0.3680, Lambda: 0.5000, Cross Entropy: 5.2233, Perplexity: 37.3558\n",
            "  Alpha: 0.3680, Lambda: 0.6000, Cross Entropy: 5.3109, Perplexity: 39.6943\n",
            "  Alpha: 0.3680, Lambda: 0.7000, Cross Entropy: 5.4192, Perplexity: 42.7909\n",
            "  Alpha: 0.3680, Lambda: 0.8000, Cross Entropy: 5.5597, Perplexity: 47.1680\n",
            "  Alpha: 0.3680, Lambda: 0.9000, Cross Entropy: 5.7611, Perplexity: 54.2345\n",
            "  Alpha: 0.3680, Lambda: 1.0000, Cross Entropy: 6.2042, Perplexity: 73.7287\n",
            "  Alpha: 0.3884, Lambda: 0.0000, Cross Entropy: 4.9891, Perplexity: 31.7598\n",
            "  Alpha: 0.3884, Lambda: 0.1000, Cross Entropy: 5.0188, Perplexity: 32.4197\n",
            "  Alpha: 0.3884, Lambda: 0.2000, Cross Entropy: 5.0602, Perplexity: 33.3635\n",
            "  Alpha: 0.3884, Lambda: 0.3000, Cross Entropy: 5.1113, Perplexity: 34.5661\n",
            "  Alpha: 0.3884, Lambda: 0.4000, Cross Entropy: 5.1725, Perplexity: 36.0654\n",
            "  Alpha: 0.3884, Lambda: 0.5000, Cross Entropy: 5.2457, Perplexity: 37.9409\n",
            "  Alpha: 0.3884, Lambda: 0.6000, Cross Entropy: 5.3339, Perplexity: 40.3326\n",
            "  Alpha: 0.3884, Lambda: 0.7000, Cross Entropy: 5.4428, Perplexity: 43.4968\n",
            "  Alpha: 0.3884, Lambda: 0.8000, Cross Entropy: 5.5839, Perplexity: 47.9653\n",
            "  Alpha: 0.3884, Lambda: 0.9000, Cross Entropy: 5.7859, Perplexity: 55.1725\n",
            "  Alpha: 0.3884, Lambda: 1.0000, Cross Entropy: 6.2289, Perplexity: 75.0062\n",
            "  Alpha: 0.4088, Lambda: 0.0000, Cross Entropy: 5.0072, Perplexity: 32.1608\n",
            "  Alpha: 0.4088, Lambda: 0.1000, Cross Entropy: 5.0378, Perplexity: 32.8497\n",
            "  Alpha: 0.4088, Lambda: 0.2000, Cross Entropy: 5.0799, Perplexity: 33.8227\n",
            "  Alpha: 0.4088, Lambda: 0.3000, Cross Entropy: 5.1316, Perplexity: 35.0573\n",
            "  Alpha: 0.4088, Lambda: 0.4000, Cross Entropy: 5.1935, Perplexity: 36.5929\n",
            "  Alpha: 0.4088, Lambda: 0.5000, Cross Entropy: 5.2672, Perplexity: 38.5109\n",
            "  Alpha: 0.4088, Lambda: 0.6000, Cross Entropy: 5.3559, Perplexity: 40.9543\n",
            "  Alpha: 0.4088, Lambda: 0.7000, Cross Entropy: 5.4655, Perplexity: 44.1840\n",
            "  Alpha: 0.4088, Lambda: 0.8000, Cross Entropy: 5.6071, Perplexity: 48.7413\n",
            "  Alpha: 0.4088, Lambda: 0.9000, Cross Entropy: 5.8095, Perplexity: 56.0850\n",
            "  Alpha: 0.4088, Lambda: 1.0000, Cross Entropy: 6.2527, Perplexity: 76.2503\n",
            "  Alpha: 0.4291, Lambda: 0.0000, Cross Entropy: 5.0247, Perplexity: 32.5531\n",
            "  Alpha: 0.4291, Lambda: 0.1000, Cross Entropy: 5.0561, Perplexity: 33.2699\n",
            "  Alpha: 0.4291, Lambda: 0.2000, Cross Entropy: 5.0989, Perplexity: 34.2712\n",
            "  Alpha: 0.4291, Lambda: 0.3000, Cross Entropy: 5.1512, Perplexity: 35.5368\n",
            "  Alpha: 0.4291, Lambda: 0.4000, Cross Entropy: 5.2136, Perplexity: 37.1076\n",
            "  Alpha: 0.4291, Lambda: 0.5000, Cross Entropy: 5.2879, Perplexity: 39.0670\n",
            "  Alpha: 0.4291, Lambda: 0.6000, Cross Entropy: 5.3771, Perplexity: 41.5606\n",
            "  Alpha: 0.4291, Lambda: 0.7000, Cross Entropy: 5.4872, Perplexity: 44.8538\n",
            "  Alpha: 0.4291, Lambda: 0.8000, Cross Entropy: 5.6293, Perplexity: 49.4975\n",
            "  Alpha: 0.4291, Lambda: 0.9000, Cross Entropy: 5.8322, Perplexity: 56.9739\n",
            "  Alpha: 0.4291, Lambda: 1.0000, Cross Entropy: 6.2754, Perplexity: 77.4631\n",
            "  Alpha: 0.4495, Lambda: 0.0000, Cross Entropy: 5.0417, Perplexity: 32.9373\n",
            "  Alpha: 0.4495, Lambda: 0.1000, Cross Entropy: 5.0739, Perplexity: 33.6811\n",
            "  Alpha: 0.4495, Lambda: 0.2000, Cross Entropy: 5.1173, Perplexity: 34.7097\n",
            "  Alpha: 0.4495, Lambda: 0.3000, Cross Entropy: 5.1701, Perplexity: 36.0054\n",
            "  Alpha: 0.4495, Lambda: 0.4000, Cross Entropy: 5.2331, Perplexity: 37.6105\n",
            "  Alpha: 0.4495, Lambda: 0.5000, Cross Entropy: 5.3078, Perplexity: 39.6101\n",
            "  Alpha: 0.4495, Lambda: 0.6000, Cross Entropy: 5.3975, Perplexity: 42.1524\n",
            "  Alpha: 0.4495, Lambda: 0.7000, Cross Entropy: 5.5080, Perplexity: 45.5076\n",
            "  Alpha: 0.4495, Lambda: 0.8000, Cross Entropy: 5.6506, Perplexity: 50.2352\n",
            "  Alpha: 0.4495, Lambda: 0.9000, Cross Entropy: 5.8540, Perplexity: 57.8407\n",
            "  Alpha: 0.4495, Lambda: 1.0000, Cross Entropy: 6.2973, Perplexity: 78.6468\n",
            "  Alpha: 0.4699, Lambda: 0.0000, Cross Entropy: 5.0581, Perplexity: 33.3139\n",
            "  Alpha: 0.4699, Lambda: 0.1000, Cross Entropy: 5.0910, Perplexity: 34.0837\n",
            "  Alpha: 0.4699, Lambda: 0.2000, Cross Entropy: 5.1350, Perplexity: 35.1389\n",
            "  Alpha: 0.4699, Lambda: 0.3000, Cross Entropy: 5.1884, Perplexity: 36.4638\n",
            "  Alpha: 0.4699, Lambda: 0.4000, Cross Entropy: 5.2518, Perplexity: 38.1023\n",
            "  Alpha: 0.4699, Lambda: 0.5000, Cross Entropy: 5.3270, Perplexity: 40.1410\n",
            "  Alpha: 0.4699, Lambda: 0.6000, Cross Entropy: 5.4172, Perplexity: 42.7309\n",
            "  Alpha: 0.4699, Lambda: 0.7000, Cross Entropy: 5.5281, Perplexity: 46.1463\n",
            "  Alpha: 0.4699, Lambda: 0.8000, Cross Entropy: 5.6712, Perplexity: 50.9557\n",
            "  Alpha: 0.4699, Lambda: 0.9000, Cross Entropy: 5.8750, Perplexity: 58.6870\n",
            "  Alpha: 0.4699, Lambda: 1.0000, Cross Entropy: 6.3184, Perplexity: 79.8031\n",
            "  Alpha: 0.4903, Lambda: 0.0000, Cross Entropy: 5.0740, Perplexity: 33.6833\n",
            "  Alpha: 0.4903, Lambda: 0.1000, Cross Entropy: 5.1076, Perplexity: 34.4782\n",
            "  Alpha: 0.4903, Lambda: 0.2000, Cross Entropy: 5.1522, Perplexity: 35.5593\n",
            "  Alpha: 0.4903, Lambda: 0.3000, Cross Entropy: 5.2060, Perplexity: 36.9127\n",
            "  Alpha: 0.4903, Lambda: 0.4000, Cross Entropy: 5.2699, Perplexity: 38.5837\n",
            "  Alpha: 0.4903, Lambda: 0.5000, Cross Entropy: 5.3456, Perplexity: 40.6606\n",
            "  Alpha: 0.4903, Lambda: 0.6000, Cross Entropy: 5.4362, Perplexity: 43.2967\n",
            "  Alpha: 0.4903, Lambda: 0.7000, Cross Entropy: 5.5475, Perplexity: 46.7710\n",
            "  Alpha: 0.4903, Lambda: 0.8000, Cross Entropy: 5.6910, Perplexity: 51.6600\n",
            "  Alpha: 0.4903, Lambda: 0.9000, Cross Entropy: 5.8952, Perplexity: 59.5141\n",
            "  Alpha: 0.4903, Lambda: 1.0000, Cross Entropy: 6.3387, Perplexity: 80.9337\n",
            "  Alpha: 0.5107, Lambda: 0.0000, Cross Entropy: 5.0894, Perplexity: 34.0459\n",
            "  Alpha: 0.5107, Lambda: 0.1000, Cross Entropy: 5.1237, Perplexity: 34.8652\n",
            "  Alpha: 0.5107, Lambda: 0.2000, Cross Entropy: 5.1688, Perplexity: 35.9714\n",
            "  Alpha: 0.5107, Lambda: 0.3000, Cross Entropy: 5.2231, Perplexity: 37.3526\n",
            "  Alpha: 0.5107, Lambda: 0.4000, Cross Entropy: 5.2874, Perplexity: 39.0553\n",
            "  Alpha: 0.5107, Lambda: 0.5000, Cross Entropy: 5.3635, Perplexity: 41.1694\n",
            "  Alpha: 0.5107, Lambda: 0.6000, Cross Entropy: 5.4545, Perplexity: 43.8508\n",
            "  Alpha: 0.5107, Lambda: 0.7000, Cross Entropy: 5.5663, Perplexity: 47.3824\n",
            "  Alpha: 0.5107, Lambda: 0.8000, Cross Entropy: 5.7101, Perplexity: 52.3492\n",
            "  Alpha: 0.5107, Lambda: 0.9000, Cross Entropy: 5.9146, Perplexity: 60.3230\n",
            "  Alpha: 0.5107, Lambda: 1.0000, Cross Entropy: 6.3583, Perplexity: 82.0400\n",
            "  Alpha: 0.5311, Lambda: 0.0000, Cross Entropy: 5.1044, Perplexity: 34.4021\n",
            "  Alpha: 0.5311, Lambda: 0.1000, Cross Entropy: 5.1394, Perplexity: 35.2451\n",
            "  Alpha: 0.5311, Lambda: 0.2000, Cross Entropy: 5.1849, Perplexity: 36.3758\n",
            "  Alpha: 0.5311, Lambda: 0.3000, Cross Entropy: 5.2397, Perplexity: 37.7841\n",
            "  Alpha: 0.5311, Lambda: 0.4000, Cross Entropy: 5.3044, Perplexity: 39.5177\n",
            "  Alpha: 0.5311, Lambda: 0.5000, Cross Entropy: 5.3809, Perplexity: 41.6681\n",
            "  Alpha: 0.5311, Lambda: 0.6000, Cross Entropy: 5.4723, Perplexity: 44.3937\n",
            "  Alpha: 0.5311, Lambda: 0.7000, Cross Entropy: 5.5844, Perplexity: 47.9813\n",
            "  Alpha: 0.5311, Lambda: 0.8000, Cross Entropy: 5.7286, Perplexity: 53.0241\n",
            "  Alpha: 0.5311, Lambda: 0.9000, Cross Entropy: 5.9335, Perplexity: 61.1150\n",
            "  Alpha: 0.5311, Lambda: 1.0000, Cross Entropy: 6.3772, Perplexity: 83.1235\n",
            "  Alpha: 0.5515, Lambda: 0.0000, Cross Entropy: 5.1190, Perplexity: 34.7521\n",
            "  Alpha: 0.5515, Lambda: 0.1000, Cross Entropy: 5.1545, Perplexity: 35.6182\n",
            "  Alpha: 0.5515, Lambda: 0.2000, Cross Entropy: 5.2006, Perplexity: 36.7727\n",
            "  Alpha: 0.5515, Lambda: 0.3000, Cross Entropy: 5.2558, Perplexity: 38.2075\n",
            "  Alpha: 0.5515, Lambda: 0.4000, Cross Entropy: 5.3209, Perplexity: 39.9713\n",
            "  Alpha: 0.5515, Lambda: 0.5000, Cross Entropy: 5.3977, Perplexity: 42.1574\n",
            "  Alpha: 0.5515, Lambda: 0.6000, Cross Entropy: 5.4895, Perplexity: 44.9261\n",
            "  Alpha: 0.5515, Lambda: 0.7000, Cross Entropy: 5.6019, Perplexity: 48.5684\n",
            "  Alpha: 0.5515, Lambda: 0.8000, Cross Entropy: 5.7465, Perplexity: 53.6856\n",
            "  Alpha: 0.5515, Lambda: 0.9000, Cross Entropy: 5.9517, Perplexity: 61.8909\n",
            "  Alpha: 0.5515, Lambda: 1.0000, Cross Entropy: 6.3955, Perplexity: 84.1852\n",
            "  Alpha: 0.5719, Lambda: 0.0000, Cross Entropy: 5.1332, Perplexity: 35.0963\n",
            "  Alpha: 0.5719, Lambda: 0.1000, Cross Entropy: 5.1693, Perplexity: 35.9848\n",
            "  Alpha: 0.5719, Lambda: 0.2000, Cross Entropy: 5.2158, Perplexity: 37.1627\n",
            "  Alpha: 0.5719, Lambda: 0.3000, Cross Entropy: 5.2714, Perplexity: 38.6234\n",
            "  Alpha: 0.5719, Lambda: 0.4000, Cross Entropy: 5.3369, Perplexity: 40.4168\n",
            "  Alpha: 0.5719, Lambda: 0.5000, Cross Entropy: 5.4141, Perplexity: 42.6375\n",
            "  Alpha: 0.5719, Lambda: 0.6000, Cross Entropy: 5.5062, Perplexity: 45.4485\n",
            "  Alpha: 0.5719, Lambda: 0.7000, Cross Entropy: 5.6190, Perplexity: 49.1444\n",
            "  Alpha: 0.5719, Lambda: 0.8000, Cross Entropy: 5.7638, Perplexity: 54.3343\n",
            "  Alpha: 0.5719, Lambda: 0.9000, Cross Entropy: 5.9693, Perplexity: 62.6515\n",
            "  Alpha: 0.5719, Lambda: 1.0000, Cross Entropy: 6.4132, Perplexity: 85.2263\n",
            "  Alpha: 0.5922, Lambda: 0.0000, Cross Entropy: 5.1471, Perplexity: 35.4350\n",
            "  Alpha: 0.5922, Lambda: 0.1000, Cross Entropy: 5.1837, Perplexity: 36.3454\n",
            "  Alpha: 0.5922, Lambda: 0.2000, Cross Entropy: 5.2306, Perplexity: 37.5461\n",
            "  Alpha: 0.5922, Lambda: 0.3000, Cross Entropy: 5.2866, Perplexity: 39.0320\n",
            "  Alpha: 0.5922, Lambda: 0.4000, Cross Entropy: 5.3524, Perplexity: 40.8543\n",
            "  Alpha: 0.5922, Lambda: 0.5000, Cross Entropy: 5.4299, Perplexity: 43.1092\n",
            "  Alpha: 0.5922, Lambda: 0.6000, Cross Entropy: 5.5224, Perplexity: 45.9615\n",
            "  Alpha: 0.5922, Lambda: 0.7000, Cross Entropy: 5.6355, Perplexity: 49.7099\n",
            "  Alpha: 0.5922, Lambda: 0.8000, Cross Entropy: 5.7806, Perplexity: 54.9710\n",
            "  Alpha: 0.5922, Lambda: 0.9000, Cross Entropy: 5.9864, Perplexity: 63.3978\n",
            "  Alpha: 0.5922, Lambda: 1.0000, Cross Entropy: 6.4304, Perplexity: 86.2478\n",
            "  Alpha: 0.6126, Lambda: 0.0000, Cross Entropy: 5.1606, Perplexity: 35.7684\n",
            "  Alpha: 0.6126, Lambda: 0.1000, Cross Entropy: 5.1977, Perplexity: 36.7002\n",
            "  Alpha: 0.6126, Lambda: 0.2000, Cross Entropy: 5.2450, Perplexity: 37.9231\n",
            "  Alpha: 0.6126, Lambda: 0.3000, Cross Entropy: 5.3014, Perplexity: 39.4338\n",
            "  Alpha: 0.6126, Lambda: 0.4000, Cross Entropy: 5.3675, Perplexity: 41.2845\n",
            "  Alpha: 0.6126, Lambda: 0.5000, Cross Entropy: 5.4454, Perplexity: 43.5727\n",
            "  Alpha: 0.6126, Lambda: 0.6000, Cross Entropy: 5.5381, Perplexity: 46.4655\n",
            "  Alpha: 0.6126, Lambda: 0.7000, Cross Entropy: 5.6515, Perplexity: 50.2653\n",
            "  Alpha: 0.6126, Lambda: 0.8000, Cross Entropy: 5.7969, Perplexity: 55.5961\n",
            "  Alpha: 0.6126, Lambda: 0.9000, Cross Entropy: 6.0029, Perplexity: 64.1304\n",
            "  Alpha: 0.6126, Lambda: 1.0000, Cross Entropy: 6.4471, Perplexity: 87.2507\n",
            "  Alpha: 0.6330, Lambda: 0.0000, Cross Entropy: 5.1738, Perplexity: 36.0968\n",
            "  Alpha: 0.6330, Lambda: 0.1000, Cross Entropy: 5.2114, Perplexity: 37.0494\n",
            "  Alpha: 0.6330, Lambda: 0.2000, Cross Entropy: 5.2591, Perplexity: 38.2942\n",
            "  Alpha: 0.6330, Lambda: 0.3000, Cross Entropy: 5.3158, Perplexity: 39.8291\n",
            "  Alpha: 0.6330, Lambda: 0.4000, Cross Entropy: 5.3822, Perplexity: 41.7076\n",
            "  Alpha: 0.6330, Lambda: 0.5000, Cross Entropy: 5.4604, Perplexity: 44.0284\n",
            "  Alpha: 0.6330, Lambda: 0.6000, Cross Entropy: 5.5534, Perplexity: 46.9610\n",
            "  Alpha: 0.6330, Lambda: 0.7000, Cross Entropy: 5.6671, Perplexity: 50.8112\n",
            "  Alpha: 0.6330, Lambda: 0.8000, Cross Entropy: 5.8128, Perplexity: 56.2104\n",
            "  Alpha: 0.6330, Lambda: 0.9000, Cross Entropy: 6.0190, Perplexity: 64.8500\n",
            "  Alpha: 0.6330, Lambda: 1.0000, Cross Entropy: 6.4633, Perplexity: 88.2359\n",
            "  Alpha: 0.6534, Lambda: 0.0000, Cross Entropy: 5.1867, Perplexity: 36.4203\n",
            "  Alpha: 0.6534, Lambda: 0.1000, Cross Entropy: 5.2247, Perplexity: 37.3933\n",
            "  Alpha: 0.6534, Lambda: 0.2000, Cross Entropy: 5.2728, Perplexity: 38.6595\n",
            "  Alpha: 0.6534, Lambda: 0.3000, Cross Entropy: 5.3298, Perplexity: 40.2182\n",
            "  Alpha: 0.6534, Lambda: 0.4000, Cross Entropy: 5.3966, Perplexity: 42.1239\n",
            "  Alpha: 0.6534, Lambda: 0.5000, Cross Entropy: 5.4750, Perplexity: 44.4768\n",
            "  Alpha: 0.6534, Lambda: 0.6000, Cross Entropy: 5.5683, Perplexity: 47.4483\n",
            "  Alpha: 0.6534, Lambda: 0.7000, Cross Entropy: 5.6822, Perplexity: 51.3481\n",
            "  Alpha: 0.6534, Lambda: 0.8000, Cross Entropy: 5.8282, Perplexity: 56.8143\n",
            "  Alpha: 0.6534, Lambda: 0.9000, Cross Entropy: 6.0347, Perplexity: 65.5571\n",
            "  Alpha: 0.6534, Lambda: 1.0000, Cross Entropy: 6.4790, Perplexity: 89.2041\n",
            "  Alpha: 0.6738, Lambda: 0.0000, Cross Entropy: 5.1993, Perplexity: 36.7392\n",
            "  Alpha: 0.6738, Lambda: 0.1000, Cross Entropy: 5.2377, Perplexity: 37.7321\n",
            "  Alpha: 0.6738, Lambda: 0.2000, Cross Entropy: 5.2861, Perplexity: 39.0193\n",
            "  Alpha: 0.6738, Lambda: 0.3000, Cross Entropy: 5.3435, Perplexity: 40.6014\n",
            "  Alpha: 0.6738, Lambda: 0.4000, Cross Entropy: 5.4105, Perplexity: 42.5338\n",
            "  Alpha: 0.6738, Lambda: 0.5000, Cross Entropy: 5.4892, Perplexity: 44.9182\n",
            "  Alpha: 0.6738, Lambda: 0.6000, Cross Entropy: 5.5828, Perplexity: 47.9279\n",
            "  Alpha: 0.6738, Lambda: 0.7000, Cross Entropy: 5.6970, Perplexity: 51.8762\n",
            "  Alpha: 0.6738, Lambda: 0.8000, Cross Entropy: 5.8432, Perplexity: 57.4082\n",
            "  Alpha: 0.6738, Lambda: 0.9000, Cross Entropy: 6.0499, Perplexity: 66.2525\n",
            "  Alpha: 0.6738, Lambda: 1.0000, Cross Entropy: 6.4944, Perplexity: 90.1560\n",
            "  Alpha: 0.6942, Lambda: 0.0000, Cross Entropy: 5.2115, Perplexity: 37.0537\n",
            "  Alpha: 0.6942, Lambda: 0.1000, Cross Entropy: 5.2504, Perplexity: 38.0661\n",
            "  Alpha: 0.6942, Lambda: 0.2000, Cross Entropy: 5.2992, Perplexity: 39.3738\n",
            "  Alpha: 0.6942, Lambda: 0.3000, Cross Entropy: 5.3568, Perplexity: 40.9788\n",
            "  Alpha: 0.6942, Lambda: 0.4000, Cross Entropy: 5.4242, Perplexity: 42.9375\n",
            "  Alpha: 0.6942, Lambda: 0.5000, Cross Entropy: 5.5031, Perplexity: 45.3528\n",
            "  Alpha: 0.6942, Lambda: 0.6000, Cross Entropy: 5.5969, Perplexity: 48.4001\n",
            "  Alpha: 0.6942, Lambda: 0.7000, Cross Entropy: 5.7114, Perplexity: 52.3960\n",
            "  Alpha: 0.6942, Lambda: 0.8000, Cross Entropy: 5.8578, Perplexity: 57.9927\n",
            "  Alpha: 0.6942, Lambda: 0.9000, Cross Entropy: 6.0647, Perplexity: 66.9365\n",
            "  Alpha: 0.6942, Lambda: 1.0000, Cross Entropy: 6.5093, Perplexity: 91.0924\n",
            "  Alpha: 0.7146, Lambda: 0.0000, Cross Entropy: 5.2236, Perplexity: 37.3640\n",
            "  Alpha: 0.7146, Lambda: 0.1000, Cross Entropy: 5.2629, Perplexity: 38.3955\n",
            "  Alpha: 0.7146, Lambda: 0.2000, Cross Entropy: 5.3119, Perplexity: 39.7234\n",
            "  Alpha: 0.7146, Lambda: 0.3000, Cross Entropy: 5.3698, Perplexity: 41.3508\n",
            "  Alpha: 0.7146, Lambda: 0.4000, Cross Entropy: 5.4375, Perplexity: 43.3353\n",
            "  Alpha: 0.7146, Lambda: 0.5000, Cross Entropy: 5.5167, Perplexity: 45.7809\n",
            "  Alpha: 0.7146, Lambda: 0.6000, Cross Entropy: 5.6107, Perplexity: 48.8652\n",
            "  Alpha: 0.7146, Lambda: 0.7000, Cross Entropy: 5.7254, Perplexity: 52.9080\n",
            "  Alpha: 0.7146, Lambda: 0.8000, Cross Entropy: 5.8720, Perplexity: 58.5681\n",
            "  Alpha: 0.7146, Lambda: 0.9000, Cross Entropy: 6.0792, Perplexity: 67.6097\n",
            "  Alpha: 0.7146, Lambda: 1.0000, Cross Entropy: 6.5238, Perplexity: 92.0139\n",
            "  Alpha: 0.7350, Lambda: 0.0000, Cross Entropy: 5.2353, Perplexity: 37.6701\n",
            "  Alpha: 0.7350, Lambda: 0.1000, Cross Entropy: 5.2750, Perplexity: 38.7204\n",
            "  Alpha: 0.7350, Lambda: 0.2000, Cross Entropy: 5.3244, Perplexity: 40.0681\n",
            "  Alpha: 0.7350, Lambda: 0.3000, Cross Entropy: 5.3826, Perplexity: 41.7176\n",
            "  Alpha: 0.7350, Lambda: 0.4000, Cross Entropy: 5.4505, Perplexity: 43.7274\n",
            "  Alpha: 0.7350, Lambda: 0.5000, Cross Entropy: 5.5299, Perplexity: 46.2029\n",
            "  Alpha: 0.7350, Lambda: 0.6000, Cross Entropy: 5.6242, Perplexity: 49.3235\n",
            "  Alpha: 0.7350, Lambda: 0.7000, Cross Entropy: 5.7391, Perplexity: 53.4123\n",
            "  Alpha: 0.7350, Lambda: 0.8000, Cross Entropy: 5.8859, Perplexity: 59.1349\n",
            "  Alpha: 0.7350, Lambda: 0.9000, Cross Entropy: 6.0932, Perplexity: 68.2726\n",
            "  Alpha: 0.7350, Lambda: 1.0000, Cross Entropy: 6.5379, Perplexity: 92.9211\n",
            "  Alpha: 0.7553, Lambda: 0.0000, Cross Entropy: 5.2469, Perplexity: 37.9723\n",
            "  Alpha: 0.7553, Lambda: 0.1000, Cross Entropy: 5.2869, Perplexity: 39.0410\n",
            "  Alpha: 0.7553, Lambda: 0.2000, Cross Entropy: 5.3366, Perplexity: 40.4081\n",
            "  Alpha: 0.7553, Lambda: 0.3000, Cross Entropy: 5.3950, Perplexity: 42.0794\n",
            "  Alpha: 0.7553, Lambda: 0.4000, Cross Entropy: 5.4632, Perplexity: 44.1141\n",
            "  Alpha: 0.7553, Lambda: 0.5000, Cross Entropy: 5.5428, Perplexity: 46.6190\n",
            "  Alpha: 0.7553, Lambda: 0.6000, Cross Entropy: 5.6374, Perplexity: 49.7752\n",
            "  Alpha: 0.7553, Lambda: 0.7000, Cross Entropy: 5.7525, Perplexity: 53.9093\n",
            "  Alpha: 0.7553, Lambda: 0.8000, Cross Entropy: 5.8995, Perplexity: 59.6933\n",
            "  Alpha: 0.7553, Lambda: 0.9000, Cross Entropy: 6.1070, Perplexity: 68.9255\n",
            "  Alpha: 0.7553, Lambda: 1.0000, Cross Entropy: 6.5517, Perplexity: 93.8145\n",
            "  Alpha: 0.7757, Lambda: 0.0000, Cross Entropy: 5.2582, Perplexity: 38.2708\n",
            "  Alpha: 0.7757, Lambda: 0.1000, Cross Entropy: 5.2986, Perplexity: 39.3575\n",
            "  Alpha: 0.7757, Lambda: 0.2000, Cross Entropy: 5.3485, Perplexity: 40.7437\n",
            "  Alpha: 0.7757, Lambda: 0.3000, Cross Entropy: 5.4072, Perplexity: 42.4364\n",
            "  Alpha: 0.7757, Lambda: 0.4000, Cross Entropy: 5.4756, Perplexity: 44.4956\n",
            "  Alpha: 0.7757, Lambda: 0.5000, Cross Entropy: 5.5555, Perplexity: 47.0293\n",
            "  Alpha: 0.7757, Lambda: 0.6000, Cross Entropy: 5.6502, Perplexity: 50.2207\n",
            "  Alpha: 0.7757, Lambda: 0.7000, Cross Entropy: 5.7655, Perplexity: 54.3993\n",
            "  Alpha: 0.7757, Lambda: 0.8000, Cross Entropy: 5.9127, Perplexity: 60.2437\n",
            "  Alpha: 0.7757, Lambda: 0.9000, Cross Entropy: 6.1204, Perplexity: 69.5689\n",
            "  Alpha: 0.7757, Lambda: 1.0000, Cross Entropy: 6.5652, Perplexity: 94.6948\n",
            "  Alpha: 0.7961, Lambda: 0.0000, Cross Entropy: 5.2692, Perplexity: 38.5655\n",
            "  Alpha: 0.7961, Lambda: 0.1000, Cross Entropy: 5.3100, Perplexity: 39.6700\n",
            "  Alpha: 0.7961, Lambda: 0.2000, Cross Entropy: 5.3602, Perplexity: 41.0750\n",
            "  Alpha: 0.7961, Lambda: 0.3000, Cross Entropy: 5.4192, Perplexity: 42.7887\n",
            "  Alpha: 0.7961, Lambda: 0.4000, Cross Entropy: 5.4877, Perplexity: 44.8720\n",
            "  Alpha: 0.7961, Lambda: 0.5000, Cross Entropy: 5.5679, Perplexity: 47.4342\n",
            "  Alpha: 0.7961, Lambda: 0.6000, Cross Entropy: 5.6628, Perplexity: 50.6601\n",
            "  Alpha: 0.7961, Lambda: 0.7000, Cross Entropy: 5.7783, Perplexity: 54.8826\n",
            "  Alpha: 0.7961, Lambda: 0.8000, Cross Entropy: 5.9257, Perplexity: 60.7864\n",
            "  Alpha: 0.7961, Lambda: 0.9000, Cross Entropy: 6.1335, Perplexity: 70.2031\n",
            "  Alpha: 0.7961, Lambda: 1.0000, Cross Entropy: 6.5784, Perplexity: 95.5622\n",
            "  Alpha: 0.8165, Lambda: 0.0000, Cross Entropy: 5.2801, Perplexity: 38.8568\n",
            "  Alpha: 0.8165, Lambda: 0.1000, Cross Entropy: 5.3212, Perplexity: 39.9787\n",
            "  Alpha: 0.8165, Lambda: 0.2000, Cross Entropy: 5.3716, Perplexity: 41.4022\n",
            "  Alpha: 0.8165, Lambda: 0.3000, Cross Entropy: 5.4308, Perplexity: 43.1365\n",
            "  Alpha: 0.8165, Lambda: 0.4000, Cross Entropy: 5.4996, Perplexity: 45.2436\n",
            "  Alpha: 0.8165, Lambda: 0.5000, Cross Entropy: 5.5800, Perplexity: 47.8338\n",
            "  Alpha: 0.8165, Lambda: 0.6000, Cross Entropy: 5.6751, Perplexity: 51.0937\n",
            "  Alpha: 0.8165, Lambda: 0.7000, Cross Entropy: 5.7908, Perplexity: 55.3594\n",
            "  Alpha: 0.8165, Lambda: 0.8000, Cross Entropy: 5.9383, Perplexity: 61.3217\n",
            "  Alpha: 0.8165, Lambda: 0.9000, Cross Entropy: 6.1463, Perplexity: 70.8284\n",
            "  Alpha: 0.8165, Lambda: 1.0000, Cross Entropy: 6.5912, Perplexity: 96.4174\n",
            "  Alpha: 0.8369, Lambda: 0.0000, Cross Entropy: 5.2907, Perplexity: 39.1446\n",
            "  Alpha: 0.8369, Lambda: 0.1000, Cross Entropy: 5.3321, Perplexity: 40.2836\n",
            "  Alpha: 0.8369, Lambda: 0.2000, Cross Entropy: 5.3829, Perplexity: 41.7254\n",
            "  Alpha: 0.8369, Lambda: 0.3000, Cross Entropy: 5.4423, Perplexity: 43.4801\n",
            "  Alpha: 0.8369, Lambda: 0.4000, Cross Entropy: 5.5113, Perplexity: 45.6106\n",
            "  Alpha: 0.8369, Lambda: 0.5000, Cross Entropy: 5.5918, Perplexity: 48.2283\n",
            "  Alpha: 0.8369, Lambda: 0.6000, Cross Entropy: 5.6871, Perplexity: 51.5218\n",
            "  Alpha: 0.8369, Lambda: 0.7000, Cross Entropy: 5.8030, Perplexity: 55.8299\n",
            "  Alpha: 0.8369, Lambda: 0.8000, Cross Entropy: 5.9507, Perplexity: 61.8498\n",
            "  Alpha: 0.8369, Lambda: 0.9000, Cross Entropy: 6.1588, Perplexity: 71.4453\n",
            "  Alpha: 0.8369, Lambda: 1.0000, Cross Entropy: 6.6038, Perplexity: 97.2608\n",
            "  Alpha: 0.8573, Lambda: 0.0000, Cross Entropy: 5.3012, Perplexity: 39.4292\n",
            "  Alpha: 0.8573, Lambda: 0.1000, Cross Entropy: 5.3429, Perplexity: 40.5850\n",
            "  Alpha: 0.8573, Lambda: 0.2000, Cross Entropy: 5.3939, Perplexity: 42.0447\n",
            "  Alpha: 0.8573, Lambda: 0.3000, Cross Entropy: 5.4535, Perplexity: 43.8195\n",
            "  Alpha: 0.8573, Lambda: 0.4000, Cross Entropy: 5.5227, Perplexity: 45.9730\n",
            "  Alpha: 0.8573, Lambda: 0.5000, Cross Entropy: 5.6034, Perplexity: 48.6179\n",
            "  Alpha: 0.8573, Lambda: 0.6000, Cross Entropy: 5.6989, Perplexity: 51.9444\n",
            "  Alpha: 0.8573, Lambda: 0.7000, Cross Entropy: 5.8149, Perplexity: 56.2944\n",
            "  Alpha: 0.8573, Lambda: 0.8000, Cross Entropy: 5.9628, Perplexity: 62.3711\n",
            "  Alpha: 0.8573, Lambda: 0.9000, Cross Entropy: 6.1710, Perplexity: 72.0540\n",
            "  Alpha: 0.8573, Lambda: 1.0000, Cross Entropy: 6.6161, Perplexity: 98.0927\n",
            "  Alpha: 0.8777, Lambda: 0.0000, Cross Entropy: 5.3115, Perplexity: 39.7106\n",
            "  Alpha: 0.8777, Lambda: 0.1000, Cross Entropy: 5.3534, Perplexity: 40.8830\n",
            "  Alpha: 0.8777, Lambda: 0.2000, Cross Entropy: 5.4046, Perplexity: 42.3604\n",
            "  Alpha: 0.8777, Lambda: 0.3000, Cross Entropy: 5.4645, Perplexity: 44.1549\n",
            "  Alpha: 0.8777, Lambda: 0.4000, Cross Entropy: 5.5339, Perplexity: 46.3311\n",
            "  Alpha: 0.8777, Lambda: 0.5000, Cross Entropy: 5.6148, Perplexity: 49.0028\n",
            "  Alpha: 0.8777, Lambda: 0.6000, Cross Entropy: 5.7104, Perplexity: 52.3618\n",
            "  Alpha: 0.8777, Lambda: 0.7000, Cross Entropy: 5.8266, Perplexity: 56.7532\n",
            "  Alpha: 0.8777, Lambda: 0.8000, Cross Entropy: 5.9747, Perplexity: 62.8858\n",
            "  Alpha: 0.8777, Lambda: 0.9000, Cross Entropy: 6.1830, Perplexity: 72.6547\n",
            "  Alpha: 0.8777, Lambda: 1.0000, Cross Entropy: 6.6281, Perplexity: 98.9136\n",
            "  Alpha: 0.8981, Lambda: 0.0000, Cross Entropy: 5.3215, Perplexity: 39.9889\n",
            "  Alpha: 0.8981, Lambda: 0.1000, Cross Entropy: 5.3638, Perplexity: 41.1776\n",
            "  Alpha: 0.8981, Lambda: 0.2000, Cross Entropy: 5.4152, Perplexity: 42.6724\n",
            "  Alpha: 0.8981, Lambda: 0.3000, Cross Entropy: 5.4753, Perplexity: 44.4864\n",
            "  Alpha: 0.8981, Lambda: 0.4000, Cross Entropy: 5.5449, Perplexity: 46.6850\n",
            "  Alpha: 0.8981, Lambda: 0.5000, Cross Entropy: 5.6259, Perplexity: 49.3831\n",
            "  Alpha: 0.8981, Lambda: 0.6000, Cross Entropy: 5.7218, Perplexity: 52.7742\n",
            "  Alpha: 0.8981, Lambda: 0.7000, Cross Entropy: 5.8381, Perplexity: 57.2063\n",
            "  Alpha: 0.8981, Lambda: 0.8000, Cross Entropy: 5.9863, Perplexity: 63.3940\n",
            "  Alpha: 0.8981, Lambda: 0.9000, Cross Entropy: 6.1947, Perplexity: 73.2478\n",
            "  Alpha: 0.8981, Lambda: 1.0000, Cross Entropy: 6.6399, Perplexity: 99.7238\n",
            "  Alpha: 0.9184, Lambda: 0.0000, Cross Entropy: 5.3314, Perplexity: 40.2642\n",
            "  Alpha: 0.9184, Lambda: 0.1000, Cross Entropy: 5.3740, Perplexity: 41.4690\n",
            "  Alpha: 0.9184, Lambda: 0.2000, Cross Entropy: 5.4256, Perplexity: 42.9809\n",
            "  Alpha: 0.9184, Lambda: 0.3000, Cross Entropy: 5.4859, Perplexity: 44.8141\n",
            "  Alpha: 0.9184, Lambda: 0.4000, Cross Entropy: 5.5557, Perplexity: 47.0349\n",
            "  Alpha: 0.9184, Lambda: 0.5000, Cross Entropy: 5.6369, Perplexity: 49.7590\n",
            "  Alpha: 0.9184, Lambda: 0.6000, Cross Entropy: 5.7329, Perplexity: 53.1818\n",
            "  Alpha: 0.9184, Lambda: 0.7000, Cross Entropy: 5.8493, Perplexity: 57.6539\n",
            "  Alpha: 0.9184, Lambda: 0.8000, Cross Entropy: 5.9977, Perplexity: 63.8960\n",
            "  Alpha: 0.9184, Lambda: 0.9000, Cross Entropy: 6.2062, Perplexity: 73.8335\n",
            "  Alpha: 0.9184, Lambda: 1.0000, Cross Entropy: 6.6514, Perplexity: 100.5237\n",
            "  Alpha: 0.9388, Lambda: 0.0000, Cross Entropy: 5.3412, Perplexity: 40.5366\n",
            "  Alpha: 0.9388, Lambda: 0.1000, Cross Entropy: 5.3840, Perplexity: 41.7573\n",
            "  Alpha: 0.9388, Lambda: 0.2000, Cross Entropy: 5.4358, Perplexity: 43.2861\n",
            "  Alpha: 0.9388, Lambda: 0.3000, Cross Entropy: 5.4963, Perplexity: 45.1383\n",
            "  Alpha: 0.9388, Lambda: 0.4000, Cross Entropy: 5.5662, Perplexity: 47.3808\n",
            "  Alpha: 0.9388, Lambda: 0.5000, Cross Entropy: 5.6476, Perplexity: 50.1306\n",
            "  Alpha: 0.9388, Lambda: 0.6000, Cross Entropy: 5.7437, Perplexity: 53.5846\n",
            "  Alpha: 0.9388, Lambda: 0.7000, Cross Entropy: 5.8604, Perplexity: 58.0963\n",
            "  Alpha: 0.9388, Lambda: 0.8000, Cross Entropy: 6.0088, Perplexity: 64.3921\n",
            "  Alpha: 0.9388, Lambda: 0.9000, Cross Entropy: 6.2175, Perplexity: 74.4121\n",
            "  Alpha: 0.9388, Lambda: 1.0000, Cross Entropy: 6.6627, Perplexity: 101.3135\n",
            "  Alpha: 0.9592, Lambda: 0.0000, Cross Entropy: 5.3507, Perplexity: 40.8063\n",
            "  Alpha: 0.9592, Lambda: 0.1000, Cross Entropy: 5.3938, Perplexity: 42.0425\n",
            "  Alpha: 0.9592, Lambda: 0.2000, Cross Entropy: 5.4459, Perplexity: 43.5880\n",
            "  Alpha: 0.9592, Lambda: 0.3000, Cross Entropy: 5.5065, Perplexity: 45.4589\n",
            "  Alpha: 0.9592, Lambda: 0.4000, Cross Entropy: 5.5766, Perplexity: 47.7229\n",
            "  Alpha: 0.9592, Lambda: 0.5000, Cross Entropy: 5.6582, Perplexity: 50.4980\n",
            "  Alpha: 0.9592, Lambda: 0.6000, Cross Entropy: 5.7544, Perplexity: 53.9828\n",
            "  Alpha: 0.9592, Lambda: 0.7000, Cross Entropy: 5.8712, Perplexity: 58.5337\n",
            "  Alpha: 0.9592, Lambda: 0.8000, Cross Entropy: 6.0198, Perplexity: 64.8823\n",
            "  Alpha: 0.9592, Lambda: 0.9000, Cross Entropy: 6.2285, Perplexity: 74.9837\n",
            "  Alpha: 0.9592, Lambda: 1.0000, Cross Entropy: 6.6737, Perplexity: 102.0937\n",
            "  Alpha: 0.9796, Lambda: 0.0000, Cross Entropy: 5.3601, Perplexity: 41.0732\n",
            "  Alpha: 0.9796, Lambda: 0.1000, Cross Entropy: 5.4034, Perplexity: 42.3248\n",
            "  Alpha: 0.9796, Lambda: 0.2000, Cross Entropy: 5.4557, Perplexity: 43.8868\n",
            "  Alpha: 0.9796, Lambda: 0.3000, Cross Entropy: 5.5165, Perplexity: 45.7761\n",
            "  Alpha: 0.9796, Lambda: 0.4000, Cross Entropy: 5.5868, Perplexity: 48.0613\n",
            "  Alpha: 0.9796, Lambda: 0.5000, Cross Entropy: 5.6685, Perplexity: 50.8615\n",
            "  Alpha: 0.9796, Lambda: 0.6000, Cross Entropy: 5.7649, Perplexity: 54.3767\n",
            "  Alpha: 0.9796, Lambda: 0.7000, Cross Entropy: 5.8818, Perplexity: 58.9661\n",
            "  Alpha: 0.9796, Lambda: 0.8000, Cross Entropy: 6.0305, Perplexity: 65.3670\n",
            "  Alpha: 0.9796, Lambda: 0.9000, Cross Entropy: 6.2393, Perplexity: 75.5487\n",
            "  Alpha: 0.9796, Lambda: 1.0000, Cross Entropy: 6.6846, Perplexity: 102.8644\n",
            "  Alpha: 1.0000, Lambda: 0.0000, Cross Entropy: 5.3694, Perplexity: 41.3375\n",
            "  Alpha: 1.0000, Lambda: 0.1000, Cross Entropy: 5.4129, Perplexity: 42.6042\n",
            "  Alpha: 1.0000, Lambda: 0.2000, Cross Entropy: 5.4654, Perplexity: 44.1824\n",
            "  Alpha: 1.0000, Lambda: 0.3000, Cross Entropy: 5.5264, Perplexity: 46.0900\n",
            "  Alpha: 1.0000, Lambda: 0.4000, Cross Entropy: 5.5968, Perplexity: 48.3961\n",
            "  Alpha: 1.0000, Lambda: 0.5000, Cross Entropy: 5.6787, Perplexity: 51.2210\n",
            "  Alpha: 1.0000, Lambda: 0.6000, Cross Entropy: 5.7752, Perplexity: 54.7663\n",
            "  Alpha: 1.0000, Lambda: 0.7000, Cross Entropy: 5.8922, Perplexity: 59.3937\n",
            "  Alpha: 1.0000, Lambda: 0.8000, Cross Entropy: 6.0410, Perplexity: 65.8462\n",
            "  Alpha: 1.0000, Lambda: 0.9000, Cross Entropy: 6.2500, Perplexity: 76.1072\n",
            "  Alpha: 1.0000, Lambda: 1.0000, Cross Entropy: 6.6952, Perplexity: 103.6260\n",
            "---------------------------------------------\n",
            "Optimal Alpha: 0.0010, Optimal Lambda: 0.4000, Best Perplexity: 18.6266\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "best_alpha, best_lambda,best_perplexity = ngram_model.fine_tune(processed_dev_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUpizBmhxHIP"
      },
      "source": [
        "### Evaluation after Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR5NgIJpxHIP",
        "outputId": "01b12224-b3c7-4189-ad4d-56e7d7c1cf04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Ngram Model with alpha = 0.001 and lambda = 0.4\n",
            "============================================================\n",
            "Evaluating Interpolation Model on Train Set:\n",
            "  Cross Entropy: 2.5379\n",
            "  Perplexity: 5.8076\n",
            "---------------------------------------------\n",
            "Evaluating Interpolation Model on Dev Set:\n",
            "  Cross Entropy: 4.2193\n",
            "  Perplexity: 18.6266\n",
            "---------------------------------------------\n",
            "Evaluating Interpolation Model on Test Set:\n",
            "  Cross Entropy: 4.4071\n",
            "  Perplexity: 21.2157\n",
            "---------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f\"Evaluating Ngram Model with alpha = {ngram_model.alpha} and lambda = {ngram_model.lamda}\")\n",
        "print(\"=\" * 60)\n",
        "# Evaluate train and dev and test sets for interpolated ngram\n",
        "for dataset_name, corpus in [(\"Train\", processed_train_corpus),(\"Dev\", processed_dev_corpus), (\"Test\", processed_test_corpus)]:\n",
        "    Ngram.evaluate_ngram_model(ngram_model, corpus, dataset_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giTEb6vaxHIP"
      },
      "source": [
        "## Demonstration of Autocompletion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxgjF98zxHIP"
      },
      "source": [
        "In this part, the bi-gram and tri-gram models are utilized for autocompleting an incomplete sentence, through predicting potential next words based on the context provided by preceding words.\n",
        "\n",
        "The bigram model considers the immediate previous word to suggest the next word, while the trigram model takes into account the two preceding words. The following examples  of generated texts confirm that the trigram\n",
        "model generates more fluent texts than the bigram model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtlKqxYfxHIP"
      },
      "source": [
        "### 1. Selecting the most probable next word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_hx1z7NxHIP"
      },
      "source": [
        "$\\text{'Neither did he care that...'}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIVdrS2ExHIP",
        "outputId": "f7ba01d0-7a69-4bb4-c0cb-7813b0f8715f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bi-gram completion :  Neither did he care that he had been a little more than the same time\n",
            "Tri-gram completion:  Neither did he care that he had been a good thing for a couple of weeks , his mother said , `` I don't know what was so beautiful , so that they had been\n"
          ]
        }
      ],
      "source": [
        "start_sentence = ['*start*', 'Neither', 'did', 'he', 'care', 'that']\n",
        "best_sequence = ngram_model.ngram_auto_complete(start_sentence, n=2, max_length=10)\n",
        "print('Bi-gram completion : ', Tools.format_sequence(best_sequence))\n",
        "\n",
        "best_sequence = ngram_model.ngram_auto_complete(start_sentence, n=3, max_length=30)\n",
        "print('Tri-gram completion: ', Tools.format_sequence(best_sequence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfs-SrDvxHIP"
      },
      "source": [
        "$\\text{'He had...'}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md0aRVA9xHIP",
        "outputId": "627f8262-7066-4dd3-8a4e-11a1902428de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bi-gram completion :  He had been a little more than the same time , and\n",
            "Tri-gram completion:  He had never been here at this hour . *end*\n"
          ]
        }
      ],
      "source": [
        "start_sentence = ['*start*', 'He','had']\n",
        "best_sequence = ngram_model.ngram_auto_complete(start_sentence, n=2, max_length=10)\n",
        "print('Bi-gram completion : ', Tools.format_sequence(best_sequence))\n",
        "\n",
        "best_sequence = ngram_model.ngram_auto_complete(start_sentence, n=3, max_length=40)\n",
        "print('Tri-gram completion: ', Tools.format_sequence(best_sequence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9fxweRGxHIQ"
      },
      "source": [
        "$\\text{'She never ...'}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbIexF8OxHIQ",
        "outputId": "9830848f-2bb9-4eba-bff9-8fb419b780a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bi-gram completion :  She never been a little more than the same time , and\n",
            "Tri-gram completion:  She never got on the other end of the tractor in time\n"
          ]
        }
      ],
      "source": [
        "start_sentence = ['*start*', 'She', 'never']\n",
        "best_sequence = ngram_model.ngram_auto_complete(start_sentence, n=2, max_length=10)\n",
        "print('Bi-gram completion : ', Tools.format_sequence(best_sequence))\n",
        "\n",
        "best_sequence = ngram_model.ngram_auto_complete(start_sentence, n=3, max_length=10)\n",
        "print('Tri-gram completion: ', Tools.format_sequence(best_sequence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE0odgZmxHIQ"
      },
      "source": [
        "$\\text{'I am...'}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBPJmewlxHIQ",
        "outputId": "7d418cca-eed5-4cd0-9267-d310402dfe8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bi-gram completion :  I am innocent '' . *end*\n",
            "Tri-gram completion:  I am innocent '' . *end*\n"
          ]
        }
      ],
      "source": [
        "start_sentence = ['*start*', 'I', 'am']\n",
        "best_sequence = ngram_model.ngram_auto_complete(start_sentence, n=2, max_length=5)\n",
        "print('Bi-gram completion : ', Tools.format_sequence(best_sequence))\n",
        "\n",
        "best_sequence = ngram_model.ngram_auto_complete(start_sentence, n=3, max_length=5)\n",
        "print('Tri-gram completion: ', Tools.format_sequence(best_sequence))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c42foLhKxHIQ"
      },
      "source": [
        "### 2. Using Beam Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9eToWZmxHIQ"
      },
      "source": [
        "$\\text{'I took...'}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP_5zxtdxHIQ",
        "outputId": "d0b5d08c-3fee-473e-9095-a9b2ed627938"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bi-gram completion :  I took him . *end*\n",
            "Tri-gram completion:  I took off one of these days , he said . *end*\n"
          ]
        }
      ],
      "source": [
        "start_sentence = ['*start*', 'I', 'took']\n",
        "best_sequence = ngram_model.beam_search_autocomplete(initial_state=start_sentence, n=2, max_depth=12, beam_width=5)\n",
        "print('Bi-gram completion : ', Tools.format_sequence(best_sequence))\n",
        "\n",
        "best_sequence = ngram_model.beam_search_autocomplete(initial_state=start_sentence, n=3, max_depth=12, beam_width=5)\n",
        "print('Tri-gram completion: ', Tools.format_sequence(best_sequence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tGqw4tRxHIQ"
      },
      "source": [
        "$\\text{'When he thought that...'}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E09GBaAwxHIQ",
        "outputId": "3b098b76-8203-4be3-b153-630f0ba1fd68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bi-gram completion :  When he thought that '' . *end*\n",
            "Tri-gram completion:  When he thought that maybe Wally was right . *end*\n"
          ]
        }
      ],
      "source": [
        "start_sentence =['*start*', 'When','he','thought','that']\n",
        "best_sequence = ngram_model.beam_search_autocomplete(initial_state=start_sentence, n=2, max_depth=12, beam_width=5)\n",
        "print('Bi-gram completion : ', Tools.format_sequence(best_sequence))\n",
        "\n",
        "best_sequence = ngram_model.beam_search_autocomplete(initial_state=start_sentence, n=3, max_depth=12, beam_width=5)\n",
        "print('Tri-gram completion: ', Tools.format_sequence(best_sequence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2UR1zOyxHIR"
      },
      "source": [
        "$\\text{'She...'}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccfjGcBGxHIR",
        "outputId": "788c1efa-93d4-4ae5-9d59-db6baae68226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bi-gram completion :  She said . *end*\n",
            "Tri-gram completion:  She said , `` I don't know what he did . *end*\n"
          ]
        }
      ],
      "source": [
        "start_sentence = ['*start*', 'She']\n",
        "best_sequence = ngram_model.beam_search_autocomplete(initial_state=start_sentence, n=2, max_depth=12, beam_width=3)\n",
        "print('Bi-gram completion : ', Tools.format_sequence(best_sequence))\n",
        "\n",
        "best_sequence = ngram_model.beam_search_autocomplete(initial_state=start_sentence, n=3, max_depth=12, beam_width=3)\n",
        "print('Tri-gram completion: ', Tools.format_sequence(best_sequence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P87Lf-F-xHIW"
      },
      "source": [
        "$\\text{'She never...'}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6I4FZkCxHIW",
        "outputId": "9bfc6722-c95b-4df6-f1be-dd9be9d52e7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bi-gram completion :  She never heard her . *end*\n",
            "Tri-gram completion:  She never got on well with his lips . *end*\n"
          ]
        }
      ],
      "source": [
        "start_sentence = ['*start*', 'She', 'never']\n",
        "best_sequence = ngram_model.beam_search_autocomplete(initial_state=start_sentence, n=2, max_depth=10, beam_width=4)\n",
        "print('Bi-gram completion : ', Tools.format_sequence(best_sequence))\n",
        "\n",
        "best_sequence = ngram_model.beam_search_autocomplete(initial_state=start_sentence, n=3, max_depth=10, beam_width=4)\n",
        "print('Tri-gram completion: ', Tools.format_sequence(best_sequence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaNcQPU-xHIW"
      },
      "source": [
        "$\\text{' ``That's...'}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9fhZuFCxHIW",
        "outputId": "8b20905f-f3fe-4fbc-9183-fd901ab68f73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bi-gram completion :  `` That's what he had been a little . *end*\n",
            "Tri-gram completion:  `` That's what you have a drink . *end*\n"
          ]
        }
      ],
      "source": [
        "start_sentence = ['``', 'That\\'s']\n",
        "best_sequence = ngram_model.beam_search_autocomplete(initial_state=start_sentence, n=2, max_depth=10, beam_width=4)\n",
        "print('Bi-gram completion : ', Tools.format_sequence(best_sequence))\n",
        "\n",
        "best_sequence = ngram_model.beam_search_autocomplete(initial_state=start_sentence, n=3, max_depth=10, beam_width=4)\n",
        "print('Tri-gram completion: ', Tools.format_sequence(best_sequence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfeTfVFtxHIX",
        "outputId": "77f5a115-6c03-4152-c24a-c8d9653f0629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bi-gram completion :  `` That's true . *end*\n",
            "Tri-gram completion:  `` That's true '' , she said . *end*\n"
          ]
        }
      ],
      "source": [
        "start_sentence = ['``', 'That\\'s']\n",
        "best_sequence = ngram_model.beam_search_autocomplete(initial_state=start_sentence, n=2, max_depth=10, beam_width=20)\n",
        "print('Bi-gram completion : ', Tools.format_sequence(best_sequence))\n",
        "\n",
        "best_sequence = ngram_model.beam_search_autocomplete(initial_state=start_sentence, n=3, max_depth=10, beam_width=20)\n",
        "print('Tri-gram completion: ', Tools.format_sequence(best_sequence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw3voXclxHIX"
      },
      "source": [
        "$\\text{'A young...'}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G13iMD5kxHIX",
        "outputId": "6b26fca5-02db-4716-e222-ebebf302ba73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A young man . *end*\n",
            "A young man . *end*\n"
          ]
        }
      ],
      "source": [
        "start_sentence = ['*start*', 'A', 'young']\n",
        "best_sequence = ngram_model.beam_search_autocomplete(initial_state=start_sentence, n=2, max_depth=10, beam_width=5)\n",
        "print(Tools.format_sequence(best_sequence))\n",
        "\n",
        "best_sequence = ngram_model.beam_search_autocomplete(initial_state=start_sentence, n=3, max_depth=20, beam_width=5)\n",
        "print(Tools.format_sequence(best_sequence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvBhareHxHIX"
      },
      "source": [
        "## Demonstration of Context-aware Spelling Correction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zON0aOnxHIX"
      },
      "source": [
        "$\\text{'This is a reason why I thought that .'} \\xrightarrow{\\text{noise}} \\text{'Tais ist ank etwason pbry k thught taat .'} \\xrightarrow{\\text{spelling corrector}} ... $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6MOeKMKxHIX",
        "outputId": "b0ad182d-f1bd-4ebd-ed9c-e15c1e177540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is a reason why I had to be\n"
          ]
        }
      ],
      "source": [
        "test_sentence = ['Tais', 'ist', 'ank', 'etwason', 'pbry', 'k', 'thught', 'taat', '.']\n",
        "correct_sent = ngram_model.beam_search_spelling_corrector(test_sentence, n=3)\n",
        "print(' '.join(correct_sent))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeiMbg20xHIY"
      },
      "source": [
        "In this part, an artificial test dataset is created to evaluate the context-aware spelling corrector. The process involves the following steps:\n",
        "\n",
        "1. **Dataset Selection**: The same test dataset used for evaluating the language models was utilized.\n",
        "\n",
        "2. **Character Replacement**: To introduce noise into the sentences, each character is replaced with an adjacent keyboard character with a small probability. This approach simulates common typing errors, as characters that are near each other on the keyboard are more likely to be mistyped.\n",
        "\n",
        "3. **Character Deletion**: There is another small probability of randomly deleting some characters from the sentences was introduced. This simulates realistic scenarios where characters might be inadvertently omitted during typing.\n",
        "\n",
        "The resulting dataset retains the overall structure and context of the original sentences while incorporating typographical errors that challenge the spelling corrector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x8yeEpKxHIY"
      },
      "source": [
        "### Replacement mapping (QWERTY keyboard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwyZPpIzxHIY"
      },
      "outputs": [],
      "source": [
        "replacement_mapping = {\n",
        "    'a': ['q', 'w', 's', 'z'],\n",
        "    'b': ['v', 'g', 'h', 'n'],\n",
        "    'c': ['x', 'd', 'f', 'v'],\n",
        "    'd': ['s', 'e', 'r', 'f', 'x', 'c'],\n",
        "    'e': ['w', 's', 'd', 'r'],\n",
        "    'f': ['d', 'r', 't', 'g', 'c', 'v'],\n",
        "    'g': ['f', 't', 'y', 'h', 'v', 'b'],\n",
        "    'h': ['g', 'y', 'u', 'j', 'b', 'n'],\n",
        "    'i': ['u', 'j', 'k', 'o'],\n",
        "    'j': ['h', 'u', 'i', 'k', 'n', 'm'],\n",
        "    'k': ['j', 'i', 'o', 'l', 'm'],\n",
        "    'l': ['k', 'o', 'p'],\n",
        "    'm': ['n', 'j', 'k'],\n",
        "    'n': ['b', 'h', 'j', 'm'],\n",
        "    'o': ['i', 'k', 'l', 'p'],\n",
        "    'p': ['o', 'l'],\n",
        "    'q': ['a', 'w'],\n",
        "    'r': ['e', 'd', 'f', 't'],\n",
        "    's': ['a', 'w', 'e', 'd', 'x', 'z'],\n",
        "    't': ['r', 'f', 'g', 'y'],\n",
        "    'u': ['y', 'h', 'j', 'i'],\n",
        "    'v': ['c', 'f', 'g', 'b'],\n",
        "    'w': ['q', 'a', 's', 'e'],\n",
        "    'x': ['z', 's', 'd', 'c'],\n",
        "    'y': ['t', 'g', 'h', 'u'],\n",
        "    'z': ['a', 's', 'x'],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf9gXyRkxHIY"
      },
      "outputs": [],
      "source": [
        "random.seed = 50\n",
        "\n",
        "noisy_test_corpus = [\n",
        "    [TextProcessor.introduce_noise(word, replacement_mapping, 0.1, 0.05) for word in sentence]\n",
        "    for sentence in test_corpus\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUHCaeQlxHIY"
      },
      "source": [
        "### Spelling Correction of Noisy Sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCDYu2pQxHIY"
      },
      "outputs": [],
      "source": [
        "corrected_corpus = []\n",
        "\n",
        "for noisy_sentence in noisy_test_corpus:\n",
        "    corrected_sentence = ngram_model.beam_search_spelling_corrector(noisy_sentence, n=3)\n",
        "    corrected_corpus.append(corrected_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb_hVwzDxHIY",
        "outputId": "4db20d8d-cfdc-4858-b0d5-82cf67059792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Randomly Selected Original, Noisy, and Corrected Sentences:\n",
            "\n",
            "Original:  We'll drop Mr. Rawlings off in Ardmore '' , Julia said , and for the merest second George was reminded of her father's tone with servants .\n",
            "Noisy:     Qe'll crop Nr. Dawlint if on Srdoe '' , Hukiz aid , and for the merest sexond Yeorge wss femided of her fatnef' tone with srrvants .\n",
            "Corrected: `` You are the only one of the day he was , and for a moment , her eyes , so he can be in town in\n",
            "\n",
            "Original:  Mousie Chandler had been to school with her someplace near Baltimore and tried to explain rather than defend her to the gang having lunch at Horne's .\n",
            "Noisy:     Nousiw Xhandpr had bren ro school wit hwd soeplqce near Naltimlre ad griex ro explain rqthdr than defend her to he gang havin lunh at Uorne's \n",
            "Corrected: I had had no room school it had been a tiny baby when No in rather than end her to let go past her , and I\n",
            "\n",
            "Original:  The heart , Phil .\n",
            "Noisy:     Yhe hear , Ohil .\n",
            "Corrected: She had a lot .\n",
            "\n",
            "Original:  Johnnie I suddenly realized he'd been totally out of my thoughts all evening .\n",
            "Noisy:     Nohnnke J suddeny realized he'd been total out of my thogjts qkl vening .\n",
            "Corrected: `` I don't like to keep a bad one . hot ? being .\n",
            "\n",
            "Original:  Of course , it wasn't Anne and George's fault that one family crisis seemed to follow another , and weren't they always emphasizing that they really didn't know what they would do without Theresa ? ?\n",
            "Noisy:     f coursw , it wash'r Znne ane Veorge's fault hqt one fsmily crsiz deemwd tl gllow another , and weren't they alwsys dmphqsizimg that yhey really din't inow what the would do without Yherwsa ? ?\n",
            "Corrected: `` You can get any kind of work . that one man to bed '' , he said , and they all knew he had been in on the other end of a way , had\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Randomly Selected Original, Noisy, and Corrected Sentences:\\n\")\n",
        "combined_data = list(zip(test_corpus, noisy_test_corpus, corrected_corpus))\n",
        "sampled_data = random.sample(combined_data, 5)\n",
        "\n",
        "for original, noisy, corrected in sampled_data:\n",
        "    print(f\"Original:  {' '.join(original)}\")\n",
        "    print(f\"Noisy:     {' '.join(noisy)}\")\n",
        "    print(f\"Corrected: {' '.join(corrected)}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvRA5KlrxHIZ"
      },
      "source": [
        "### Evaluation of the Context-aware Spelling Corrector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvY5NXMoxHIZ"
      },
      "source": [
        "[Word Error Rate (WER)](https://huggingface.co/spaces/evaluate-metric/wer)  $= \\frac{S + D + I}{N} = \\frac{S + D + I}{S + D + C}$\n",
        "\n",
        "> This value indicates the average number of errors per reference word. The lower the value, the better the performance (0 is a perfect score).\n",
        "\n",
        "[Character error rate (CER)](https://huggingface.co/spaces/evaluate-metric/cer) $= \\frac{S + D + I}{N} = \\frac{S + D + I}{S + D + C}$\n",
        "\n",
        "> CER’s output is not always a number between 0 and 1, in particular when there is a high number of insertions. The lower the value, the better the performance (0 is a perfect score).\n",
        "\n",
        "\n",
        "where\n",
        "\n",
        "* $S$ : # of substitutions\n",
        "* $D$ : # of deletions\n",
        "* $I$ : # of insertions\n",
        "* $C$ : # of correct words/characters\n",
        "* $N$ : # of words/characters in the reference $(N=S+D+C)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKPy1ax3xHIZ",
        "outputId": "2d28cf35-87d6-4981-d7e6-9acfc87b1304"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Ngram Model with lambda1 = 0.3 and lambda2 = 1.0\n",
            "============================================================\n",
            "Word Error Rate (WER): 0.76\n",
            "Character Error Rate (CER): 0.57\n"
          ]
        }
      ],
      "source": [
        "print(f\"Evaluating Ngram Model with lambda1 = {ngram_model.lambda1} and lambda2 = {ngram_model.lambda2}\")\n",
        "print(\"=\" * 60)\n",
        "wer_score , cer_score = ngram_model.evaluate_spelling_correction(corrected_corpus,test_corpus)\n",
        "print(f\"Word Error Rate (WER): {wer_score:.2f}\")\n",
        "print(f\"Character Error Rate (CER): {cer_score:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIxhOOh_xHIZ"
      },
      "source": [
        "### Fine-Tuning $\\lambda_1$ and $\\lambda_2$ for Optimal WER and CER\n",
        "\n",
        "In this section, we fine-tune the **`lambda1`** and **`lambda2`** hyperparameters to optimize our $n$-gram language model. The goal is to find values for these parameters that minimize **wer** and **cer** on the development set.\n",
        "\n",
        "- **Lambda1 ($\\lambda_1$)**: Controls the influence of n-gram probabilities in the model.\n",
        "- **Lambda2 ($\\lambda_2$)**: Determines the weighting of the inverse Levenshtein distance in the beam search spelling correction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsSW9Z6cxHIZ"
      },
      "outputs": [],
      "source": [
        "random.seed = 50\n",
        "\n",
        "noisy_dev_corpus = [\n",
        "    [TextProcessor.introduce_noise(word, replacement_mapping, 0.1, 0.05) for word in sentence]\n",
        "    for sentence in dev_corpus\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioorHzmXxHIa",
        "outputId": "be3dd6bd-724d-43b6-fd67-95f5aa63c822"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Lambda1: 0.0000, Lambda2: 0.0000, WER: 0.9920, CER: 0.7730\n",
            "  Lambda1: 0.0000, Lambda2: 0.1000, WER: 0.4427, CER: 0.2588\n",
            "  Lambda1: 0.0000, Lambda2: 0.2000, WER: 0.4427, CER: 0.2588\n",
            "  Lambda1: 0.0000, Lambda2: 0.3000, WER: 0.4427, CER: 0.2588\n",
            "  Lambda1: 0.0000, Lambda2: 0.4000, WER: 0.4427, CER: 0.2588\n",
            "  Lambda1: 0.0000, Lambda2: 0.5000, WER: 0.4427, CER: 0.2588\n",
            "  Lambda1: 0.0000, Lambda2: 0.6000, WER: 0.4427, CER: 0.2588\n",
            "  Lambda1: 0.0000, Lambda2: 0.7000, WER: 0.4427, CER: 0.2588\n",
            "  Lambda1: 0.0000, Lambda2: 0.8000, WER: 0.4427, CER: 0.2588\n",
            "  Lambda1: 0.0000, Lambda2: 0.9000, WER: 0.4427, CER: 0.2588\n",
            "  Lambda1: 0.0000, Lambda2: 1.0000, WER: 0.4427, CER: 0.2588\n",
            "  Lambda1: 0.1000, Lambda2: 0.0000, WER: 0.9635, CER: 0.7448\n",
            "  Lambda1: 0.1000, Lambda2: 0.1000, WER: 0.8867, CER: 0.6826\n",
            "  Lambda1: 0.1000, Lambda2: 0.2000, WER: 0.8338, CER: 0.6325\n",
            "  Lambda1: 0.1000, Lambda2: 0.3000, WER: 0.7940, CER: 0.5933\n",
            "  Lambda1: 0.1000, Lambda2: 0.4000, WER: 0.6744, CER: 0.5072\n",
            "  Lambda1: 0.1000, Lambda2: 0.5000, WER: 0.5378, CER: 0.4109\n",
            "  Lambda1: 0.1000, Lambda2: 0.6000, WER: 0.4810, CER: 0.3663\n",
            "  Lambda1: 0.1000, Lambda2: 0.7000, WER: 0.4469, CER: 0.3378\n",
            "  Lambda1: 0.1000, Lambda2: 0.8000, WER: 0.4349, CER: 0.3201\n",
            "  Lambda1: 0.1000, Lambda2: 0.9000, WER: 0.4250, CER: 0.3018\n",
            "  Lambda1: 0.1000, Lambda2: 1.0000, WER: 0.4198, CER: 0.2886\n",
            "  Lambda1: 0.2000, Lambda2: 0.0000, WER: 0.9635, CER: 0.7448\n",
            "  Lambda1: 0.2000, Lambda2: 0.1000, WER: 0.9306, CER: 0.7237\n",
            "  Lambda1: 0.2000, Lambda2: 0.2000, WER: 0.8867, CER: 0.6826\n",
            "  Lambda1: 0.2000, Lambda2: 0.3000, WER: 0.8610, CER: 0.6552\n",
            "  Lambda1: 0.2000, Lambda2: 0.4000, WER: 0.8338, CER: 0.6325\n",
            "  Lambda1: 0.2000, Lambda2: 0.5000, WER: 0.8180, CER: 0.6127\n",
            "  Lambda1: 0.2000, Lambda2: 0.6000, WER: 0.7940, CER: 0.5933\n",
            "  Lambda1: 0.2000, Lambda2: 0.7000, WER: 0.7470, CER: 0.5582\n",
            "  Lambda1: 0.2000, Lambda2: 0.8000, WER: 0.6744, CER: 0.5072\n",
            "  Lambda1: 0.2000, Lambda2: 0.9000, WER: 0.6076, CER: 0.4563\n",
            "  Lambda1: 0.2000, Lambda2: 1.0000, WER: 0.5378, CER: 0.4109\n",
            "  Lambda1: 0.3000, Lambda2: 0.0000, WER: 0.9635, CER: 0.7448\n",
            "  Lambda1: 0.3000, Lambda2: 0.1000, WER: 0.9409, CER: 0.7333\n",
            "  Lambda1: 0.3000, Lambda2: 0.2000, WER: 0.9152, CER: 0.7090\n",
            "  Lambda1: 0.3000, Lambda2: 0.3000, WER: 0.8867, CER: 0.6826\n",
            "  Lambda1: 0.3000, Lambda2: 0.4000, WER: 0.8638, CER: 0.6606\n",
            "  Lambda1: 0.3000, Lambda2: 0.5000, WER: 0.8508, CER: 0.6472\n",
            "  Lambda1: 0.3000, Lambda2: 0.6000, WER: 0.8338, CER: 0.6325\n",
            "  Lambda1: 0.3000, Lambda2: 0.7000, WER: 0.8299, CER: 0.6243\n",
            "  Lambda1: 0.3000, Lambda2: 0.8000, WER: 0.8112, CER: 0.6070\n",
            "  Lambda1: 0.3000, Lambda2: 0.9000, WER: 0.7940, CER: 0.5933\n",
            "  Lambda1: 0.3000, Lambda2: 1.0000, WER: 0.7621, CER: 0.5693\n",
            "  Lambda1: 0.4000, Lambda2: 0.0000, WER: 0.9635, CER: 0.7448\n",
            "  Lambda1: 0.4000, Lambda2: 0.1000, WER: 0.9419, CER: 0.7346\n",
            "  Lambda1: 0.4000, Lambda2: 0.2000, WER: 0.9306, CER: 0.7237\n",
            "  Lambda1: 0.4000, Lambda2: 0.3000, WER: 0.9072, CER: 0.7015\n",
            "  Lambda1: 0.4000, Lambda2: 0.4000, WER: 0.8867, CER: 0.6826\n",
            "  Lambda1: 0.4000, Lambda2: 0.5000, WER: 0.8671, CER: 0.6658\n",
            "  Lambda1: 0.4000, Lambda2: 0.6000, WER: 0.8610, CER: 0.6552\n",
            "  Lambda1: 0.4000, Lambda2: 0.7000, WER: 0.8453, CER: 0.6423\n",
            "  Lambda1: 0.4000, Lambda2: 0.8000, WER: 0.8338, CER: 0.6325\n",
            "  Lambda1: 0.4000, Lambda2: 0.9000, WER: 0.8341, CER: 0.6267\n",
            "  Lambda1: 0.4000, Lambda2: 1.0000, WER: 0.8180, CER: 0.6127\n",
            "  Lambda1: 0.5000, Lambda2: 0.0000, WER: 0.9635, CER: 0.7448\n",
            "  Lambda1: 0.5000, Lambda2: 0.1000, WER: 0.9425, CER: 0.7352\n",
            "  Lambda1: 0.5000, Lambda2: 0.2000, WER: 0.9351, CER: 0.7290\n",
            "  Lambda1: 0.5000, Lambda2: 0.3000, WER: 0.9200, CER: 0.7122\n",
            "  Lambda1: 0.5000, Lambda2: 0.4000, WER: 0.9050, CER: 0.6999\n",
            "  Lambda1: 0.5000, Lambda2: 0.5000, WER: 0.8867, CER: 0.6826\n",
            "  Lambda1: 0.5000, Lambda2: 0.6000, WER: 0.8715, CER: 0.6695\n",
            "  Lambda1: 0.5000, Lambda2: 0.7000, WER: 0.8623, CER: 0.6574\n",
            "  Lambda1: 0.5000, Lambda2: 0.8000, WER: 0.8555, CER: 0.6521\n",
            "  Lambda1: 0.5000, Lambda2: 0.9000, WER: 0.8427, CER: 0.6404\n",
            "  Lambda1: 0.5000, Lambda2: 1.0000, WER: 0.8338, CER: 0.6325\n",
            "  Lambda1: 0.6000, Lambda2: 0.0000, WER: 0.9635, CER: 0.7448\n",
            "  Lambda1: 0.6000, Lambda2: 0.1000, WER: 0.9452, CER: 0.7364\n",
            "  Lambda1: 0.6000, Lambda2: 0.2000, WER: 0.9409, CER: 0.7333\n",
            "  Lambda1: 0.6000, Lambda2: 0.3000, WER: 0.9306, CER: 0.7237\n",
            "  Lambda1: 0.6000, Lambda2: 0.4000, WER: 0.9152, CER: 0.7090\n",
            "  Lambda1: 0.6000, Lambda2: 0.5000, WER: 0.9021, CER: 0.6974\n",
            "  Lambda1: 0.6000, Lambda2: 0.6000, WER: 0.8867, CER: 0.6826\n",
            "  Lambda1: 0.6000, Lambda2: 0.7000, WER: 0.8744, CER: 0.6719\n",
            "  Lambda1: 0.6000, Lambda2: 0.8000, WER: 0.8638, CER: 0.6606\n",
            "  Lambda1: 0.6000, Lambda2: 0.9000, WER: 0.8610, CER: 0.6552\n",
            "  Lambda1: 0.6000, Lambda2: 1.0000, WER: 0.8508, CER: 0.6472\n",
            "  Lambda1: 0.7000, Lambda2: 0.0000, WER: 0.9635, CER: 0.7448\n",
            "  Lambda1: 0.7000, Lambda2: 0.1000, WER: 0.9470, CER: 0.7382\n",
            "  Lambda1: 0.7000, Lambda2: 0.2000, WER: 0.9409, CER: 0.7336\n",
            "  Lambda1: 0.7000, Lambda2: 0.3000, WER: 0.9349, CER: 0.7288\n",
            "  Lambda1: 0.7000, Lambda2: 0.4000, WER: 0.9226, CER: 0.7153\n",
            "  Lambda1: 0.7000, Lambda2: 0.5000, WER: 0.9127, CER: 0.7070\n",
            "  Lambda1: 0.7000, Lambda2: 0.6000, WER: 0.8988, CER: 0.6948\n",
            "  Lambda1: 0.7000, Lambda2: 0.7000, WER: 0.8867, CER: 0.6826\n",
            "  Lambda1: 0.7000, Lambda2: 0.8000, WER: 0.8776, CER: 0.6726\n",
            "  Lambda1: 0.7000, Lambda2: 0.9000, WER: 0.8646, CER: 0.6635\n",
            "  Lambda1: 0.7000, Lambda2: 1.0000, WER: 0.8632, CER: 0.6585\n",
            "  Lambda1: 0.8000, Lambda2: 0.0000, WER: 0.9635, CER: 0.7448\n",
            "  Lambda1: 0.8000, Lambda2: 0.1000, WER: 0.9480, CER: 0.7395\n",
            "  Lambda1: 0.8000, Lambda2: 0.2000, WER: 0.9419, CER: 0.7346\n",
            "  Lambda1: 0.8000, Lambda2: 0.3000, WER: 0.9361, CER: 0.7304\n",
            "  Lambda1: 0.8000, Lambda2: 0.4000, WER: 0.9306, CER: 0.7237\n",
            "  Lambda1: 0.8000, Lambda2: 0.5000, WER: 0.9188, CER: 0.7102\n",
            "  Lambda1: 0.8000, Lambda2: 0.6000, WER: 0.9072, CER: 0.7015\n",
            "  Lambda1: 0.8000, Lambda2: 0.7000, WER: 0.8957, CER: 0.6922\n",
            "  Lambda1: 0.8000, Lambda2: 0.8000, WER: 0.8867, CER: 0.6826\n",
            "  Lambda1: 0.8000, Lambda2: 0.9000, WER: 0.8797, CER: 0.6745\n",
            "  Lambda1: 0.8000, Lambda2: 1.0000, WER: 0.8671, CER: 0.6658\n",
            "  Lambda1: 0.9000, Lambda2: 0.0000, WER: 0.9635, CER: 0.7448\n",
            "  Lambda1: 0.9000, Lambda2: 0.1000, WER: 0.9487, CER: 0.7402\n",
            "  Lambda1: 0.9000, Lambda2: 0.2000, WER: 0.9434, CER: 0.7360\n",
            "  Lambda1: 0.9000, Lambda2: 0.3000, WER: 0.9409, CER: 0.7333\n",
            "  Lambda1: 0.9000, Lambda2: 0.4000, WER: 0.9317, CER: 0.7267\n",
            "  Lambda1: 0.9000, Lambda2: 0.5000, WER: 0.9243, CER: 0.7170\n",
            "  Lambda1: 0.9000, Lambda2: 0.6000, WER: 0.9152, CER: 0.7090\n",
            "  Lambda1: 0.9000, Lambda2: 0.7000, WER: 0.9078, CER: 0.7016\n",
            "  Lambda1: 0.9000, Lambda2: 0.8000, WER: 0.8954, CER: 0.6919\n",
            "  Lambda1: 0.9000, Lambda2: 0.9000, WER: 0.8867, CER: 0.6826\n",
            "  Lambda1: 0.9000, Lambda2: 1.0000, WER: 0.8799, CER: 0.6741\n",
            "  Lambda1: 1.0000, Lambda2: 0.0000, WER: 0.9635, CER: 0.7448\n",
            "  Lambda1: 1.0000, Lambda2: 0.1000, WER: 0.9493, CER: 0.7406\n",
            "  Lambda1: 1.0000, Lambda2: 0.2000, WER: 0.9425, CER: 0.7352\n",
            "  Lambda1: 1.0000, Lambda2: 0.3000, WER: 0.9405, CER: 0.7332\n",
            "  Lambda1: 1.0000, Lambda2: 0.4000, WER: 0.9351, CER: 0.7290\n",
            "  Lambda1: 1.0000, Lambda2: 0.5000, WER: 0.9306, CER: 0.7237\n",
            "  Lambda1: 1.0000, Lambda2: 0.6000, WER: 0.9200, CER: 0.7122\n",
            "  Lambda1: 1.0000, Lambda2: 0.7000, WER: 0.9146, CER: 0.7085\n",
            "  Lambda1: 1.0000, Lambda2: 0.8000, WER: 0.9050, CER: 0.6999\n",
            "  Lambda1: 1.0000, Lambda2: 0.9000, WER: 0.8951, CER: 0.6910\n",
            "  Lambda1: 1.0000, Lambda2: 1.0000, WER: 0.8867, CER: 0.6826\n",
            "---------------------------------------------\n",
            "Optimal Lambda1: 0.1000, Optimal Lambda2: 1.0000, Best Avg WER: 0.4198, Best Avg CER: 0.2886\n",
            "---------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(np.float64(0.1), np.float64(1.0), 0.41975308641975306, 0.2886060229742316)"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ngram_model.fine_tune_lambdas(noisy_dev_corpus,dev_corpus,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9DSYiEMxHIa"
      },
      "outputs": [],
      "source": [
        "corrected_corpus = []\n",
        "ngram_model.set_lambda1(0.1)\n",
        "ngram_model.set_lambda2(1.0)\n",
        "for noisy_sentence in noisy_test_corpus:\n",
        "    corrected_sentence = ngram_model.beam_search_spelling_corrector(noisy_sentence, n=3,beam_width=2)\n",
        "    corrected_corpus.append(corrected_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRu1gZk5xHIa",
        "outputId": "61d5207e-60ae-4a39-9688-f09c667f8789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Ngram Model with lambda1 = 0.1 and lambda2 = 1.0\n",
            "============================================================\n",
            "Word Error Rate (WER): 0.42\n",
            "Character Error Rate (CER): 0.29\n"
          ]
        }
      ],
      "source": [
        "print(f\"Evaluating Ngram Model with lambda1 = {ngram_model.lambda1} and lambda2 = {ngram_model.lambda2}\")\n",
        "print(\"=\" * 60)\n",
        "wer_score , cer_score = ngram_model.evaluate_spelling_correction(corrected_corpus,test_corpus)\n",
        "print(f\"Word Error Rate (WER): {wer_score:.2f}\")\n",
        "print(f\"Character Error Rate (CER): {cer_score:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}