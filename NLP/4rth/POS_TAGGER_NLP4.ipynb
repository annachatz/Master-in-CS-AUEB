{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtMOOIJRnc8a"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TaXpSQzWnc8d",
        "outputId": "9e145acf-4ef2-40b6-c02b-53da1c026ab3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-11 15:49:49.064922: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-11 15:49:49.149563: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
            "2024-12-11 15:49:49.149577: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
            "2024-12-11 15:49:49.170281: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-11 15:49:49.617348: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
            "2024-12-11 15:49:49.617395: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
            "2024-12-11 15:49:49.617402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/annachatz/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/annachatz/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /home/annachatz/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /home/annachatz/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/annachatz/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from torch.optim import Adam\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, auc\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.help import upenn_tagset\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tag.perceptron import PerceptronTagger\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "from scipy.stats import uniform\n",
        "from statistics import mode\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import pyconll\n",
        "import time\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKcSQJtdnc8f"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txEifliDnc8f"
      },
      "outputs": [],
      "source": [
        "class UtilsPOSTagging:\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_conllu(path):\n",
        "        \"\"\"\n",
        "        Processes .conllu files to extract relevant linguistic information:\n",
        "        - For each sentence:\n",
        "        - Each token is checked for the presence of a valid word form (token.form) and POS tag (token.upos).\n",
        "        - If both are present, the word form (converted to lowercase) and its corresponding POS tag are stored in a tuple.\n",
        "        - Only non-empty sentences are added to the output.\n",
        "        It returns a list of sentences, where each sentence is represented as a list of (word, POS tag) tuples.\n",
        "        \"\"\"\n",
        "\n",
        "        data = pyconll.load_from_file(path)\n",
        "        print(f\"Loaded {len(data)} sentences from {path}\")\n",
        "        tagged_sentences = []\n",
        "\n",
        "        for sentence in data:\n",
        "            tagged_sentence = []\n",
        "            for token in sentence:\n",
        "                if token.form and token.upos:  # Ensure both form and UPOS are valid\n",
        "                    tagged_sentence.append((token.form.lower(), token.upos))\n",
        "            if tagged_sentence:  # Skip empty sentences\n",
        "                tagged_sentences.append(tagged_sentence)\n",
        "        return tagged_sentences\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_statistics(tagged_sentences, title):\n",
        "        num_sentences = len(tagged_sentences)\n",
        "        num_words = sum(len(sentence) for sentence in tagged_sentences)\n",
        "        avg_sentence_length = num_words / num_sentences if num_sentences > 0 else 0\n",
        "\n",
        "        stats =  {\n",
        "            \"num_sentences\": num_sentences,\n",
        "            \"num_words\": num_words,\n",
        "            \"avg_sentence_length\": avg_sentence_length,\n",
        "        }\n",
        "\n",
        "        # Print statistics\n",
        "        print(f\"{title} Data Statistics:\")\n",
        "        print(f\"  Number of Sentences: {stats['num_sentences']}\")\n",
        "        print(f\"  Number of Words: {stats['num_words']}\")\n",
        "        print(f\"  Average Sentence Length: {stats['avg_sentence_length']:.2f}\\n\")\n",
        "\n",
        "        return stats\n",
        "\n",
        "    def sliding_window(sentences_as_words, sentences_as_tags, window_size=3, embedding_dim=300):\n",
        "        \"\"\"\n",
        "        Generate features and labels for an MLP model using sliding window word embeddings.\n",
        "\n",
        "        Parameters:\n",
        "            sentences_as_words (list of lists): Word embeddings for each sentence.\n",
        "            sentences_as_tags (list of lists): Corresponding POS tags for each word in the sentences.\n",
        "            window_size (int): Size of the sliding window (default is 3).\n",
        "            embedding_dim (int): Dimensionality of the word embeddings (default is 300).\n",
        "\n",
        "        Returns:\n",
        "            np.array: Feature vectors for the MLP model (flattened sliding windows).\n",
        "            np.array: Corresponding labels (POS tags for the center words).\n",
        "        \"\"\"\n",
        "        padding_value = np.zeros(embedding_dim)\n",
        "        features = []\n",
        "        tags = []\n",
        "\n",
        "        for words, pos_tags in zip(sentences_as_words, sentences_as_tags):\n",
        "            # Pad embeddings and tags at both ends\n",
        "            pad_size = window_size // 2\n",
        "            padded_words = [padding_value] * pad_size + words + [padding_value] * pad_size\n",
        "            padded_tags = [None] * pad_size + pos_tags + [None] * pad_size\n",
        "\n",
        "            # Sliding window over the padded sentence\n",
        "            for i in range(pad_size, len(padded_words) - pad_size):\n",
        "                # Extract the sliding window of embeddings\n",
        "                window = padded_words[i - pad_size:i + pad_size + 1]\n",
        "                features.append(np.concatenate(window))  # Flatten the window\n",
        "                tags.append(padded_tags[i])  # Center word's POS tag\n",
        "\n",
        "        return np.array(features), np.array(tags)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_word_embedding(word, model, embedding_dim = 300):\n",
        "        \"\"\"\n",
        "        Retrieves a word's embedding from a pre-trained model, returning a zero vector if the word is not in the model's vocabulary.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return model[word]\n",
        "        except KeyError:\n",
        "            return np.zeros(embedding_dim)\n",
        "\n",
        "    @staticmethod\n",
        "    def tags(sentences):\n",
        "        \"\"\"Extracts POS tags from sentences.\"\"\"\n",
        "        return [[tag for _, tag in sentence] for sentence in sentences]\n",
        "\n",
        "    @staticmethod\n",
        "    def words(sentences):\n",
        "        \"\"\"Extracts words from sentences.\"\"\"\n",
        "        return [[word for word, _ in sentence] for sentence in sentences]\n",
        "\n",
        "    @staticmethod\n",
        "    def map_tags_to_numbers(tags, tag_to_int):\n",
        "        return [tag_to_int[tag] for tag in tags]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSqWx-3anc8g"
      },
      "outputs": [],
      "source": [
        "class Report:\n",
        "\n",
        "    @staticmethod\n",
        "    def plot_training_history(history):\n",
        "        # Plotting loss\n",
        "        plt.figure()\n",
        "        plt.plot(history['train_loss'], label='Training Loss')\n",
        "        plt.plot(history['val_loss'], label='Validation Loss')\n",
        "        plt.title('Loss Over Epochs')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        # Plotting accuracy\n",
        "        plt.figure()\n",
        "        plt.plot(history['train_accuracy'], label='Training Accuracy')\n",
        "        plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.title('Accuracy Over Epochs')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        # Plotting F1 score\n",
        "        plt.figure()\n",
        "        plt.plot(history['train_f1_score'], label='Training F1 Score')\n",
        "        plt.plot(history['val_f1_score'], label='Validation F1 Score')\n",
        "        plt.title('F1 Score Over Epochs')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('F1 Score')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def display_metrics_scores(estimators, classes, datasets, class_names):\n",
        "        \"\"\"\n",
        "        Displays the evaluation metrics (precision, recall, F1, PR-AUC) for multiple classifiers across\n",
        "        different data subsets (Train, Dev, Test) in a styled table format, with class names displayed.\n",
        "\n",
        "        Parameters:\n",
        "        - estimators: Dictionary of classifiers\n",
        "        - classes: List of numerical class labels\n",
        "        - datasets: Dictionary of data subsets (Train, Dev, Test)\n",
        "        - class_names: Dictionary mapping class numbers to class names\n",
        "        \"\"\"\n",
        "        # inverted_class_names = {v: k for k, v in class_names.items()}\n",
        "        def style_results(results, subset_name):\n",
        "            combined_df = pd.concat(results, ignore_index=True)\n",
        "\n",
        "            # combined_df['Class'] = combined_df['Class'].replace(inverted_class_names)\n",
        "            combined_df['Class'] = combined_df['Class']\n",
        "\n",
        "            # Style and format the table\n",
        "            styled = combined_df.style.set_properties(\n",
        "                subset=pd.IndexSlice[combined_df['Classifier'].duplicated(keep='first') == False, :],\n",
        "                **{'border-top': '3px solid black'}\n",
        "            ).set_table_styles([\n",
        "                {'selector': 'th', 'props': [('text-align', 'center')]},\n",
        "                {'selector': 'td', 'props': [('text-align', 'center')]},\n",
        "                {'selector': 'index', 'props': [('display', 'none')]}\n",
        "            ])\n",
        "            return styled\n",
        "\n",
        "        # Run evaluation and store results in separate tables for each subset\n",
        "        train_results = []\n",
        "        dev_results = []\n",
        "        test_results = []\n",
        "\n",
        "        for name, estimator in estimators.items():\n",
        "            results_df = Report._evaluate_classifier(estimator, datasets, classes, name)\n",
        "            train_results.append(results_df[results_df['Subset'] == 'Train'])\n",
        "            dev_results.append(results_df[results_df['Subset'] == 'Dev'])\n",
        "            test_results.append(results_df[results_df['Subset'] == 'Test'])\n",
        "\n",
        "        styled_train_results = style_results(train_results, 'Train')\n",
        "        styled_dev_results = style_results(dev_results, 'Dev')\n",
        "        styled_test_results = style_results(test_results, 'Test')\n",
        "\n",
        "        display(styled_train_results)\n",
        "        print()\n",
        "        display(styled_dev_results)\n",
        "        print()\n",
        "        display(styled_test_results)\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _evaluate_classifier(clf, datasets, classes, classifier_name):\n",
        "        all_results = []\n",
        "\n",
        "        for subset_name, (subset_data, labels_dict) in datasets.items():\n",
        "            X = subset_data[classifier_name]\n",
        "            y = labels_dict[classifier_name]\n",
        "\n",
        "            # Handle BiLSTM-specific labels and predictions\n",
        "            if classifier_name == 'BiLSTM':\n",
        "                y_pred = clf.predict(X)\n",
        "\n",
        "                # Flatten labels and ignore padding (if required)\n",
        "                y = y.flatten()  # Ensure labels are flattened\n",
        "                y_pred = y_pred.flatten()\n",
        "\n",
        "            else:\n",
        "                # Standard processing for other classifiers\n",
        "                y_pred = clf.predict(X)\n",
        "\n",
        "            # Calculate metrics only if shapes align\n",
        "            if len(y) == len(y_pred):\n",
        "                y_scores = clf.predict_proba(X) if hasattr(clf, \"predict_proba\") else None\n",
        "                metrics, macro_metrics = Report._calculate_metrics(y, y_pred, y_scores, classes)\n",
        "\n",
        "                # Append the macro-average metrics as a single row\n",
        "                all_results.append({\n",
        "                    'Classifier': classifier_name,\n",
        "                    'Subset': subset_name,\n",
        "                    'Class': 'Macro Average',\n",
        "                    **macro_metrics\n",
        "                })\n",
        "\n",
        "                # Append each class's metrics as separate rows\n",
        "                for cls, cls_metrics in metrics.items():\n",
        "                    all_results.append({\n",
        "                        'Classifier': classifier_name,\n",
        "                        'Subset': subset_name,\n",
        "                        'Class': cls,\n",
        "                        **cls_metrics\n",
        "                    })\n",
        "            else:\n",
        "                print(f\"Skipping metrics for {classifier_name} on {subset_name} due to shape mismatch.\")\n",
        "\n",
        "        return pd.DataFrame(all_results)\n",
        "\n",
        "    @staticmethod\n",
        "    def _calculate_metrics(y_true, y_pred, y_scores, classes):\n",
        "        metrics = {}\n",
        "\n",
        "        for cls in classes:\n",
        "            # Filter predictions for the current class\n",
        "            cls_mask = y_true == cls\n",
        "\n",
        "            # Calculate precision, recall, F1 for the current class\n",
        "            precision = precision_score(y_true, y_pred, labels=[cls], average='macro', zero_division=0)\n",
        "            recall = recall_score(y_true, y_pred, labels=[cls], average='macro', zero_division=0)\n",
        "            f1 = f1_score(y_true, y_pred, labels=[cls], average='macro', zero_division=0)\n",
        "\n",
        "            # Calculate PR-AUC if probabilities are available\n",
        "            if y_scores is not None:\n",
        "                precisions, recalls, _ = precision_recall_curve(cls_mask, y_scores[:, cls])\n",
        "                pr_auc = auc(recalls, precisions)\n",
        "            else:\n",
        "                pr_auc = 0.0\n",
        "\n",
        "            metrics[cls] = {\n",
        "                'Precision': precision,\n",
        "                'Recall': recall,\n",
        "                'F1': f1,\n",
        "                'PR-AUC': pr_auc\n",
        "            }\n",
        "\n",
        "        # Compute macro metrics across all classes\n",
        "        macro_metrics = {\n",
        "            'Macro Precision': precision_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "            'Macro Recall': recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "            'Macro F1': f1_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "            'Macro PR-AUC': sum(metrics[cls]['PR-AUC'] for cls in classes) / len(classes) if y_scores is not None else 0.0\n",
        "        }\n",
        "\n",
        "        return metrics, macro_metrics\n",
        "\n",
        "\n",
        "    def display_confusion_matrix(estimators, datasets, classes):\n",
        "        \"\"\"\n",
        "        Displays the confusion matrix for each classifier across different data subsets\n",
        "        (Train, Dev, Test) in a heatmap format.\n",
        "        \"\"\"\n",
        "        for name, estimator in estimators.items():\n",
        "            for subset_name, (subset_data, y_true) in datasets.items():\n",
        "                X = subset_data[name]\n",
        "\n",
        "                y_pred = estimator.predict(X)\n",
        "\n",
        "                cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
        "\n",
        "                plt.figure(figsize=(8, 6))\n",
        "                sns.heatmap(cm, annot=True, fmt=\"d\", cmap=sns.cubehelix_palette(as_cmap=True), xticklabels=classes, yticklabels=classes)\n",
        "                plt.xlabel(\"Predicted Label\")\n",
        "                plt.ylabel(\"True Label\")\n",
        "                plt.title(f\"Confusion Matrix for {name} - {subset_name} Set\")\n",
        "                plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrwGtcIOnc8h"
      },
      "source": [
        "## Most Frequent Tag Classifier (Baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-47YrKanc8h"
      },
      "outputs": [],
      "source": [
        "class MostFrequentTagBaseline:\n",
        "    def __init__(self):\n",
        "        self.feature_to_most_frequent_tag = None\n",
        "        self.feature_to_tag_probabilities = None\n",
        "        self.most_frequent_overall_tag = None\n",
        "        self.overall_tag_probabilities = None\n",
        "\n",
        "    def fit(self, train_features, train_labels_flattened):\n",
        "        \"\"\"\n",
        "        Trains the baseline model by calculating the most frequent tag for each feature (row)\n",
        "        and the overall most frequent tag.\n",
        "\n",
        "        Parameters:\n",
        "            train_features (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
        "            train_labels_flattened (list): List of tags for each row in train_features.\n",
        "        \"\"\"\n",
        "        feature_tag_counts = defaultdict(Counter)\n",
        "        overall_tag_counts = Counter()\n",
        "\n",
        "        features_as_strings = [tuple(row) for row in train_features]\n",
        "\n",
        "        # Count tags for each feature and overall tags\n",
        "        for feature, tag in zip(features_as_strings, train_labels_flattened):\n",
        "            feature_tag_counts[feature][tag] += 1\n",
        "            overall_tag_counts[tag] += 1\n",
        "\n",
        "        # Determine the most frequent tag for each feature\n",
        "        self.feature_to_most_frequent_tag = {\n",
        "            feature: tag_counts.most_common(1)[0][0] for feature, tag_counts in feature_tag_counts.items()\n",
        "        }\n",
        "\n",
        "        self.feature_to_tag_probabilities = {\n",
        "            feature: {\n",
        "                tag: count / sum(tag_counts.values())\n",
        "                for tag, count in tag_counts.items()\n",
        "            }\n",
        "            for feature, tag_counts in feature_tag_counts.items()\n",
        "        }\n",
        "\n",
        "        # Determine the overall most frequent tag\n",
        "        self.most_frequent_overall_tag = overall_tag_counts.most_common(1)[0][0]\n",
        "\n",
        "        total_tags = sum(overall_tag_counts.values())\n",
        "        self.overall_tag_probabilities = {\n",
        "            tag: count / total_tags for tag, count in overall_tag_counts.items()\n",
        "        }\n",
        "\n",
        "    def predict(self, test_features):\n",
        "        \"\"\"\n",
        "        Predicts tags for the test set using the trained baseline model.\n",
        "\n",
        "        Parameters:\n",
        "            test_features (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
        "\n",
        "        Returns:\n",
        "            list: Predicted tags for the test set.\n",
        "        \"\"\"\n",
        "        features_as_strings = [tuple(row) for row in test_features]\n",
        "\n",
        "        predicted_tags = []\n",
        "        for feature in features_as_strings:\n",
        "            if feature in self.feature_to_most_frequent_tag:\n",
        "                predicted_tags.append(self.feature_to_most_frequent_tag[feature])\n",
        "            else:\n",
        "                predicted_tags.append(self.most_frequent_overall_tag)\n",
        "        return predicted_tags\n",
        "\n",
        "    def predict_proba(self, test_features):\n",
        "        \"\"\"\n",
        "        Predicts probabilities of each tag for the test set using the trained baseline model.\n",
        "\n",
        "        Parameters:\n",
        "            test_features (numpy.ndarray): Feature matrix of shape (n_samples, n_features).\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: A 2D array where each row corresponds to a sample,\n",
        "                        and each column corresponds to the probability of a tag.\n",
        "        \"\"\"\n",
        "        features_as_strings = [tuple(row) for row in test_features]\n",
        "\n",
        "        all_tags = sorted(self.overall_tag_probabilities.keys())\n",
        "        predicted_probabilities = []\n",
        "        for feature in features_as_strings:\n",
        "            if feature in self.feature_to_tag_probabilities:\n",
        "                tag_probabilities = self.feature_to_tag_probabilities[feature]\n",
        "            else:\n",
        "                tag_probabilities = self.overall_tag_probabilities\n",
        "\n",
        "            probabilities = [tag_probabilities.get(tag, 0.0) for tag in all_tags]\n",
        "            predicted_probabilities.append(probabilities)\n",
        "        return np.array(predicted_probabilities)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUAXQ20inc8i"
      },
      "source": [
        "## BiLSTM with \"Deep\" Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe6Oig9Fnc8i"
      },
      "outputs": [],
      "source": [
        "class BiLSTM_Attention(nn.Module):\n",
        "    def __init__(self, input_dim, n_classes, dropout_prob_emb, dropout_prob_att, dropout_prob_out, hidden_dim, lstm_hidden_dim, lstm_stacks, attention_hidden_sizes, max_words, matrix_embeddings=None):\n",
        "        super(BiLSTM_Attention, self).__init__()\n",
        "        if matrix_embeddings is not None:\n",
        "            self.embeddings = nn.Embedding(num_embeddings=max_words + 2, embedding_dim=input_dim, padding_idx=0)\n",
        "            self.embeddings.weight = nn.Parameter(torch.tensor(matrix_embeddings, dtype=torch.float32))\n",
        "            self.embeddings.weight.requires_grad = False  # Freeze embeddings\n",
        "        else:\n",
        "            self.embeddings = nn.Embedding(num_embeddings=max_words + 2, embedding_dim=input_dim, padding_idx=0)\n",
        "\n",
        "        # BiLSTM layer\n",
        "        self.bilstm = nn.LSTM(input_size=input_dim,\n",
        "                              hidden_size=lstm_hidden_dim,\n",
        "                              num_layers=lstm_stacks,\n",
        "                              bidirectional=True,\n",
        "                              batch_first=True)\n",
        "\n",
        "        # Dropouts\n",
        "        self.dropout_emb = nn.Dropout(dropout_prob_emb)\n",
        "        self.dropout_att = nn.Dropout(dropout_prob_att)\n",
        "        self.dropout_out = nn.Dropout(dropout_prob_out)\n",
        "\n",
        "        # Attention MLP\n",
        "        self.deep_attention_mlp = self._build_attention_mlp(2 * lstm_hidden_dim, attention_hidden_sizes)\n",
        "\n",
        "        # Classification layer\n",
        "        self.classifier = nn.Linear(2 * lstm_hidden_dim, n_classes)\n",
        "\n",
        "    def _build_attention_mlp(self, input_size, hidden_sizes):\n",
        "        layers = []\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(input_size, hidden_size))\n",
        "            layers.append(nn.Tanh())\n",
        "            input_size = hidden_size\n",
        "        layers.append(nn.Linear(hidden_sizes[-1], 1))  # Output layer\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Embedding layer\n",
        "        x = self.embeddings(input)  # (batch_size, seq_length, embedding_dim)\n",
        "        x = self.dropout_emb(x)\n",
        "\n",
        "        # BiLSTM layer\n",
        "        lstm_out, _ = self.bilstm(x)  # (batch_size, seq_length, 2*lstm_hidden_dim)\n",
        "\n",
        "        # Attention scores\n",
        "        attention_scores = self.deep_attention_mlp(lstm_out)  # (batch_size, seq_length, 1)\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)  # (batch_size, seq_length, 1)\n",
        "\n",
        "        # Apply attention weights\n",
        "        attended_output = lstm_out * attention_weights  # (batch_size, seq_length, 2*lstm_hidden_dim)\n",
        "\n",
        "        # Dropout after attention\n",
        "        attended_output = self.dropout_att(attended_output)\n",
        "\n",
        "        # Classify each token\n",
        "        logits = self.classifier(attended_output)  # (batch_size, seq_length, n_classes)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx1jwl5Knc8i"
      },
      "outputs": [],
      "source": [
        "class UtilsBILSTM:\n",
        "    def train_and_evaluate(model, optimizer, criterion, device, epochs, train_loader, test_loader, title='Test'):\n",
        "        \"\"\"\n",
        "        Train and evaluate a PyTorch model.\n",
        "\n",
        "        Parameters:\n",
        "        - model: nn.Module, the model to train\n",
        "        - train_loader: DataLoader, DataLoader for training data\n",
        "        - test_loader: DataLoader, DataLoader for testing/validation data\n",
        "        - optimizer: Optimizer, the optimizer to use for training\n",
        "        - criterion: Loss function\n",
        "        - epochs: int, number of epochs to train\n",
        "        - device: torch.device, device to train on (CPU or GPU)\n",
        "\n",
        "        Returns:\n",
        "        - test_accuracy: float, accuracy on the test set\n",
        "        - test_f1: float, F1 score on the test set\n",
        "        \"\"\"\n",
        "        model.to(device)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            model.train()  # Set the model to training mode\n",
        "            running_loss = 0.0  # Track the loss for the epoch\n",
        "\n",
        "            for batch in train_loader:\n",
        "                features, labels = batch\n",
        "                features = features.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(features)\n",
        "                outputs = outputs.view(-1, outputs.size(-1))\n",
        "                labels = labels.view(-1)\n",
        "\n",
        "                loss = criterion(outputs, labels)  # Compute loss\n",
        "                loss.backward()  # Backward pass\n",
        "                optimizer.step()  # Update model parameters\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            # Print average loss for the epoch\n",
        "            epoch_loss = running_loss / len(train_loader)\n",
        "            print(f'Epoch [{epoch + 1}/{epochs}], Average Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        # Evaluation phase\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_labels = []\n",
        "        all_predictions = []\n",
        "\n",
        "        with torch.no_grad():  # No gradient calculation during evaluation\n",
        "            for batch in test_loader:\n",
        "                features, labels = batch\n",
        "                features = features.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(features)\n",
        "                predictions = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "                all_predictions.extend(predictions.cpu().view(-1).numpy())\n",
        "                all_labels.extend(labels.cpu().view(-1).numpy())\n",
        "\n",
        "        valid_labels = [l for l in all_labels if l != 0]\n",
        "        valid_predictions = [p for l, p in zip(all_labels, all_predictions) if l != 0]\n",
        "\n",
        "        # Compute metrics\n",
        "        test_f1 = f1_score(valid_labels, valid_predictions, average='macro')\n",
        "        test_accuracy = sum([l == p for l, p in zip(valid_labels, valid_predictions)]) / len(valid_labels)\n",
        "\n",
        "        print(f'{title} Accuracy: {test_accuracy:.4f}')\n",
        "        print(f'{title} F1 Score: {test_f1:.4f}')\n",
        "\n",
        "        return test_accuracy, test_f1\n",
        "\n",
        "    @staticmethod\n",
        "    def train_and_validate(model, optimizer, criterion, device, epochs, train_loader, val_loader):\n",
        "        model.to(device)\n",
        "\n",
        "        # Initialize history and best model tracking\n",
        "        history = {\n",
        "            'train_loss': [], 'val_loss': [],\n",
        "            'train_accuracy': [], 'val_accuracy': [],\n",
        "            'train_f1_score': [], 'val_f1_score': []\n",
        "        }\n",
        "        best_val_loss = float(\"inf\")\n",
        "        best_model_path = None\n",
        "\n",
        "        start_training_time = time.time()\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            all_train_labels = []\n",
        "            all_train_predictions = []\n",
        "\n",
        "\n",
        "            for batch in train_loader:\n",
        "                features, labels = batch\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(features)  # (batch_size, seq_length, n_classes)\n",
        "                loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "                predictions = torch.argmax(outputs, dim=-1).view(-1)  # Flatten predictions\n",
        "                flat_labels = labels.view(-1)\n",
        "                valid_labels = flat_labels[flat_labels != 0].cpu().numpy()\n",
        "                valid_predictions = predictions[flat_labels != 0].cpu().numpy()\n",
        "\n",
        "                all_train_labels.extend(valid_labels)\n",
        "                all_train_predictions.extend(valid_predictions)\n",
        "\n",
        "            train_accuracy = accuracy_score(all_train_labels, all_train_predictions)\n",
        "            train_f1_score = f1_score(all_train_labels, all_train_predictions, average='weighted')\n",
        "            train_loss = running_loss / len(train_loader)\n",
        "\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['train_accuracy'].append(train_accuracy)\n",
        "            history['train_f1_score'].append(train_f1_score)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, F1 Score: {train_f1_score:.4f}\")\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_running_loss = 0.0\n",
        "            all_val_labels = []\n",
        "            all_val_predictions = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for features, labels in val_loader:\n",
        "                    features, labels = features.to(device), labels.to(device)\n",
        "                    outputs = model(features)\n",
        "\n",
        "                    # Flatten outputs and labels for loss computation\n",
        "                    loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "                    val_running_loss += loss.item()\n",
        "\n",
        "                    predictions = torch.argmax(outputs, dim=-1).view(-1)  # Flatten predictions\n",
        "                    labels = labels.view(-1)  # Flatten labels\n",
        "\n",
        "                    # Collect valid labels and predictions (excluding padding)\n",
        "                    valid_labels = labels[labels != 0].cpu().numpy()\n",
        "                    valid_predictions = predictions[labels != 0].cpu().numpy()\n",
        "\n",
        "                    all_val_labels.extend(valid_labels)\n",
        "                    all_val_predictions.extend(valid_predictions)\n",
        "\n",
        "        # Calculate validation metrics\n",
        "            val_loss = val_running_loss / len(val_loader)\n",
        "            val_accuracy = accuracy_score(all_val_labels, all_val_predictions)\n",
        "            val_f1 = f1_score(all_val_labels, all_val_predictions, average='weighted')\n",
        "\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_accuracy'].append(val_accuracy)\n",
        "            history['val_f1_score'].append(val_f1)\n",
        "\n",
        "            print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, F1 Score: {val_f1:.4f}\")\n",
        "\n",
        "            # Save checkpoint if validation loss improves\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                new_model_path = f'bilstm_best_model_epoch_{epoch+1}.pth'\n",
        "\n",
        "                if best_model_path is not None:\n",
        "                    os.remove(best_model_path)\n",
        "\n",
        "                torch.save(model.state_dict(), new_model_path)\n",
        "                best_model_path = new_model_path\n",
        "\n",
        "        end_training_time = time.time()\n",
        "        print(f\"Training complete! Total time: {end_training_time - start_training_time:.2f} seconds\")\n",
        "\n",
        "        return history, best_model_path\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCknfzAxnc8j"
      },
      "outputs": [],
      "source": [
        "class TrainEvaluate:\n",
        "    @staticmethod\n",
        "    def train_model(model, optimizer, criterion, device, epochs, train_loader):\n",
        "        \"\"\"\n",
        "        Train the model.\n",
        "\n",
        "        Parameters:\n",
        "        - model: nn.Module, the model to train\n",
        "        - optimizer: Optimizer, optimizer to use for training\n",
        "        - criterion: Loss function\n",
        "        - device: torch.device, device to train on (CPU or GPU)\n",
        "        - epochs: int, number of epochs\n",
        "        - train_loader: DataLoader, training data\n",
        "\n",
        "        Returns:\n",
        "        - model: The trained model\n",
        "        \"\"\"\n",
        "        model.to(device)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for batch in train_loader:\n",
        "                features, labels = batch\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            avg_loss = running_loss / len(train_loader)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def predict(model, X, device,batch_size=32):\n",
        "        \"\"\"\n",
        "        Predict class labels for each token in the sequences.\n",
        "\n",
        "        Parameters:\n",
        "        - model: nn.Module, the trained model\n",
        "        - X: array-like, input features (sequences)\n",
        "        - device: torch.device, device to evaluate on (CPU or GPU)\n",
        "\n",
        "        Returns:\n",
        "        - predictions: np.ndarray, predicted labels for each token\n",
        "        \"\"\"\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        dataset = TensorDataset(torch.tensor(X, dtype=torch.long))\n",
        "        data_loader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "        all_predictions = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in data_loader:\n",
        "                features = batch[0].to(device)\n",
        "                outputs = model(features)\n",
        "                predictions = torch.argmax(outputs, dim=-1).cpu().numpy()\n",
        "                all_predictions.extend(predictions)\n",
        "\n",
        "        return np.concatenate(all_predictions, axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def evaluate_model(model, criterion, device, test_loader):\n",
        "        \"\"\"\n",
        "        Evaluate the model and print a classification report.\n",
        "\n",
        "        Parameters:\n",
        "        - model: nn.Module, the trained model\n",
        "        - criterion: Loss function\n",
        "        - device: torch.device, device to evaluate on (CPU or GPU)\n",
        "        - test_loader: DataLoader, testing data\n",
        "\n",
        "        Returns:\n",
        "        - report: str, classification report\n",
        "        \"\"\"\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        all_predictions = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                features, labels = batch\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(features)\n",
        "                predictions = torch.argmax(outputs, dim=-1).view(-1)\n",
        "                labels = labels.view(-1)\n",
        "\n",
        "                valid_labels = labels[labels != 0].cpu().numpy()\n",
        "                valid_predictions = predictions[labels != 0].cpu().numpy()\n",
        "\n",
        "                all_labels.extend(valid_labels)\n",
        "                all_predictions.extend(valid_predictions)\n",
        "\n",
        "        # Generate classification report\n",
        "        report = classification_report(all_labels, all_predictions, zero_division=0)\n",
        "        print(\"Classification Report:\")\n",
        "        print(report)\n",
        "\n",
        "        # Additional metrics\n",
        "        accuracy = accuracy_score(all_labels, all_predictions)\n",
        "        f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "        return report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42rxAmu3nc8j"
      },
      "outputs": [],
      "source": [
        "class BiLSTMAdapter:\n",
        "    def __init__(self, model, device, batch_size=32):\n",
        "        \"\"\"\n",
        "        Adapter for BiLSTM to provide predict and predict_proba methods.\n",
        "\n",
        "        Parameters:\n",
        "        - model: The trained BiLSTM model.\n",
        "        - device: The device (CPU/GPU) for predictions.\n",
        "        - batch_size: Batch size for processing input sequences.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for each token in the sequences.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input features (sequences).\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: Predicted labels (flattened).\n",
        "        \"\"\"\n",
        "        return TrainEvaluate.predict(self.model, X, self.device, self.batch_size)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict class probabilities for each token in the sequences.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Input features (sequences).\n",
        "\n",
        "        Returns:\n",
        "        - probabilities: Predicted probabilities (flattened).\n",
        "        \"\"\"\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        dataset = TensorDataset(torch.tensor(X, dtype=torch.long))\n",
        "        data_loader = DataLoader(dataset, batch_size=self.batch_size)\n",
        "\n",
        "        all_probabilities = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in data_loader:\n",
        "                features = batch[0].to(self.device)\n",
        "                outputs = self.model(features)\n",
        "                probabilities = torch.softmax(outputs, dim=-1).cpu().numpy()\n",
        "                all_probabilities.extend(probabilities)\n",
        "\n",
        "        return np.vstack(all_probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMS2cIganc8k"
      },
      "source": [
        "## MLP Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jGYR8ylnc8k"
      },
      "outputs": [],
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_size=500, hidden_sizes=[512, 256], output_size=2, dropout_probs=0.5):\n",
        "        \"\"\"\n",
        "        Initializes the MLPModel with configurable hidden layers and dropout probabilities.\n",
        "\n",
        "        Parameters:\n",
        "        - input_size: int, size of the input vector (default: 500)\n",
        "        - hidden_sizes: list of int, sizes of the hidden layers (default: [512, 256])\n",
        "        - output_size: int, size of the output layer (default: 20)\n",
        "        - dropout_probs: float or list of floats, dropout probability for each layer\n",
        "          (default: 0.5, or specify one value per hidden layer)\n",
        "        \"\"\"\n",
        "        super(MLPModel, self).__init__()\n",
        "\n",
        "        # Ensure dropout_probs is a list (one value per hidden layer)\n",
        "        if isinstance(dropout_probs, float):\n",
        "            dropout_probs = [dropout_probs] * len(hidden_sizes)\n",
        "        elif len(dropout_probs) != len(hidden_sizes):\n",
        "            raise ValueError(\"dropout_probs must be a single float or a list of the same length as hidden_sizes\")\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.dropouts = nn.ModuleList()\n",
        "\n",
        "        # Input to the first hidden layer\n",
        "        self.layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
        "        self.dropouts.append(nn.Dropout(dropout_probs[0]))\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(1, len(hidden_sizes)):\n",
        "            self.layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
        "            self.dropouts.append(nn.Dropout(dropout_probs[i]))\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(hidden_sizes[-1], output_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Forward pass of the MLPModel.\n",
        "\n",
        "        Parameters:\n",
        "        - input: torch.Tensor, shape (batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "        - logits: torch.Tensor, shape (batch_size, output_size)\n",
        "        \"\"\"\n",
        "        x = input\n",
        "        for layer, dropout in zip(self.layers, self.dropouts):\n",
        "            x = F.relu(layer(x))\n",
        "            x = dropout(x)\n",
        "        logits = self.output_layer(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etnbjbHlnc8k"
      },
      "outputs": [],
      "source": [
        "class MLPClassifier:\n",
        "    def __init__(self, model, optimizer, criterion, device, epochs):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.device = device\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, data_loader):\n",
        "        \"\"\"\n",
        "        Train the model for the specified number of epochs.\n",
        "\n",
        "        Parameters:\n",
        "        - data_loader: DataLoader, DataLoader for training data\n",
        "\n",
        "        Returns:\n",
        "        - history: dict, containing loss for each epoch\n",
        "        \"\"\"\n",
        "        self.model.to(self.device)\n",
        "        history = {'loss': []}\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for features, labels in data_loader:\n",
        "                features, labels = features.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(features)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            avg_loss = running_loss / len(data_loader)\n",
        "            history['loss'].append(avg_loss)\n",
        "            print(f\"Epoch [{epoch + 1}/{self.epochs}] - Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for samples in X.\n",
        "\n",
        "        Parameters:\n",
        "        - X: array-like of shape (n_samples, n_features), input features.\n",
        "\n",
        "        Returns:\n",
        "        - predictions: np.ndarray of shape (n_samples,), predicted class labels.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        predictions = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X_tensor)\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            predictions = predicted.cpu().numpy()\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict class probabilities for samples in X.\n",
        "\n",
        "        Parameters:\n",
        "        - X: array-like of shape (n_samples, n_features), input features.\n",
        "\n",
        "        Returns:\n",
        "        - probabilities: np.ndarray of shape (n_samples, n_classes), predicted probabilities.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        probabilities = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X_tensor)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            probabilities = probs.cpu().numpy()\n",
        "\n",
        "        return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNv8w26Tnc8l"
      },
      "outputs": [],
      "source": [
        "class UtilsMLP:\n",
        "    def train_and_evaluate(model, optimizer, criterion, device, epochs, train_loader, test_loader, title='Test'):\n",
        "        \"\"\"\n",
        "        Train and evaluate a PyTorch model.\n",
        "\n",
        "        Parameters:\n",
        "        - model: nn.Module, the model to train\n",
        "        - train_loader: DataLoader, DataLoader for training data\n",
        "        - test_loader: DataLoader, DataLoader for testing/validation data\n",
        "        - optimizer: Optimizer, the optimizer to use for training\n",
        "        - criterion: Loss function\n",
        "        - epochs: int, number of epochs to train\n",
        "        - device: torch.device, device to train on (CPU or GPU)\n",
        "\n",
        "        Returns:\n",
        "        - test_accuracy: float, accuracy on the test set\n",
        "        - test_f1: float, F1 score on the test set\n",
        "        \"\"\"\n",
        "        model.to(device)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            model.train()  # Set the model to training mode\n",
        "            running_loss = 0.0  # Track the loss for the epoch\n",
        "\n",
        "            for batch in train_loader:\n",
        "                features, labels = batch\n",
        "                features = features.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)  # Compute loss\n",
        "                loss.backward()  # Backward pass\n",
        "                optimizer.step()  # Update model parameters\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            # Print average loss for the epoch\n",
        "            epoch_loss = running_loss / len(train_loader)\n",
        "            print(f'Epoch [{epoch + 1}/{epochs}], Average Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        # Evaluation phase\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_labels = []\n",
        "        all_predictions = []\n",
        "\n",
        "        with torch.no_grad():  # No gradient calculation during evaluation\n",
        "            for batch in test_loader:\n",
        "                features, labels = batch\n",
        "                features = features.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(features)\n",
        "                _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                # Collect all labels and predictions for F1 score\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "        test_accuracy = correct / total\n",
        "        test_f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "\n",
        "        print(f'{title} Accuracy: {test_accuracy:.4f}')\n",
        "        print(f'{title} F1 Score: {test_f1:.4f}')\n",
        "\n",
        "        return test_accuracy, test_f1\n",
        "\n",
        "    @staticmethod\n",
        "    def train_and_validate(model, optimizer, criterion, device, epochs, train_loader, val_loader):\n",
        "        \"\"\"\n",
        "        Train and validate a PyTorch model, tracking metrics and saving the best model.\n",
        "\n",
        "        Parameters:\n",
        "        - model: nn.Module, the model to train\n",
        "        - train_loader: DataLoader, DataLoader for training data\n",
        "        - val_loader: DataLoader, DataLoader for validation data\n",
        "        - optimizer: Optimizer, the optimizer to use for training\n",
        "        - criterion: Loss function\n",
        "        - epochs: int, number of epochs to train\n",
        "        - device: torch.device, device to train on (CPU or GPU)\n",
        "\n",
        "        Returns:\n",
        "        - history: dict, containing training and validation losses, accuracy, and F1 scores\n",
        "        - best_model_path: str, path to the saved best model\n",
        "        \"\"\"\n",
        "        model.to(device)\n",
        "\n",
        "        # Initialize history and best model tracking\n",
        "        history = {'train_loss': [], 'val_loss': [], 'train_accuracy': [], 'val_accuracy': [], 'train_f1_score': [], 'val_f1_score': []}\n",
        "        best_val_loss = float(\"inf\")  # Initialize to a high value\n",
        "        best_model_path = None\n",
        "\n",
        "        start_training_time = time.time()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            train_outputs = []\n",
        "            train_labels = []\n",
        "\n",
        "            for batch in train_loader:\n",
        "                features, labels = batch\n",
        "                features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                train_outputs.append(outputs)\n",
        "                train_labels.append(labels)\n",
        "\n",
        "            # Concatenate all outputs and labels for metrics calculation\n",
        "            train_outputs = torch.cat(train_outputs)\n",
        "            train_labels = torch.cat(train_labels)\n",
        "            train_classes = train_labels\n",
        "            train_predicted_classes = torch.argmax(train_outputs, dim=1)\n",
        "\n",
        "            # Calculate training metrics\n",
        "            train_loss = running_loss / len(train_loader)\n",
        "            train_accuracy = accuracy_score(train_classes.cpu(), train_predicted_classes.cpu())\n",
        "            train_f1_score = f1_score(train_classes.cpu(), train_predicted_classes.cpu(), average='weighted')\n",
        "\n",
        "            # Update history\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['train_accuracy'].append(train_accuracy)\n",
        "            history['train_f1_score'].append(train_f1_score)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, F1 Score: {train_f1_score:.4f}\")\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_running_loss = 0.0\n",
        "            all_outputs = []\n",
        "            all_labels = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for features, labels in val_loader:\n",
        "                    features, labels = features.to(device), labels.to(device)\n",
        "                    outputs = model(features)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_running_loss += loss.item()\n",
        "                    all_outputs.append(outputs)\n",
        "                    all_labels.append(labels)\n",
        "\n",
        "            # Concatenate all outputs and labels for metrics calculation\n",
        "            all_outputs = torch.cat(all_outputs)\n",
        "            all_labels = torch.cat(all_labels)\n",
        "            true_classes = all_labels\n",
        "            predicted_classes = torch.argmax(all_outputs, dim=1)\n",
        "\n",
        "            # Calculate validation metrics\n",
        "            val_loss = val_running_loss / len(val_loader)\n",
        "            val_accuracy = accuracy_score(true_classes.cpu(), predicted_classes.cpu())\n",
        "            val_f1 = f1_score(true_classes.cpu(), predicted_classes.cpu(), average='weighted')\n",
        "\n",
        "            # Update history\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_accuracy'].append(val_accuracy)\n",
        "            history['val_f1_score'].append(val_f1)\n",
        "\n",
        "            print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, F1 Score: {val_f1:.4f}\")\n",
        "\n",
        "            # Save checkpoint if validation loss improves\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                new_model_path = f'mlp_best_model_epoch_{epoch+1}.pth'\n",
        "\n",
        "                # Remove the previous best model\n",
        "                if best_model_path is not None:\n",
        "                    os.remove(best_model_path)\n",
        "\n",
        "                # Save the new best model\n",
        "                torch.save(model.state_dict(), new_model_path)\n",
        "                best_model_path = new_model_path\n",
        "\n",
        "        end_training_time = time.time()\n",
        "        print(f\"Training complete! Total time: {end_training_time - start_training_time:.2f} seconds\")\n",
        "\n",
        "        return history, best_model_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7YIZTCgnc8l"
      },
      "outputs": [],
      "source": [
        "class Classifier_Wrapper:\n",
        "    def __init__(self, model, optimizer, criterion, device, epochs):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = criterion\n",
        "        self.device = device\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, data_loader):\n",
        "        \"\"\"\n",
        "        Train the model for the specified number of epochs.\n",
        "\n",
        "        Parameters:\n",
        "        - data_loader: DataLoader, DataLoader for training data\n",
        "\n",
        "        Returns:\n",
        "        - history: dict, containing loss for each epoch\n",
        "        \"\"\"\n",
        "        self.model.to(self.device)\n",
        "        history = {'loss': []}\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.model.train()\n",
        "            running_loss = 0.0\n",
        "\n",
        "            for features, labels in data_loader:\n",
        "                features, labels = features.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(features)\n",
        "                # loss = self.criterion(outputs, labels)\n",
        "                criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
        "                loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            avg_loss = running_loss / len(data_loader)\n",
        "            history['loss'].append(avg_loss)\n",
        "            print(f\"Epoch [{epoch + 1}/{self.epochs}] - Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for samples in X.\n",
        "\n",
        "        Parameters:\n",
        "        - X: array-like of shape (n_samples, n_features), input features.\n",
        "\n",
        "        Returns:\n",
        "        - predictions: np.ndarray of shape (n_samples,), predicted class labels.\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.eval()\n",
        "        X_tensor = torch.tensor(X, dtype=torch.long).to(self.device)\n",
        "        predictions = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            outputs = self.model(X_tensor)\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "            predictions = predicted.cpu().numpy()\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Predict class probabilities for samples in X.\n",
        "\n",
        "        Parameters:\n",
        "        - X: array-like of shape (n_samples, n_features), input features.\n",
        "\n",
        "        Returns:\n",
        "        - probabilities: np.ndarray of shape (n_samples, n_classes), predicted probabilities.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
        "        probabilities = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X_tensor)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            probabilities = probs.cpu().numpy()\n",
        "\n",
        "        return probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1Hs0NdPnc8m"
      },
      "source": [
        "## Loading and Parsing Universal Dependencies Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1PCOVajnc8m"
      },
      "source": [
        "We developed a part-of-speech (POS) tagger for the English EWT dataset from the Universal Dependencies treebanks. The objective was to create a system that accurately assigns POS tags to words in sentences, using only the words, sentences, and POS tags available in the dataset. Additional annotations, such as syntactic dependencies, were not included in this task.\n",
        "\n",
        "In this section, we are loading and parsing the `.conllu` files in the specified directory using the `pyconll` library to extract useful linguistic annotations (e.g., word forms and POS tags).\n",
        "\n",
        "The variables `UD_EN_TRAIN`, `UD_EN_DEV`, and `UD_EN_TEST` specify the filenames for the training, development, and test datasets. The `base` variable contains the directory where these `.conllu` files are stored.\n",
        "\n",
        "Then, The parsing function (`parse_conllu`) is called for each dataset (training, validation, and test), and the processed data is stored in `train_sentences`, `val_sentences`, and `test_sentences`. Each dataset will be a list of tokenized and annotated sentences. For example:\n",
        "     \n",
        "   ```python\n",
        "   [\n",
        "      [('the', 'DET'), ('cat', 'NOUN'), ('sleeps', 'VERB')],\n",
        "      [('a', 'DET'), ('dog', 'NOUN'), ('barked', 'VERB')]\n",
        "   ]\n",
        "   ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnL2ZdYenc8m",
        "outputId": "5e2b23f8-875a-4825-97e3-e6d92fbdc9e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ],
      "source": [
        "# Use the English Web Treebank (EWT) dataset\n",
        "UD_EN_TRAIN = 'en_ewt-ud-train.conllu'\n",
        "UD_EN_DEV = 'en_ewt-ud-dev.conllu'\n",
        "UD_EN_TEST = 'en_ewt-ud-test.conllu'\n",
        "\n",
        "base = '/home/annachatz/ud-treebanks-v2.15/UD_English-EWT/'\n",
        "\n",
        "# Should return True\n",
        "print(os.path.exists(base + UD_EN_TRAIN))\n",
        "print(os.path.exists(base + UD_EN_DEV))\n",
        "print(os.path.exists(base + UD_EN_TEST))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3gGXkn5nc8m",
        "outputId": "d7a0526f-5109-4ca8-af7b-2f07dda80e3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 12544 sentences from /home/annachatz/ud-treebanks-v2.15/UD_English-EWT/en_ewt-ud-train.conllu\n",
            "Loaded 2001 sentences from /home/annachatz/ud-treebanks-v2.15/UD_English-EWT/en_ewt-ud-dev.conllu\n",
            "Loaded 2077 sentences from /home/annachatz/ud-treebanks-v2.15/UD_English-EWT/en_ewt-ud-test.conllu\n"
          ]
        }
      ],
      "source": [
        "train_sentences = UtilsPOSTagging.parse_conllu(base + UD_EN_TRAIN)\n",
        "dev_sentences = UtilsPOSTagging.parse_conllu(base + UD_EN_DEV)\n",
        "test_sentences = UtilsPOSTagging.parse_conllu(base + UD_EN_TEST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M10MyiXsnc8m"
      },
      "source": [
        "### Statistics about the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qnPT5tRnc8n",
        "outputId": "6d811186-00cc-4820-c11f-573f5cc6e2ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Data Statistics:\n",
            "  Number of Sentences: 12544\n",
            "  Number of Words: 204609\n",
            "  Average Sentence Length: 16.31\n",
            "\n",
            "Development Data Statistics:\n",
            "  Number of Sentences: 2001\n",
            "  Number of Words: 25152\n",
            "  Average Sentence Length: 12.57\n",
            "\n",
            "Test Data Statistics:\n",
            "  Number of Sentences: 2077\n",
            "  Number of Words: 25096\n",
            "  Average Sentence Length: 12.08\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate statistics for each dataset\n",
        "train_stats = UtilsPOSTagging.calculate_statistics(train_sentences, 'Training')\n",
        "dev_stats = UtilsPOSTagging.calculate_statistics(dev_sentences, 'Development')\n",
        "test_stats = UtilsPOSTagging.calculate_statistics(test_sentences, 'Test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tghn5-KGnc8n",
        "outputId": "1927e211-c262-4108-adb5-a874b56dfeb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Randomly selected examples\n",
            "1. Sentence: ([('if', 'SCONJ'), ('these', 'DET'), ('amendmnets', 'NOUN'), ('are', 'AUX'), ('agreeable', 'ADJ'), ('to', 'ADP'), ('you', 'PRON'), ('can', 'AUX'), ('you', 'PRON'), ('please', 'INTJ'), ('print', 'VERB'), ('and', 'CCONJ'), ('arrange', 'VERB'), ('for', 'ADP'), ('execution', 'NOUN'), ('.', 'PUNCT')],)\n",
            "2. Sentence: ([('currency', 'NOUN'), (':', 'PUNCT')],)\n",
            "3. Sentence: ([('the', 'DET'), ('difficulty', 'NOUN'), ('lies', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('fact', 'NOUN'), ('that', 'SCONJ'), ('the', 'DET'), ('confirmation', 'NOUN'), ('system', 'NOUN'), ('reads', 'VERB'), ('the', 'DET'), ('counterparty', 'NOUN'), (\"'s\", 'PART'), ('name', 'NOUN'), ('from', 'ADP'), ('global', 'ADJ'), ('counterparty', 'NOUN'), ('.', 'PUNCT')],)\n",
            "4. Sentence: ([('thanks', 'NOUN'), ('in', 'ADP'), ('advance', 'NOUN'), ('.', 'PUNCT')],)\n",
            "5. Sentence: ([('this', 'DET'), ('message', 'NOUN'), ('is', 'AUX'), ('confidential', 'ADJ'), ('.', 'PUNCT')],)\n"
          ]
        }
      ],
      "source": [
        "# Randomly select 5 examples to display\n",
        "random_indices = np.random.randint(0, len(train_sentences)-1, size=5)\n",
        "selected_examples = [train_sentences[i] for i in random_indices]\n",
        "\n",
        "print(\"\\nRandomly selected examples\")\n",
        "\n",
        "for i, sent in enumerate(zip(selected_examples)):\n",
        "    print(f\"{i+1}. Sentence:\", sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4O3xTGWnc8n"
      },
      "source": [
        "## Preparing Features and Labels (Tags) for POS Tagging with BILSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FFRf-CEnc8n"
      },
      "source": [
        "We prepare data for training a Bidirectional Long Short-Term Memory (BILSTM) model to predict Part-of-Speech (POS) tags using sliding window word embeddings.\n",
        "\n",
        "We use the pre-trained `word2vec-google-news-300` model from Gensim's API to obtain 300-dimensional word embeddings for the words in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MvzEKHOnc8n"
      },
      "outputs": [],
      "source": [
        "word2vec_model = api.load(\"word2vec-google-news-300\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkM125kenc8o"
      },
      "source": [
        "**Processing Sentences**:\n",
        "   - The `UtilsPOSTagging.words` function extracts words from the input sentences, and `UtilsPOSTagging.tags` extracts the corresponding POS tags.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkKpMjiAnc8o"
      },
      "outputs": [],
      "source": [
        "# Process the train set\n",
        "train_as_words = UtilsPOSTagging.words(train_sentences)\n",
        "train_as_tags = UtilsPOSTagging.tags(train_sentences)\n",
        "train_as_embed = [[UtilsPOSTagging.get_word_embedding(word, word2vec_model, word2vec_model.vector_size) for word in sentence] for sentence in train_as_words]\n",
        "\n",
        "# Process the dev set\n",
        "dev_as_words = UtilsPOSTagging.words(dev_sentences)\n",
        "dev_as_tags = UtilsPOSTagging.tags(dev_sentences)\n",
        "dev_as_embed = [[UtilsPOSTagging.get_word_embedding(word, word2vec_model, word2vec_model.vector_size) for word in sentence] for sentence in dev_as_words]\n",
        "\n",
        "# Process the test set\n",
        "test_as_words = UtilsPOSTagging.words(test_sentences)\n",
        "test_as_tags = UtilsPOSTagging.tags(test_sentences)\n",
        "test_as_embed = [[UtilsPOSTagging.get_word_embedding(word, word2vec_model, word2vec_model.vector_size) for word in sentence] for sentence in test_as_words]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3hbWJ1Snc8o"
      },
      "source": [
        "## Vocabulary Creation\n",
        "\n",
        "- **Word Vocabulary**: Extract all unique words from the training dataset to create a `word2idx` mapping.\n",
        "  - Special tokens include:\n",
        "    - `UNK` (unknown words).\n",
        "    - `PAD` (padding).\n",
        "- **Tag Vocabulary**: Extract all unique POS tags from the training tag sequences to create a `tag2idx` mapping.\n",
        "  - Includes a special `PAD` tag.\n",
        "- **Reverse Tag Mapping**: A reverse mapping (`idx2tag`) is created to decode tag indices back into their corresponding tags.\n",
        "\n",
        "---\n",
        "\n",
        "## Encoding Sequences\n",
        "\n",
        "- Word and tag sequences are converted into numerical indices using the `word2idx` and `tag2idx` mappings.\n",
        "- For words not present in the vocabulary, the `UNK` token index is used.\n",
        "- This step ensures that the text data is compatible with the numerical computations of the model.\n",
        "\n",
        "---\n",
        "\n",
        "## Padding Sequences\n",
        "\n",
        "- Since sentences in the dataset vary in length, padding is applied to make all sequences equal in length.\n",
        "- **Padding Details**:\n",
        "  - `PAD` token is used for both words and tags.\n",
        "  - A fixed maximum length (`max_len`) is defined to standardize the sequence lengths.\n",
        "  - Sequences longer than `max_len` are truncated, while shorter ones are padded.\n",
        "\n",
        "---\n",
        "\n",
        "## One-Hot Encoding of Tags\n",
        "\n",
        "- The padded tag sequences are converted to a categorical format (one-hot encoded) to match the output requirements of the model.\n",
        "- Each tag is represented as a vector with a length equal to the total number of unique tags.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXQiStVLnc8o"
      },
      "outputs": [],
      "source": [
        "# Build vocabularies\n",
        "words = {word for sentence in train_as_words for word in sentence}\n",
        "tags = {tag for tag_seq in train_as_tags for tag in tag_seq}\n",
        "\n",
        "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
        "word2idx[\"UNK\"] = 1  # Unknown token\n",
        "word2idx[\"PAD\"] = 0  # Padding token\n",
        "\n",
        "tag2idx = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2idx[\"PAD\"] = 0  # Padding tag\n",
        "\n",
        "idx2tag = {i: t for t, i in tag2idx.items()}\n",
        "\n",
        "# Convert tokens and tags to sequences of indices\n",
        "def encode_sequences(sequences, mapping, unknown_key=\"UNK\"):\n",
        "    if unknown_key in mapping:\n",
        "        return [[mapping.get(item, mapping[unknown_key]) for item in seq] for seq in sequences]\n",
        "    else:\n",
        "        return [[mapping[item] if item in mapping else 0 for item in seq] for seq in sequences]\n",
        "\n",
        "X_train_encoded = encode_sequences(train_as_words, word2idx)\n",
        "X_dev_encoded = encode_sequences(dev_as_words, word2idx)\n",
        "X_test_encoded = encode_sequences(test_as_words, word2idx)\n",
        "\n",
        "y_train_encoded = encode_sequences(train_as_tags, tag2idx)\n",
        "y_dev_encoded = encode_sequences(dev_as_tags, tag2idx)\n",
        "y_test_encoded = encode_sequences(test_as_tags, tag2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMPuZpbenc8p"
      },
      "outputs": [],
      "source": [
        "class TokenClassificationDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.X[idx]), torch.tensor(self.y[idx])\n",
        "\n",
        "\n",
        "\n",
        "PAD_INDEX = 0  # Padding token index\n",
        "\n",
        "# Pad sequences\n",
        "max_len = 150\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_encoded, maxlen=max_len, padding=\"post\", value=PAD_INDEX)\n",
        "X_dev_padded = pad_sequences(X_dev_encoded, maxlen=max_len, padding=\"post\", value=PAD_INDEX)\n",
        "X_test_padded = pad_sequences(X_test_encoded, maxlen=max_len, padding=\"post\", value=PAD_INDEX)\n",
        "\n",
        "y_train_padded = pad_sequences(y_train_encoded, maxlen=max_len, padding=\"post\", value=PAD_INDEX)\n",
        "y_dev_padded = pad_sequences(y_dev_encoded, maxlen=max_len, padding=\"post\", value=PAD_INDEX)\n",
        "y_test_padded = pad_sequences(y_test_encoded, maxlen=max_len, padding=\"post\", value=PAD_INDEX)\n",
        "\n",
        "# Convert y to categorical\n",
        "y_train_padded = to_categorical(y_train_padded, num_classes=len(tag2idx))\n",
        "y_dev_padded = to_categorical(y_dev_padded, num_classes=len(tag2idx))\n",
        "y_test_padded = to_categorical(y_test_padded, num_classes=len(tag2idx))\n",
        "\n",
        "train_dataset = TokenClassificationDataset(X_train_padded, y_train_padded.argmax(-1))\n",
        "dev_dataset = TokenClassificationDataset(X_dev_padded, y_dev_padded.argmax(-1))\n",
        "test_dataset = TokenClassificationDataset(X_test_padded, y_test_padded.argmax(-1))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWP65dK4nc8p"
      },
      "outputs": [],
      "source": [
        "embedding_dim = word2vec_model.vector_size\n",
        "embedding_matrix = np.zeros((len(word2idx), embedding_dim))\n",
        "\n",
        "for word, idx in word2idx.items():\n",
        "    if word in word2vec_model:\n",
        "        embedding_matrix[idx] = word2vec_model[word]\n",
        "    else:\n",
        "        # Initialize with zeros for unknown words\n",
        "        embedding_matrix[idx] = np.zeros(embedding_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTfxQ39Pnc8p"
      },
      "source": [
        "## Training and Testing for BILSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXt4PQXjnc8p",
        "outputId": "e0eb31c0-9d92-43ee-be32-ec6ca5ad856b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XyOEOdFnc8q",
        "outputId": "a3e7ad36-f1ce-4610-ce97-3079fd528f6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BiLSTM_Attention(\n",
              "  (embeddings): Embedding(16658, 300, padding_idx=0)\n",
              "  (bilstm): LSTM(300, 128, batch_first=True, bidirectional=True)\n",
              "  (dropout_emb): Dropout(p=0.1, inplace=False)\n",
              "  (dropout_att): Dropout(p=0.1, inplace=False)\n",
              "  (dropout_out): Dropout(p=0.1, inplace=False)\n",
              "  (deep_attention_mlp): Sequential(\n",
              "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (1): Tanh()\n",
              "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (3): Tanh()\n",
              "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (5): Tanh()\n",
              "    (6): Linear(in_features=64, out_features=1, bias=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=256, out_features=18, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = BiLSTM_Attention(\n",
        "    input_dim=embedding_dim,  # Word2Vec embedding size\n",
        "    n_classes=len(tag2idx),\n",
        "    dropout_prob_emb=0.1,\n",
        "    dropout_prob_att=0.1,\n",
        "    dropout_prob_out=0.1,\n",
        "    hidden_dim=128,\n",
        "    lstm_hidden_dim=128,\n",
        "    lstm_stacks=1,\n",
        "    attention_hidden_sizes=[256,128,64],\n",
        "    max_words=len(word2idx),\n",
        "    matrix_embeddings=embedding_matrix\n",
        ")\n",
        "model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy5Pmgianc8q"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "epochs=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlebkJg9nc8q",
        "outputId": "ddae2b9e-7523-4e33-e1d8-09b2b04103bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Average Loss: 0.6805\n",
            "Epoch [2/20], Average Loss: 0.6200\n",
            "Epoch [3/20], Average Loss: 0.5756\n",
            "Epoch [4/20], Average Loss: 0.5391\n",
            "Epoch [5/20], Average Loss: 0.5089\n",
            "Epoch [6/20], Average Loss: 0.4830\n",
            "Epoch [7/20], Average Loss: 0.4640\n",
            "Epoch [8/20], Average Loss: 0.4424\n",
            "Epoch [9/20], Average Loss: 0.4250\n",
            "Epoch [10/20], Average Loss: 0.4093\n",
            "Epoch [11/20], Average Loss: 0.3960\n",
            "Epoch [12/20], Average Loss: 0.3824\n",
            "Epoch [13/20], Average Loss: 0.3698\n",
            "Epoch [14/20], Average Loss: 0.3263\n",
            "Epoch [15/20], Average Loss: 0.2893\n",
            "Epoch [16/20], Average Loss: 0.2640\n",
            "Epoch [17/20], Average Loss: 0.2473\n",
            "Epoch [18/20], Average Loss: 0.2351\n",
            "Epoch [19/20], Average Loss: 0.2240\n",
            "Epoch [20/20], Average Loss: 0.2143\n",
            "Test Accuracy: 0.8916\n",
            "Test F1 Score: 0.8109\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate the BILSTM Classifier (Before hyperparameter tuning)\n",
        "test_accuracy, macro_f1 = UtilsBILSTM.train_and_evaluate(model, optimizer, criterion, device, epochs, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbqwojmlnc8r"
      },
      "source": [
        "# Hyperparameter Tuning for BiLSTM Model\n",
        "\n",
        "---\n",
        "\n",
        "## Hyperparameters to Tune\n",
        "\n",
        "### 1. **LSTM Hidden Layer Size (`lstm_hidden_dim`)**\n",
        "   - Represents the number of hidden units in the LSTM layers.\n",
        "   - **Values to Test**: `[128, 256]`.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Dropout Probabilities**\n",
        "Dropout is a regularization technique to prevent overfitting by randomly deactivating neurons during training. We tune the dropout rates for the following components:\n",
        "   - **Embeddings (`dropout_prob_emb`)**: Dropout applied after the embedding layer.\n",
        "   - **Attention Layer (`dropout_prob_att`)**: Dropout applied in the attention mechanism.\n",
        "   - **Output Layer (`dropout_prob_out`)**: Dropout applied before the final prediction.\n",
        "   - **Values to Test**: `[0.1, 0.2]`.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Attention MLP Hidden Sizes (`attention_hidden_sizes`)**\n",
        "   - Defines the architecture of the MLP used in the attention mechanism.\n",
        "   - **Values to Test**: `[[128, 64], [256, 128]]`.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Learning Rate (`learning_rate`)**\n",
        "   - Controls the step size for weight updates during optimization.\n",
        "   - Smaller learning rates provide more precise updates but require longer training.\n",
        "   - **Values to Test**: `[0.01, 0.001, 0.0001]`.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Number of LSTM Layers (`lstm_stacks`)**\n",
        "   - Specifies the number of stacked LSTM layers in the model.\n",
        "   - **Values to Test**: `[1, 2]`.\n",
        "\n",
        "---\n",
        "\n",
        "## Tuning Procedure\n",
        "\n",
        "1. **Grid Search**:\n",
        "   - A grid of all possible combinations of the hyperparameters is generated.\n",
        "   - Each combination is tested to evaluate its performance on the development dataset.\n",
        "\n",
        "2. **Evaluation Metric**:\n",
        "   - The **macro F1 score** on the development set is used as the primary metric to assess model performance.\n",
        "   - Higher macro F1 scores indicate better overall performance.\n",
        "\n",
        "3. **Best Model Selection**:\n",
        "   - The combination of hyperparameters that yields the highest macro F1 score on the development dataset is selected as the best configuration.\n",
        "\n",
        "---\n",
        "\n",
        "## Key Steps in the Process\n",
        "\n",
        "1. **Initialize Model**:\n",
        "   - For each combination of hyperparameters, a new instance of the `BiLSTM_Attention` model is created with the specified values.\n",
        "\n",
        "2. **Training**:\n",
        "   - The model is trained using the Adam optimizer and cross-entropy loss, ignoring padding tokens.\n",
        "\n",
        "3. **Evaluation**:\n",
        "   - After training, the model is evaluated on the development set to compute accuracy and macro F1 score.\n",
        "\n",
        "4. **Update Best Parameters**:\n",
        "   - If the current configuration achieves a higher macro F1 score than the previous best, the parameters are updated.\n",
        "\n",
        "---\n",
        "\n",
        "## Output\n",
        "\n",
        "At the end of the process, the best-performing hyperparameters and the corresponding macro F1 score are reported:\n",
        "- **Best Hyperparameters**: The combination of values that resulted in the best development set performance.\n",
        "- **Best Development Macro F1 Score**: The highest macro F1 score achieved during the grid search.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXApQwlpnc8s",
        "outputId": "b29613d3-9b58-4329-9ef4-537b5d957dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.3124\n",
            "Epoch [2/10], Average Loss: 0.6457\n",
            "Epoch [3/10], Average Loss: 0.4380\n",
            "Epoch [4/10], Average Loss: 0.3483\n",
            "Epoch [5/10], Average Loss: 0.2896\n",
            "Epoch [6/10], Average Loss: 0.2629\n",
            "Epoch [7/10], Average Loss: 0.2375\n",
            "Epoch [8/10], Average Loss: 0.2141\n",
            "Epoch [9/10], Average Loss: 0.1998\n",
            "Epoch [10/10], Average Loss: 0.1892\n",
            "Test Accuracy: 0.8929\n",
            "Test F1 Score: 0.8137\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.1153\n",
            "Epoch [2/10], Average Loss: 0.3253\n",
            "Epoch [3/10], Average Loss: 0.1970\n",
            "Epoch [4/10], Average Loss: 0.1570\n",
            "Epoch [5/10], Average Loss: 0.1279\n",
            "Epoch [6/10], Average Loss: 0.1078\n",
            "Epoch [7/10], Average Loss: 0.0949\n",
            "Epoch [8/10], Average Loss: 0.0884\n",
            "Epoch [9/10], Average Loss: 0.0811\n",
            "Epoch [10/10], Average Loss: 0.0763\n",
            "Test Accuracy: 0.9135\n",
            "Test F1 Score: 0.8433\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.1158\n",
            "Epoch [2/10], Average Loss: 0.5177\n",
            "Epoch [3/10], Average Loss: 0.3916\n",
            "Epoch [4/10], Average Loss: 0.3214\n",
            "Epoch [5/10], Average Loss: 0.2719\n",
            "Epoch [6/10], Average Loss: 0.2518\n",
            "Epoch [7/10], Average Loss: 0.2293\n",
            "Epoch [8/10], Average Loss: 0.2106\n",
            "Epoch [9/10], Average Loss: 0.2190\n",
            "Epoch [10/10], Average Loss: 0.3078\n",
            "Test Accuracy: 0.8854\n",
            "Test F1 Score: 0.7995\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.0260\n",
            "Epoch [2/10], Average Loss: 0.2794\n",
            "Epoch [3/10], Average Loss: 0.1807\n",
            "Epoch [4/10], Average Loss: 0.1405\n",
            "Epoch [5/10], Average Loss: 0.1139\n",
            "Epoch [6/10], Average Loss: 0.0981\n",
            "Epoch [7/10], Average Loss: 0.0835\n",
            "Epoch [8/10], Average Loss: 0.0756\n",
            "Epoch [9/10], Average Loss: 0.0726\n",
            "Epoch [10/10], Average Loss: 0.0648\n",
            "Test Accuracy: 0.9166\n",
            "Test F1 Score: 0.8387\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.3056\n",
            "Epoch [2/10], Average Loss: 1.7962\n",
            "Epoch [3/10], Average Loss: 1.5465\n",
            "Epoch [4/10], Average Loss: 1.3845\n",
            "Epoch [5/10], Average Loss: 1.2397\n",
            "Epoch [6/10], Average Loss: 1.1150\n",
            "Epoch [7/10], Average Loss: 1.0079\n",
            "Epoch [8/10], Average Loss: 0.9281\n",
            "Epoch [9/10], Average Loss: 0.8664\n",
            "Epoch [10/10], Average Loss: 0.8102\n",
            "Test Accuracy: 0.7543\n",
            "Test F1 Score: 0.5281\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.2855\n",
            "Epoch [2/10], Average Loss: 1.5229\n",
            "Epoch [3/10], Average Loss: 1.1782\n",
            "Epoch [4/10], Average Loss: 0.9943\n",
            "Epoch [5/10], Average Loss: 0.8485\n",
            "Epoch [6/10], Average Loss: 0.7321\n",
            "Epoch [7/10], Average Loss: 0.6449\n",
            "Epoch [8/10], Average Loss: 0.5551\n",
            "Epoch [9/10], Average Loss: 0.4567\n",
            "Epoch [10/10], Average Loss: 0.3887\n",
            "Test Accuracy: 0.8790\n",
            "Test F1 Score: 0.7764\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2043\n",
            "Epoch [2/10], Average Loss: 1.6305\n",
            "Epoch [3/10], Average Loss: 1.3064\n",
            "Epoch [4/10], Average Loss: 1.1204\n",
            "Epoch [5/10], Average Loss: 0.9586\n",
            "Epoch [6/10], Average Loss: 0.8379\n",
            "Epoch [7/10], Average Loss: 0.7462\n",
            "Epoch [8/10], Average Loss: 0.6746\n",
            "Epoch [9/10], Average Loss: 0.6113\n",
            "Epoch [10/10], Average Loss: 0.5274\n",
            "Test Accuracy: 0.8415\n",
            "Test F1 Score: 0.6903\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.0880\n",
            "Epoch [2/10], Average Loss: 1.2120\n",
            "Epoch [3/10], Average Loss: 0.7547\n",
            "Epoch [4/10], Average Loss: 0.5519\n",
            "Epoch [5/10], Average Loss: 0.4216\n",
            "Epoch [6/10], Average Loss: 0.3297\n",
            "Epoch [7/10], Average Loss: 0.2622\n",
            "Epoch [8/10], Average Loss: 0.2170\n",
            "Epoch [9/10], Average Loss: 0.1814\n",
            "Epoch [10/10], Average Loss: 0.1539\n",
            "Test Accuracy: 0.9052\n",
            "Test F1 Score: 0.8312\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6579\n",
            "Epoch [2/10], Average Loss: 2.3937\n",
            "Epoch [3/10], Average Loss: 2.2971\n",
            "Epoch [4/10], Average Loss: 2.2391\n",
            "Epoch [5/10], Average Loss: 2.1940\n",
            "Epoch [6/10], Average Loss: 2.1544\n",
            "Epoch [7/10], Average Loss: 2.1176\n",
            "Epoch [8/10], Average Loss: 2.0801\n",
            "Epoch [9/10], Average Loss: 2.0454\n",
            "Epoch [10/10], Average Loss: 2.0120\n",
            "Test Accuracy: 0.3839\n",
            "Test F1 Score: 0.1577\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6847\n",
            "Epoch [2/10], Average Loss: 2.3938\n",
            "Epoch [3/10], Average Loss: 2.2860\n",
            "Epoch [4/10], Average Loss: 2.2223\n",
            "Epoch [5/10], Average Loss: 2.1716\n",
            "Epoch [6/10], Average Loss: 2.1294\n",
            "Epoch [7/10], Average Loss: 2.0904\n",
            "Epoch [8/10], Average Loss: 2.0489\n",
            "Epoch [9/10], Average Loss: 2.0035\n",
            "Epoch [10/10], Average Loss: 1.9525\n",
            "Test Accuracy: 0.4183\n",
            "Test F1 Score: 0.1846\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6008\n",
            "Epoch [2/10], Average Loss: 2.3694\n",
            "Epoch [3/10], Average Loss: 2.2836\n",
            "Epoch [4/10], Average Loss: 2.2093\n",
            "Epoch [5/10], Average Loss: 2.1509\n",
            "Epoch [6/10], Average Loss: 2.1016\n",
            "Epoch [7/10], Average Loss: 2.0546\n",
            "Epoch [8/10], Average Loss: 2.0082\n",
            "Epoch [9/10], Average Loss: 1.9607\n",
            "Epoch [10/10], Average Loss: 1.9101\n",
            "Test Accuracy: 0.4266\n",
            "Test F1 Score: 0.1927\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6214\n",
            "Epoch [2/10], Average Loss: 2.3027\n",
            "Epoch [3/10], Average Loss: 2.1731\n",
            "Epoch [4/10], Average Loss: 2.0858\n",
            "Epoch [5/10], Average Loss: 1.9824\n",
            "Epoch [6/10], Average Loss: 1.8675\n",
            "Epoch [7/10], Average Loss: 1.7694\n",
            "Epoch [8/10], Average Loss: 1.6848\n",
            "Epoch [9/10], Average Loss: 1.6146\n",
            "Epoch [10/10], Average Loss: 1.5490\n",
            "Test Accuracy: 0.5640\n",
            "Test F1 Score: 0.2982\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.3564\n",
            "Epoch [2/10], Average Loss: 0.6700\n",
            "Epoch [3/10], Average Loss: 0.4334\n",
            "Epoch [4/10], Average Loss: 0.3446\n",
            "Epoch [5/10], Average Loss: 0.2894\n",
            "Epoch [6/10], Average Loss: 0.2566\n",
            "Epoch [7/10], Average Loss: 0.2317\n",
            "Epoch [8/10], Average Loss: 0.2110\n",
            "Epoch [9/10], Average Loss: 0.1980\n",
            "Epoch [10/10], Average Loss: 0.1879\n",
            "Test Accuracy: 0.9107\n",
            "Test F1 Score: 0.8271\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.3938\n",
            "Epoch [2/10], Average Loss: 0.4151\n",
            "Epoch [3/10], Average Loss: 0.2466\n",
            "Epoch [4/10], Average Loss: 0.1872\n",
            "Epoch [5/10], Average Loss: 0.1513\n",
            "Epoch [6/10], Average Loss: 0.1296\n",
            "Epoch [7/10], Average Loss: 0.1106\n",
            "Epoch [8/10], Average Loss: 0.0958\n",
            "Epoch [9/10], Average Loss: 0.0893\n",
            "Epoch [10/10], Average Loss: 0.0772\n",
            "Test Accuracy: 0.9117\n",
            "Test F1 Score: 0.8341\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.6444\n",
            "Epoch [2/10], Average Loss: 0.7501\n",
            "Epoch [3/10], Average Loss: 0.5930\n",
            "Epoch [4/10], Average Loss: 0.4941\n",
            "Epoch [5/10], Average Loss: 0.5025\n",
            "Epoch [6/10], Average Loss: 0.3919\n",
            "Epoch [7/10], Average Loss: 0.3430\n",
            "Epoch [8/10], Average Loss: 0.3145\n",
            "Epoch [9/10], Average Loss: 0.2843\n",
            "Epoch [10/10], Average Loss: 0.2698\n",
            "Test Accuracy: 0.8888\n",
            "Test F1 Score: 0.7935\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.0611\n",
            "Epoch [2/10], Average Loss: 0.2859\n",
            "Epoch [3/10], Average Loss: 0.1790\n",
            "Epoch [4/10], Average Loss: 0.1348\n",
            "Epoch [5/10], Average Loss: 0.1103\n",
            "Epoch [6/10], Average Loss: 0.0906\n",
            "Epoch [7/10], Average Loss: 0.0791\n",
            "Epoch [8/10], Average Loss: 0.0730\n",
            "Epoch [9/10], Average Loss: 0.0664\n",
            "Epoch [10/10], Average Loss: 0.0624\n",
            "Test Accuracy: 0.9167\n",
            "Test F1 Score: 0.8480\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2859\n",
            "Epoch [2/10], Average Loss: 1.7431\n",
            "Epoch [3/10], Average Loss: 1.4579\n",
            "Epoch [4/10], Average Loss: 1.3050\n",
            "Epoch [5/10], Average Loss: 1.1839\n",
            "Epoch [6/10], Average Loss: 1.0662\n",
            "Epoch [7/10], Average Loss: 0.9624\n",
            "Epoch [8/10], Average Loss: 0.8654\n",
            "Epoch [9/10], Average Loss: 0.7834\n",
            "Epoch [10/10], Average Loss: 0.7182\n",
            "Test Accuracy: 0.7459\n",
            "Test F1 Score: 0.4838\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.3020\n",
            "Epoch [2/10], Average Loss: 1.5001\n",
            "Epoch [3/10], Average Loss: 1.1556\n",
            "Epoch [4/10], Average Loss: 0.9569\n",
            "Epoch [5/10], Average Loss: 0.8095\n",
            "Epoch [6/10], Average Loss: 0.6999\n",
            "Epoch [7/10], Average Loss: 0.5861\n",
            "Epoch [8/10], Average Loss: 0.4981\n",
            "Epoch [9/10], Average Loss: 0.4251\n",
            "Epoch [10/10], Average Loss: 0.3604\n",
            "Test Accuracy: 0.8902\n",
            "Test F1 Score: 0.7890\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2571\n",
            "Epoch [2/10], Average Loss: 1.7083\n",
            "Epoch [3/10], Average Loss: 1.4327\n",
            "Epoch [4/10], Average Loss: 1.2187\n",
            "Epoch [5/10], Average Loss: 1.0175\n",
            "Epoch [6/10], Average Loss: 0.8449\n",
            "Epoch [7/10], Average Loss: 0.7429\n",
            "Epoch [8/10], Average Loss: 0.6446\n",
            "Epoch [9/10], Average Loss: 0.5622\n",
            "Epoch [10/10], Average Loss: 0.5003\n",
            "Test Accuracy: 0.8466\n",
            "Test F1 Score: 0.7031\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.1352\n",
            "Epoch [2/10], Average Loss: 1.1032\n",
            "Epoch [3/10], Average Loss: 0.7155\n",
            "Epoch [4/10], Average Loss: 0.5335\n",
            "Epoch [5/10], Average Loss: 0.4086\n",
            "Epoch [6/10], Average Loss: 0.3224\n",
            "Epoch [7/10], Average Loss: 0.2588\n",
            "Epoch [8/10], Average Loss: 0.2147\n",
            "Epoch [9/10], Average Loss: 0.1828\n",
            "Epoch [10/10], Average Loss: 0.1549\n",
            "Test Accuracy: 0.9113\n",
            "Test F1 Score: 0.8322\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6762\n",
            "Epoch [2/10], Average Loss: 2.3941\n",
            "Epoch [3/10], Average Loss: 2.2971\n",
            "Epoch [4/10], Average Loss: 2.2385\n",
            "Epoch [5/10], Average Loss: 2.1927\n",
            "Epoch [6/10], Average Loss: 2.1484\n",
            "Epoch [7/10], Average Loss: 2.1075\n",
            "Epoch [8/10], Average Loss: 2.0689\n",
            "Epoch [9/10], Average Loss: 2.0325\n",
            "Epoch [10/10], Average Loss: 1.9971\n",
            "Test Accuracy: 0.3901\n",
            "Test F1 Score: 0.1610\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6959\n",
            "Epoch [2/10], Average Loss: 2.3865\n",
            "Epoch [3/10], Average Loss: 2.2629\n",
            "Epoch [4/10], Average Loss: 2.1982\n",
            "Epoch [5/10], Average Loss: 2.1506\n",
            "Epoch [6/10], Average Loss: 2.1091\n",
            "Epoch [7/10], Average Loss: 2.0702\n",
            "Epoch [8/10], Average Loss: 2.0263\n",
            "Epoch [9/10], Average Loss: 1.9738\n",
            "Epoch [10/10], Average Loss: 1.9217\n",
            "Test Accuracy: 0.4139\n",
            "Test F1 Score: 0.1775\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6052\n",
            "Epoch [2/10], Average Loss: 2.3759\n",
            "Epoch [3/10], Average Loss: 2.3042\n",
            "Epoch [4/10], Average Loss: 2.2517\n",
            "Epoch [5/10], Average Loss: 2.2047\n",
            "Epoch [6/10], Average Loss: 2.1530\n",
            "Epoch [7/10], Average Loss: 2.1056\n",
            "Epoch [8/10], Average Loss: 2.0643\n",
            "Epoch [9/10], Average Loss: 2.0257\n",
            "Epoch [10/10], Average Loss: 1.9875\n",
            "Test Accuracy: 0.4104\n",
            "Test F1 Score: 0.1708\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6212\n",
            "Epoch [2/10], Average Loss: 2.3270\n",
            "Epoch [3/10], Average Loss: 2.2162\n",
            "Epoch [4/10], Average Loss: 2.1317\n",
            "Epoch [5/10], Average Loss: 2.0522\n",
            "Epoch [6/10], Average Loss: 1.9613\n",
            "Epoch [7/10], Average Loss: 1.8566\n",
            "Epoch [8/10], Average Loss: 1.7580\n",
            "Epoch [9/10], Average Loss: 1.6757\n",
            "Epoch [10/10], Average Loss: 1.6014\n",
            "Test Accuracy: 0.5582\n",
            "Test F1 Score: 0.2829\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.3511\n",
            "Epoch [2/10], Average Loss: 0.6701\n",
            "Epoch [3/10], Average Loss: 0.4546\n",
            "Epoch [4/10], Average Loss: 0.3523\n",
            "Epoch [5/10], Average Loss: 0.2952\n",
            "Epoch [6/10], Average Loss: 0.2622\n",
            "Epoch [7/10], Average Loss: 0.2426\n",
            "Epoch [8/10], Average Loss: 0.2228\n",
            "Epoch [9/10], Average Loss: 0.2089\n",
            "Epoch [10/10], Average Loss: 0.1970\n",
            "Test Accuracy: 0.9090\n",
            "Test F1 Score: 0.8263\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.1606\n",
            "Epoch [2/10], Average Loss: 0.3395\n",
            "Epoch [3/10], Average Loss: 0.2232\n",
            "Epoch [4/10], Average Loss: 0.1753\n",
            "Epoch [5/10], Average Loss: 0.1465\n",
            "Epoch [6/10], Average Loss: 0.1288\n",
            "Epoch [7/10], Average Loss: 0.1162\n",
            "Epoch [8/10], Average Loss: 0.1072\n",
            "Epoch [9/10], Average Loss: 0.1009\n",
            "Epoch [10/10], Average Loss: 0.0932\n",
            "Test Accuracy: 0.9203\n",
            "Test F1 Score: 0.8507\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.3568\n",
            "Epoch [2/10], Average Loss: 0.6652\n",
            "Epoch [3/10], Average Loss: 0.5089\n",
            "Epoch [4/10], Average Loss: 0.4099\n",
            "Epoch [5/10], Average Loss: 0.3729\n",
            "Epoch [6/10], Average Loss: 0.3450\n",
            "Epoch [7/10], Average Loss: 0.3011\n",
            "Epoch [8/10], Average Loss: 0.2912\n",
            "Epoch [9/10], Average Loss: 0.2757\n",
            "Epoch [10/10], Average Loss: 0.2622\n",
            "Test Accuracy: 0.8987\n",
            "Test F1 Score: 0.8084\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.1160\n",
            "Epoch [2/10], Average Loss: 0.2949\n",
            "Epoch [3/10], Average Loss: 0.1951\n",
            "Epoch [4/10], Average Loss: 0.1524\n",
            "Epoch [5/10], Average Loss: 0.1291\n",
            "Epoch [6/10], Average Loss: 0.1081\n",
            "Epoch [7/10], Average Loss: 0.0971\n",
            "Epoch [8/10], Average Loss: 0.0873\n",
            "Epoch [9/10], Average Loss: 0.0824\n",
            "Epoch [10/10], Average Loss: 0.0792\n",
            "Test Accuracy: 0.9185\n",
            "Test F1 Score: 0.8509\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2766\n",
            "Epoch [2/10], Average Loss: 1.8773\n",
            "Epoch [3/10], Average Loss: 1.6820\n",
            "Epoch [4/10], Average Loss: 1.5238\n",
            "Epoch [5/10], Average Loss: 1.3168\n",
            "Epoch [6/10], Average Loss: 1.1672\n",
            "Epoch [7/10], Average Loss: 1.0593\n",
            "Epoch [8/10], Average Loss: 0.9725\n",
            "Epoch [9/10], Average Loss: 0.8921\n",
            "Epoch [10/10], Average Loss: 0.8013\n",
            "Test Accuracy: 0.7520\n",
            "Test F1 Score: 0.5285\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.2880\n",
            "Epoch [2/10], Average Loss: 1.5481\n",
            "Epoch [3/10], Average Loss: 1.1918\n",
            "Epoch [4/10], Average Loss: 1.0001\n",
            "Epoch [5/10], Average Loss: 0.8521\n",
            "Epoch [6/10], Average Loss: 0.7449\n",
            "Epoch [7/10], Average Loss: 0.6660\n",
            "Epoch [8/10], Average Loss: 0.5780\n",
            "Epoch [9/10], Average Loss: 0.4837\n",
            "Epoch [10/10], Average Loss: 0.4238\n",
            "Test Accuracy: 0.8738\n",
            "Test F1 Score: 0.7398\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2187\n",
            "Epoch [2/10], Average Loss: 1.6577\n",
            "Epoch [3/10], Average Loss: 1.3944\n",
            "Epoch [4/10], Average Loss: 1.1973\n",
            "Epoch [5/10], Average Loss: 1.0135\n",
            "Epoch [6/10], Average Loss: 0.8474\n",
            "Epoch [7/10], Average Loss: 0.7133\n",
            "Epoch [8/10], Average Loss: 0.6331\n",
            "Epoch [9/10], Average Loss: 0.5704\n",
            "Epoch [10/10], Average Loss: 0.5243\n",
            "Test Accuracy: 0.8318\n",
            "Test F1 Score: 0.6536\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.0762\n",
            "Epoch [2/10], Average Loss: 1.0878\n",
            "Epoch [3/10], Average Loss: 0.7140\n",
            "Epoch [4/10], Average Loss: 0.5416\n",
            "Epoch [5/10], Average Loss: 0.4284\n",
            "Epoch [6/10], Average Loss: 0.3388\n",
            "Epoch [7/10], Average Loss: 0.2773\n",
            "Epoch [8/10], Average Loss: 0.2340\n",
            "Epoch [9/10], Average Loss: 0.2001\n",
            "Epoch [10/10], Average Loss: 0.1737\n",
            "Test Accuracy: 0.9110\n",
            "Test F1 Score: 0.8315\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6703\n",
            "Epoch [2/10], Average Loss: 2.4173\n",
            "Epoch [3/10], Average Loss: 2.3255\n",
            "Epoch [4/10], Average Loss: 2.2627\n",
            "Epoch [5/10], Average Loss: 2.2149\n",
            "Epoch [6/10], Average Loss: 2.1730\n",
            "Epoch [7/10], Average Loss: 2.1367\n",
            "Epoch [8/10], Average Loss: 2.1007\n",
            "Epoch [9/10], Average Loss: 2.0643\n",
            "Epoch [10/10], Average Loss: 2.0303\n",
            "Test Accuracy: 0.3877\n",
            "Test F1 Score: 0.1590\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6997\n",
            "Epoch [2/10], Average Loss: 2.4489\n",
            "Epoch [3/10], Average Loss: 2.3042\n",
            "Epoch [4/10], Average Loss: 2.2257\n",
            "Epoch [5/10], Average Loss: 2.1734\n",
            "Epoch [6/10], Average Loss: 2.1305\n",
            "Epoch [7/10], Average Loss: 2.0914\n",
            "Epoch [8/10], Average Loss: 2.0525\n",
            "Epoch [9/10], Average Loss: 2.0127\n",
            "Epoch [10/10], Average Loss: 1.9687\n",
            "Test Accuracy: 0.4004\n",
            "Test F1 Score: 0.1617\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6056\n",
            "Epoch [2/10], Average Loss: 2.3703\n",
            "Epoch [3/10], Average Loss: 2.2870\n",
            "Epoch [4/10], Average Loss: 2.2183\n",
            "Epoch [5/10], Average Loss: 2.1618\n",
            "Epoch [6/10], Average Loss: 2.1144\n",
            "Epoch [7/10], Average Loss: 2.0691\n",
            "Epoch [8/10], Average Loss: 2.0270\n",
            "Epoch [9/10], Average Loss: 1.9847\n",
            "Epoch [10/10], Average Loss: 1.9434\n",
            "Test Accuracy: 0.4007\n",
            "Test F1 Score: 0.1730\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6130\n",
            "Epoch [2/10], Average Loss: 2.2971\n",
            "Epoch [3/10], Average Loss: 2.1884\n",
            "Epoch [4/10], Average Loss: 2.1103\n",
            "Epoch [5/10], Average Loss: 2.0347\n",
            "Epoch [6/10], Average Loss: 1.9265\n",
            "Epoch [7/10], Average Loss: 1.8156\n",
            "Epoch [8/10], Average Loss: 1.7210\n",
            "Epoch [9/10], Average Loss: 1.6415\n",
            "Epoch [10/10], Average Loss: 1.5735\n",
            "Test Accuracy: 0.5747\n",
            "Test F1 Score: 0.3051\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.2078\n",
            "Epoch [2/10], Average Loss: 0.6066\n",
            "Epoch [3/10], Average Loss: 0.4806\n",
            "Epoch [4/10], Average Loss: 0.3990\n",
            "Epoch [5/10], Average Loss: 0.3359\n",
            "Epoch [6/10], Average Loss: 0.2947\n",
            "Epoch [7/10], Average Loss: 0.2713\n",
            "Epoch [8/10], Average Loss: 0.2481\n",
            "Epoch [9/10], Average Loss: 0.2273\n",
            "Epoch [10/10], Average Loss: 0.2175\n",
            "Test Accuracy: 0.9035\n",
            "Test F1 Score: 0.8193\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.2973\n",
            "Epoch [2/10], Average Loss: 0.3756\n",
            "Epoch [3/10], Average Loss: 0.2356\n",
            "Epoch [4/10], Average Loss: 0.1852\n",
            "Epoch [5/10], Average Loss: 0.1556\n",
            "Epoch [6/10], Average Loss: 0.1352\n",
            "Epoch [7/10], Average Loss: 0.1175\n",
            "Epoch [8/10], Average Loss: 0.1076\n",
            "Epoch [9/10], Average Loss: 0.0987\n",
            "Epoch [10/10], Average Loss: 0.0940\n",
            "Test Accuracy: 0.9179\n",
            "Test F1 Score: 0.8478\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.1946\n",
            "Epoch [2/10], Average Loss: 0.5695\n",
            "Epoch [3/10], Average Loss: 0.4454\n",
            "Epoch [4/10], Average Loss: 0.3606\n",
            "Epoch [5/10], Average Loss: 0.3131\n",
            "Epoch [6/10], Average Loss: 0.2788\n",
            "Epoch [7/10], Average Loss: 0.2551\n",
            "Epoch [8/10], Average Loss: 0.2393\n",
            "Epoch [9/10], Average Loss: 0.2224\n",
            "Epoch [10/10], Average Loss: 0.2102\n",
            "Test Accuracy: 0.9081\n",
            "Test F1 Score: 0.8198\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.1121\n",
            "Epoch [2/10], Average Loss: 0.3129\n",
            "Epoch [3/10], Average Loss: 0.2073\n",
            "Epoch [4/10], Average Loss: 0.1609\n",
            "Epoch [5/10], Average Loss: 0.1334\n",
            "Epoch [6/10], Average Loss: 0.1159\n",
            "Epoch [7/10], Average Loss: 0.1002\n",
            "Epoch [8/10], Average Loss: 0.0925\n",
            "Epoch [9/10], Average Loss: 0.0842\n",
            "Epoch [10/10], Average Loss: 0.0764\n",
            "Test Accuracy: 0.9205\n",
            "Test F1 Score: 0.8431\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.3365\n",
            "Epoch [2/10], Average Loss: 1.9180\n",
            "Epoch [3/10], Average Loss: 1.7043\n",
            "Epoch [4/10], Average Loss: 1.5437\n",
            "Epoch [5/10], Average Loss: 1.3688\n",
            "Epoch [6/10], Average Loss: 1.1888\n",
            "Epoch [7/10], Average Loss: 1.0715\n",
            "Epoch [8/10], Average Loss: 0.9831\n",
            "Epoch [9/10], Average Loss: 0.9041\n",
            "Epoch [10/10], Average Loss: 0.8188\n",
            "Test Accuracy: 0.7570\n",
            "Test F1 Score: 0.5208\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.2812\n",
            "Epoch [2/10], Average Loss: 1.5445\n",
            "Epoch [3/10], Average Loss: 1.1909\n",
            "Epoch [4/10], Average Loss: 0.9360\n",
            "Epoch [5/10], Average Loss: 0.7640\n",
            "Epoch [6/10], Average Loss: 0.6652\n",
            "Epoch [7/10], Average Loss: 0.5839\n",
            "Epoch [8/10], Average Loss: 0.4973\n",
            "Epoch [9/10], Average Loss: 0.4240\n",
            "Epoch [10/10], Average Loss: 0.3767\n",
            "Test Accuracy: 0.8526\n",
            "Test F1 Score: 0.6863\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2416\n",
            "Epoch [2/10], Average Loss: 1.6918\n",
            "Epoch [3/10], Average Loss: 1.4180\n",
            "Epoch [4/10], Average Loss: 1.2051\n",
            "Epoch [5/10], Average Loss: 0.9849\n",
            "Epoch [6/10], Average Loss: 0.8478\n",
            "Epoch [7/10], Average Loss: 0.7716\n",
            "Epoch [8/10], Average Loss: 0.6831\n",
            "Epoch [9/10], Average Loss: 0.5898\n",
            "Epoch [10/10], Average Loss: 0.5174\n",
            "Test Accuracy: 0.8564\n",
            "Test F1 Score: 0.7061\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.0671\n",
            "Epoch [2/10], Average Loss: 1.1085\n",
            "Epoch [3/10], Average Loss: 0.7195\n",
            "Epoch [4/10], Average Loss: 0.5428\n",
            "Epoch [5/10], Average Loss: 0.4261\n",
            "Epoch [6/10], Average Loss: 0.3422\n",
            "Epoch [7/10], Average Loss: 0.2815\n",
            "Epoch [8/10], Average Loss: 0.2377\n",
            "Epoch [9/10], Average Loss: 0.2054\n",
            "Epoch [10/10], Average Loss: 0.1773\n",
            "Test Accuracy: 0.9126\n",
            "Test F1 Score: 0.8327\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6773\n",
            "Epoch [2/10], Average Loss: 2.4338\n",
            "Epoch [3/10], Average Loss: 2.3295\n",
            "Epoch [4/10], Average Loss: 2.2649\n",
            "Epoch [5/10], Average Loss: 2.2167\n",
            "Epoch [6/10], Average Loss: 2.1745\n",
            "Epoch [7/10], Average Loss: 2.1340\n",
            "Epoch [8/10], Average Loss: 2.0939\n",
            "Epoch [9/10], Average Loss: 2.0566\n",
            "Epoch [10/10], Average Loss: 2.0215\n",
            "Test Accuracy: 0.3740\n",
            "Test F1 Score: 0.1487\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6888\n",
            "Epoch [2/10], Average Loss: 2.4130\n",
            "Epoch [3/10], Average Loss: 2.3080\n",
            "Epoch [4/10], Average Loss: 2.2379\n",
            "Epoch [5/10], Average Loss: 2.1849\n",
            "Epoch [6/10], Average Loss: 2.1390\n",
            "Epoch [7/10], Average Loss: 2.0987\n",
            "Epoch [8/10], Average Loss: 2.0598\n",
            "Epoch [9/10], Average Loss: 2.0175\n",
            "Epoch [10/10], Average Loss: 1.9718\n",
            "Test Accuracy: 0.3899\n",
            "Test F1 Score: 0.1559\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6036\n",
            "Epoch [2/10], Average Loss: 2.3558\n",
            "Epoch [3/10], Average Loss: 2.2695\n",
            "Epoch [4/10], Average Loss: 2.2053\n",
            "Epoch [5/10], Average Loss: 2.1568\n",
            "Epoch [6/10], Average Loss: 2.1119\n",
            "Epoch [7/10], Average Loss: 2.0701\n",
            "Epoch [8/10], Average Loss: 2.0292\n",
            "Epoch [9/10], Average Loss: 1.9865\n",
            "Epoch [10/10], Average Loss: 1.9445\n",
            "Test Accuracy: 0.4164\n",
            "Test F1 Score: 0.1717\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6250\n",
            "Epoch [2/10], Average Loss: 2.4507\n",
            "Epoch [3/10], Average Loss: 2.3563\n",
            "Epoch [4/10], Average Loss: 2.0988\n",
            "Epoch [5/10], Average Loss: 1.9353\n",
            "Epoch [6/10], Average Loss: 1.7892\n",
            "Epoch [7/10], Average Loss: 1.6645\n",
            "Epoch [8/10], Average Loss: 1.5670\n",
            "Epoch [9/10], Average Loss: 1.4830\n",
            "Epoch [10/10], Average Loss: 1.3920\n",
            "Test Accuracy: 0.6297\n",
            "Test F1 Score: 0.3493\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.4939\n",
            "Epoch [2/10], Average Loss: 0.7508\n",
            "Epoch [3/10], Average Loss: 0.5161\n",
            "Epoch [4/10], Average Loss: 0.3942\n",
            "Epoch [5/10], Average Loss: 0.3335\n",
            "Epoch [6/10], Average Loss: 0.2973\n",
            "Epoch [7/10], Average Loss: 0.2658\n",
            "Epoch [8/10], Average Loss: 0.2424\n",
            "Epoch [9/10], Average Loss: 0.2252\n",
            "Epoch [10/10], Average Loss: 0.2089\n",
            "Test Accuracy: 0.9042\n",
            "Test F1 Score: 0.8277\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.1419\n",
            "Epoch [2/10], Average Loss: 0.3455\n",
            "Epoch [3/10], Average Loss: 0.2137\n",
            "Epoch [4/10], Average Loss: 0.1628\n",
            "Epoch [5/10], Average Loss: 0.1327\n",
            "Epoch [6/10], Average Loss: 0.1128\n",
            "Epoch [7/10], Average Loss: 0.1001\n",
            "Epoch [8/10], Average Loss: 0.0903\n",
            "Epoch [9/10], Average Loss: 0.0852\n",
            "Epoch [10/10], Average Loss: 0.0813\n",
            "Test Accuracy: 0.9175\n",
            "Test F1 Score: 0.8437\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.1180\n",
            "Epoch [2/10], Average Loss: 0.5201\n",
            "Epoch [3/10], Average Loss: 0.4097\n",
            "Epoch [4/10], Average Loss: 0.3349\n",
            "Epoch [5/10], Average Loss: 0.2907\n",
            "Epoch [6/10], Average Loss: 0.2643\n",
            "Epoch [7/10], Average Loss: 0.2491\n",
            "Epoch [8/10], Average Loss: 0.2608\n",
            "Epoch [9/10], Average Loss: 0.4245\n",
            "Epoch [10/10], Average Loss: 0.3641\n",
            "Test Accuracy: 0.8789\n",
            "Test F1 Score: 0.7933\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.3192\n",
            "Epoch [2/10], Average Loss: 0.3279\n",
            "Epoch [3/10], Average Loss: 0.2022\n",
            "Epoch [4/10], Average Loss: 0.1498\n",
            "Epoch [5/10], Average Loss: 0.1226\n",
            "Epoch [6/10], Average Loss: 0.1069\n",
            "Epoch [7/10], Average Loss: 0.0929\n",
            "Epoch [8/10], Average Loss: 0.0855\n",
            "Epoch [9/10], Average Loss: 0.0733\n",
            "Epoch [10/10], Average Loss: 0.0703\n",
            "Test Accuracy: 0.9153\n",
            "Test F1 Score: 0.8565\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2861\n",
            "Epoch [2/10], Average Loss: 1.8933\n",
            "Epoch [3/10], Average Loss: 1.6862\n",
            "Epoch [4/10], Average Loss: 1.5232\n",
            "Epoch [5/10], Average Loss: 1.3316\n",
            "Epoch [6/10], Average Loss: 1.1568\n",
            "Epoch [7/10], Average Loss: 1.0423\n",
            "Epoch [8/10], Average Loss: 0.9491\n",
            "Epoch [9/10], Average Loss: 0.8783\n",
            "Epoch [10/10], Average Loss: 0.7927\n",
            "Test Accuracy: 0.7575\n",
            "Test F1 Score: 0.5232\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.2743\n",
            "Epoch [2/10], Average Loss: 1.6285\n",
            "Epoch [3/10], Average Loss: 1.2500\n",
            "Epoch [4/10], Average Loss: 1.0201\n",
            "Epoch [5/10], Average Loss: 0.8728\n",
            "Epoch [6/10], Average Loss: 0.7479\n",
            "Epoch [7/10], Average Loss: 0.6575\n",
            "Epoch [8/10], Average Loss: 0.5788\n",
            "Epoch [9/10], Average Loss: 0.5008\n",
            "Epoch [10/10], Average Loss: 0.4109\n",
            "Test Accuracy: 0.8783\n",
            "Test F1 Score: 0.7722\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2127\n",
            "Epoch [2/10], Average Loss: 1.7181\n",
            "Epoch [3/10], Average Loss: 1.3230\n",
            "Epoch [4/10], Average Loss: 1.1196\n",
            "Epoch [5/10], Average Loss: 0.9517\n",
            "Epoch [6/10], Average Loss: 0.8349\n",
            "Epoch [7/10], Average Loss: 0.7456\n",
            "Epoch [8/10], Average Loss: 0.6747\n",
            "Epoch [9/10], Average Loss: 0.6040\n",
            "Epoch [10/10], Average Loss: 0.5277\n",
            "Test Accuracy: 0.8552\n",
            "Test F1 Score: 0.7052\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.1133\n",
            "Epoch [2/10], Average Loss: 1.1306\n",
            "Epoch [3/10], Average Loss: 0.7419\n",
            "Epoch [4/10], Average Loss: 0.5720\n",
            "Epoch [5/10], Average Loss: 0.4545\n",
            "Epoch [6/10], Average Loss: 0.3679\n",
            "Epoch [7/10], Average Loss: 0.2993\n",
            "Epoch [8/10], Average Loss: 0.2503\n",
            "Epoch [9/10], Average Loss: 0.2080\n",
            "Epoch [10/10], Average Loss: 0.1797\n",
            "Test Accuracy: 0.9080\n",
            "Test F1 Score: 0.8285\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6643\n",
            "Epoch [2/10], Average Loss: 2.3877\n",
            "Epoch [3/10], Average Loss: 2.2974\n",
            "Epoch [4/10], Average Loss: 2.2373\n",
            "Epoch [5/10], Average Loss: 2.1909\n",
            "Epoch [6/10], Average Loss: 2.1472\n",
            "Epoch [7/10], Average Loss: 2.1064\n",
            "Epoch [8/10], Average Loss: 2.0665\n",
            "Epoch [9/10], Average Loss: 2.0285\n",
            "Epoch [10/10], Average Loss: 1.9922\n",
            "Test Accuracy: 0.3982\n",
            "Test F1 Score: 0.1631\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.7007\n",
            "Epoch [2/10], Average Loss: 2.4313\n",
            "Epoch [3/10], Average Loss: 2.2978\n",
            "Epoch [4/10], Average Loss: 2.2250\n",
            "Epoch [5/10], Average Loss: 2.1703\n",
            "Epoch [6/10], Average Loss: 2.1273\n",
            "Epoch [7/10], Average Loss: 2.0876\n",
            "Epoch [8/10], Average Loss: 2.0503\n",
            "Epoch [9/10], Average Loss: 2.0100\n",
            "Epoch [10/10], Average Loss: 1.9690\n",
            "Test Accuracy: 0.3876\n",
            "Test F1 Score: 0.1667\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.5997\n",
            "Epoch [2/10], Average Loss: 2.3554\n",
            "Epoch [3/10], Average Loss: 2.2612\n",
            "Epoch [4/10], Average Loss: 2.1951\n",
            "Epoch [5/10], Average Loss: 2.1365\n",
            "Epoch [6/10], Average Loss: 2.0858\n",
            "Epoch [7/10], Average Loss: 2.0373\n",
            "Epoch [8/10], Average Loss: 1.9879\n",
            "Epoch [9/10], Average Loss: 1.9370\n",
            "Epoch [10/10], Average Loss: 1.8846\n",
            "Test Accuracy: 0.4251\n",
            "Test F1 Score: 0.1909\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6193\n",
            "Epoch [2/10], Average Loss: 2.3079\n",
            "Epoch [3/10], Average Loss: 2.1903\n",
            "Epoch [4/10], Average Loss: 2.1108\n",
            "Epoch [5/10], Average Loss: 2.0273\n",
            "Epoch [6/10], Average Loss: 1.9176\n",
            "Epoch [7/10], Average Loss: 1.8191\n",
            "Epoch [8/10], Average Loss: 1.7314\n",
            "Epoch [9/10], Average Loss: 1.6550\n",
            "Epoch [10/10], Average Loss: 1.5854\n",
            "Test Accuracy: 0.5633\n",
            "Test F1 Score: 0.2800\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.2380\n",
            "Epoch [2/10], Average Loss: 0.6070\n",
            "Epoch [3/10], Average Loss: 0.4615\n",
            "Epoch [4/10], Average Loss: 0.3731\n",
            "Epoch [5/10], Average Loss: 0.3132\n",
            "Epoch [6/10], Average Loss: 0.2873\n",
            "Epoch [7/10], Average Loss: 0.2840\n",
            "Epoch [8/10], Average Loss: 0.2571\n",
            "Epoch [9/10], Average Loss: 0.2357\n",
            "Epoch [10/10], Average Loss: 0.2285\n",
            "Test Accuracy: 0.9048\n",
            "Test F1 Score: 0.8232\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.3958\n",
            "Epoch [2/10], Average Loss: 0.4865\n",
            "Epoch [3/10], Average Loss: 0.2557\n",
            "Epoch [4/10], Average Loss: 0.1930\n",
            "Epoch [5/10], Average Loss: 0.1529\n",
            "Epoch [6/10], Average Loss: 0.1312\n",
            "Epoch [7/10], Average Loss: 0.1115\n",
            "Epoch [8/10], Average Loss: 0.1002\n",
            "Epoch [9/10], Average Loss: 0.0982\n",
            "Epoch [10/10], Average Loss: 0.0810\n",
            "Test Accuracy: 0.9166\n",
            "Test F1 Score: 0.8377\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.1970\n",
            "Epoch [2/10], Average Loss: 0.5680\n",
            "Epoch [3/10], Average Loss: 0.4279\n",
            "Epoch [4/10], Average Loss: 0.3447\n",
            "Epoch [5/10], Average Loss: 0.2902\n",
            "Epoch [6/10], Average Loss: 0.2613\n",
            "Epoch [7/10], Average Loss: 0.2346\n",
            "Epoch [8/10], Average Loss: 0.2193\n",
            "Epoch [9/10], Average Loss: 0.2089\n",
            "Epoch [10/10], Average Loss: 0.2004\n",
            "Test Accuracy: 0.9053\n",
            "Test F1 Score: 0.8236\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.1232\n",
            "Epoch [2/10], Average Loss: 0.3092\n",
            "Epoch [3/10], Average Loss: 0.1888\n",
            "Epoch [4/10], Average Loss: 0.1414\n",
            "Epoch [5/10], Average Loss: 0.1126\n",
            "Epoch [6/10], Average Loss: 0.0967\n",
            "Epoch [7/10], Average Loss: 0.0844\n",
            "Epoch [8/10], Average Loss: 0.0740\n",
            "Epoch [9/10], Average Loss: 0.0692\n",
            "Epoch [10/10], Average Loss: 0.0695\n",
            "Test Accuracy: 0.9177\n",
            "Test F1 Score: 0.8489\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2687\n",
            "Epoch [2/10], Average Loss: 1.7809\n",
            "Epoch [3/10], Average Loss: 1.5424\n",
            "Epoch [4/10], Average Loss: 1.3930\n",
            "Epoch [5/10], Average Loss: 1.2532\n",
            "Epoch [6/10], Average Loss: 1.1164\n",
            "Epoch [7/10], Average Loss: 1.0069\n",
            "Epoch [8/10], Average Loss: 0.9185\n",
            "Epoch [9/10], Average Loss: 0.8078\n",
            "Epoch [10/10], Average Loss: 0.7170\n",
            "Test Accuracy: 0.7595\n",
            "Test F1 Score: 0.5326\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.2647\n",
            "Epoch [2/10], Average Loss: 1.5075\n",
            "Epoch [3/10], Average Loss: 1.1690\n",
            "Epoch [4/10], Average Loss: 0.9788\n",
            "Epoch [5/10], Average Loss: 0.8359\n",
            "Epoch [6/10], Average Loss: 0.7282\n",
            "Epoch [7/10], Average Loss: 0.6351\n",
            "Epoch [8/10], Average Loss: 0.5487\n",
            "Epoch [9/10], Average Loss: 0.4592\n",
            "Epoch [10/10], Average Loss: 0.3947\n",
            "Test Accuracy: 0.8807\n",
            "Test F1 Score: 0.7666\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.1923\n",
            "Epoch [2/10], Average Loss: 1.6651\n",
            "Epoch [3/10], Average Loss: 1.4381\n",
            "Epoch [4/10], Average Loss: 1.2298\n",
            "Epoch [5/10], Average Loss: 0.9945\n",
            "Epoch [6/10], Average Loss: 0.8601\n",
            "Epoch [7/10], Average Loss: 0.7633\n",
            "Epoch [8/10], Average Loss: 0.6900\n",
            "Epoch [9/10], Average Loss: 0.6117\n",
            "Epoch [10/10], Average Loss: 0.5260\n",
            "Test Accuracy: 0.8503\n",
            "Test F1 Score: 0.6941\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.1107\n",
            "Epoch [2/10], Average Loss: 1.1315\n",
            "Epoch [3/10], Average Loss: 0.7313\n",
            "Epoch [4/10], Average Loss: 0.5596\n",
            "Epoch [5/10], Average Loss: 0.4429\n",
            "Epoch [6/10], Average Loss: 0.3548\n",
            "Epoch [7/10], Average Loss: 0.2868\n",
            "Epoch [8/10], Average Loss: 0.2372\n",
            "Epoch [9/10], Average Loss: 0.1996\n",
            "Epoch [10/10], Average Loss: 0.1712\n",
            "Test Accuracy: 0.9084\n",
            "Test F1 Score: 0.8223\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6728\n",
            "Epoch [2/10], Average Loss: 2.4149\n",
            "Epoch [3/10], Average Loss: 2.3206\n",
            "Epoch [4/10], Average Loss: 2.2554\n",
            "Epoch [5/10], Average Loss: 2.2058\n",
            "Epoch [6/10], Average Loss: 2.1636\n",
            "Epoch [7/10], Average Loss: 2.1266\n",
            "Epoch [8/10], Average Loss: 2.0928\n",
            "Epoch [9/10], Average Loss: 2.0609\n",
            "Epoch [10/10], Average Loss: 2.0274\n",
            "Test Accuracy: 0.3657\n",
            "Test F1 Score: 0.1428\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.7039\n",
            "Epoch [2/10], Average Loss: 2.5146\n",
            "Epoch [3/10], Average Loss: 2.3151\n",
            "Epoch [4/10], Average Loss: 2.2159\n",
            "Epoch [5/10], Average Loss: 2.1528\n",
            "Epoch [6/10], Average Loss: 2.0966\n",
            "Epoch [7/10], Average Loss: 2.0366\n",
            "Epoch [8/10], Average Loss: 1.9702\n",
            "Epoch [9/10], Average Loss: 1.9022\n",
            "Epoch [10/10], Average Loss: 1.8417\n",
            "Test Accuracy: 0.4564\n",
            "Test F1 Score: 0.2129\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.5970\n",
            "Epoch [2/10], Average Loss: 2.3592\n",
            "Epoch [3/10], Average Loss: 2.2751\n",
            "Epoch [4/10], Average Loss: 2.2059\n",
            "Epoch [5/10], Average Loss: 2.1469\n",
            "Epoch [6/10], Average Loss: 2.0979\n",
            "Epoch [7/10], Average Loss: 2.0491\n",
            "Epoch [8/10], Average Loss: 2.0010\n",
            "Epoch [9/10], Average Loss: 1.9508\n",
            "Epoch [10/10], Average Loss: 1.8981\n",
            "Test Accuracy: 0.4301\n",
            "Test F1 Score: 0.1941\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6241\n",
            "Epoch [2/10], Average Loss: 2.4542\n",
            "Epoch [3/10], Average Loss: 2.3430\n",
            "Epoch [4/10], Average Loss: 2.0994\n",
            "Epoch [5/10], Average Loss: 1.9618\n",
            "Epoch [6/10], Average Loss: 1.8366\n",
            "Epoch [7/10], Average Loss: 1.7178\n",
            "Epoch [8/10], Average Loss: 1.6096\n",
            "Epoch [9/10], Average Loss: 1.5187\n",
            "Epoch [10/10], Average Loss: 1.4385\n",
            "Test Accuracy: 0.6382\n",
            "Test F1 Score: 0.3601\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.2411\n",
            "Epoch [2/10], Average Loss: 0.6005\n",
            "Epoch [3/10], Average Loss: 0.4796\n",
            "Epoch [4/10], Average Loss: 0.3987\n",
            "Epoch [5/10], Average Loss: 0.3531\n",
            "Epoch [6/10], Average Loss: 0.3136\n",
            "Epoch [7/10], Average Loss: 0.2885\n",
            "Epoch [8/10], Average Loss: 0.2689\n",
            "Epoch [9/10], Average Loss: 0.2546\n",
            "Epoch [10/10], Average Loss: 0.2416\n",
            "Test Accuracy: 0.9073\n",
            "Test F1 Score: 0.8253\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.3126\n",
            "Epoch [2/10], Average Loss: 0.4029\n",
            "Epoch [3/10], Average Loss: 0.2402\n",
            "Epoch [4/10], Average Loss: 0.1888\n",
            "Epoch [5/10], Average Loss: 0.1604\n",
            "Epoch [6/10], Average Loss: 0.1408\n",
            "Epoch [7/10], Average Loss: 0.1233\n",
            "Epoch [8/10], Average Loss: 0.1127\n",
            "Epoch [9/10], Average Loss: 0.1112\n",
            "Epoch [10/10], Average Loss: 0.1025\n",
            "Test Accuracy: 0.9165\n",
            "Test F1 Score: 0.8423\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.2363\n",
            "Epoch [2/10], Average Loss: 0.6130\n",
            "Epoch [3/10], Average Loss: 0.4855\n",
            "Epoch [4/10], Average Loss: 0.4153\n",
            "Epoch [5/10], Average Loss: 0.3570\n",
            "Epoch [6/10], Average Loss: 0.3165\n",
            "Epoch [7/10], Average Loss: 0.2932\n",
            "Epoch [8/10], Average Loss: 0.2677\n",
            "Epoch [9/10], Average Loss: 0.2581\n",
            "Epoch [10/10], Average Loss: 0.2529\n",
            "Test Accuracy: 0.9018\n",
            "Test F1 Score: 0.8188\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.1156\n",
            "Epoch [2/10], Average Loss: 0.3201\n",
            "Epoch [3/10], Average Loss: 0.2084\n",
            "Epoch [4/10], Average Loss: 0.1637\n",
            "Epoch [5/10], Average Loss: 0.1339\n",
            "Epoch [6/10], Average Loss: 0.1152\n",
            "Epoch [7/10], Average Loss: 0.1019\n",
            "Epoch [8/10], Average Loss: 0.0912\n",
            "Epoch [9/10], Average Loss: 0.0837\n",
            "Epoch [10/10], Average Loss: 0.0821\n",
            "Test Accuracy: 0.9180\n",
            "Test F1 Score: 0.8510\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2901\n",
            "Epoch [2/10], Average Loss: 1.8917\n",
            "Epoch [3/10], Average Loss: 1.6974\n",
            "Epoch [4/10], Average Loss: 1.5323\n",
            "Epoch [5/10], Average Loss: 1.3154\n",
            "Epoch [6/10], Average Loss: 1.1633\n",
            "Epoch [7/10], Average Loss: 1.0516\n",
            "Epoch [8/10], Average Loss: 0.9637\n",
            "Epoch [9/10], Average Loss: 0.8801\n",
            "Epoch [10/10], Average Loss: 0.7909\n",
            "Test Accuracy: 0.7554\n",
            "Test F1 Score: 0.5246\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.3325\n",
            "Epoch [2/10], Average Loss: 1.5235\n",
            "Epoch [3/10], Average Loss: 1.1510\n",
            "Epoch [4/10], Average Loss: 0.8919\n",
            "Epoch [5/10], Average Loss: 0.7479\n",
            "Epoch [6/10], Average Loss: 0.6393\n",
            "Epoch [7/10], Average Loss: 0.5531\n",
            "Epoch [8/10], Average Loss: 0.4848\n",
            "Epoch [9/10], Average Loss: 0.4215\n",
            "Epoch [10/10], Average Loss: 0.3629\n",
            "Test Accuracy: 0.8804\n",
            "Test F1 Score: 0.7476\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2090\n",
            "Epoch [2/10], Average Loss: 1.6660\n",
            "Epoch [3/10], Average Loss: 1.3390\n",
            "Epoch [4/10], Average Loss: 1.1276\n",
            "Epoch [5/10], Average Loss: 0.9705\n",
            "Epoch [6/10], Average Loss: 0.8548\n",
            "Epoch [7/10], Average Loss: 0.7655\n",
            "Epoch [8/10], Average Loss: 0.6953\n",
            "Epoch [9/10], Average Loss: 0.6316\n",
            "Epoch [10/10], Average Loss: 0.5521\n",
            "Test Accuracy: 0.8494\n",
            "Test F1 Score: 0.6886\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.1388\n",
            "Epoch [2/10], Average Loss: 1.1474\n",
            "Epoch [3/10], Average Loss: 0.7496\n",
            "Epoch [4/10], Average Loss: 0.5785\n",
            "Epoch [5/10], Average Loss: 0.4567\n",
            "Epoch [6/10], Average Loss: 0.3635\n",
            "Epoch [7/10], Average Loss: 0.2969\n",
            "Epoch [8/10], Average Loss: 0.2502\n",
            "Epoch [9/10], Average Loss: 0.2150\n",
            "Epoch [10/10], Average Loss: 0.1862\n",
            "Test Accuracy: 0.9111\n",
            "Test F1 Score: 0.8289\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6890\n",
            "Epoch [2/10], Average Loss: 2.4461\n",
            "Epoch [3/10], Average Loss: 2.3571\n",
            "Epoch [4/10], Average Loss: 2.2943\n",
            "Epoch [5/10], Average Loss: 2.2460\n",
            "Epoch [6/10], Average Loss: 2.2052\n",
            "Epoch [7/10], Average Loss: 2.1702\n",
            "Epoch [8/10], Average Loss: 2.1352\n",
            "Epoch [9/10], Average Loss: 2.1035\n",
            "Epoch [10/10], Average Loss: 2.0728\n",
            "Test Accuracy: 0.3434\n",
            "Test F1 Score: 0.1349\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6693\n",
            "Epoch [2/10], Average Loss: 2.3934\n",
            "Epoch [3/10], Average Loss: 2.2873\n",
            "Epoch [4/10], Average Loss: 2.2236\n",
            "Epoch [5/10], Average Loss: 2.1742\n",
            "Epoch [6/10], Average Loss: 2.1322\n",
            "Epoch [7/10], Average Loss: 2.0938\n",
            "Epoch [8/10], Average Loss: 2.0544\n",
            "Epoch [9/10], Average Loss: 2.0113\n",
            "Epoch [10/10], Average Loss: 1.9660\n",
            "Test Accuracy: 0.3971\n",
            "Test F1 Score: 0.1596\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.5967\n",
            "Epoch [2/10], Average Loss: 2.3350\n",
            "Epoch [3/10], Average Loss: 2.2413\n",
            "Epoch [4/10], Average Loss: 2.1763\n",
            "Epoch [5/10], Average Loss: 2.1214\n",
            "Epoch [6/10], Average Loss: 2.0689\n",
            "Epoch [7/10], Average Loss: 2.0162\n",
            "Epoch [8/10], Average Loss: 1.9622\n",
            "Epoch [9/10], Average Loss: 1.9083\n",
            "Epoch [10/10], Average Loss: 1.8540\n",
            "Test Accuracy: 0.4589\n",
            "Test F1 Score: 0.2157\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6245\n",
            "Epoch [2/10], Average Loss: 2.4509\n",
            "Epoch [3/10], Average Loss: 2.3147\n",
            "Epoch [4/10], Average Loss: 2.0837\n",
            "Epoch [5/10], Average Loss: 1.9374\n",
            "Epoch [6/10], Average Loss: 1.7994\n",
            "Epoch [7/10], Average Loss: 1.6744\n",
            "Epoch [8/10], Average Loss: 1.5703\n",
            "Epoch [9/10], Average Loss: 1.4839\n",
            "Epoch [10/10], Average Loss: 1.4059\n",
            "Test Accuracy: 0.6524\n",
            "Test F1 Score: 0.3719\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.5439\n",
            "Epoch [2/10], Average Loss: 0.7544\n",
            "Epoch [3/10], Average Loss: 0.5084\n",
            "Epoch [4/10], Average Loss: 0.3890\n",
            "Epoch [5/10], Average Loss: 0.3302\n",
            "Epoch [6/10], Average Loss: 0.2924\n",
            "Epoch [7/10], Average Loss: 0.2674\n",
            "Epoch [8/10], Average Loss: 0.2438\n",
            "Epoch [9/10], Average Loss: 0.2289\n",
            "Epoch [10/10], Average Loss: 0.2179\n",
            "Test Accuracy: 0.9074\n",
            "Test F1 Score: 0.8232\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.3215\n",
            "Epoch [2/10], Average Loss: 0.3906\n",
            "Epoch [3/10], Average Loss: 0.2509\n",
            "Epoch [4/10], Average Loss: 0.1956\n",
            "Epoch [5/10], Average Loss: 0.1650\n",
            "Epoch [6/10], Average Loss: 0.1433\n",
            "Epoch [7/10], Average Loss: 0.1295\n",
            "Epoch [8/10], Average Loss: 0.1199\n",
            "Epoch [9/10], Average Loss: 0.1094\n",
            "Epoch [10/10], Average Loss: 0.1009\n",
            "Test Accuracy: 0.9176\n",
            "Test F1 Score: 0.8461\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.2027\n",
            "Epoch [2/10], Average Loss: 0.5709\n",
            "Epoch [3/10], Average Loss: 0.4523\n",
            "Epoch [4/10], Average Loss: 0.3758\n",
            "Epoch [5/10], Average Loss: 0.3202\n",
            "Epoch [6/10], Average Loss: 0.2855\n",
            "Epoch [7/10], Average Loss: 0.2646\n",
            "Epoch [8/10], Average Loss: 0.2443\n",
            "Epoch [9/10], Average Loss: 0.2315\n",
            "Epoch [10/10], Average Loss: 0.2197\n",
            "Test Accuracy: 0.9056\n",
            "Test F1 Score: 0.8204\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.4868\n",
            "Epoch [2/10], Average Loss: 0.3942\n",
            "Epoch [3/10], Average Loss: 0.2259\n",
            "Epoch [4/10], Average Loss: 0.1748\n",
            "Epoch [5/10], Average Loss: 0.1436\n",
            "Epoch [6/10], Average Loss: 0.1281\n",
            "Epoch [7/10], Average Loss: 0.1064\n",
            "Epoch [8/10], Average Loss: 0.0982\n",
            "Epoch [9/10], Average Loss: 0.0883\n",
            "Epoch [10/10], Average Loss: 0.0897\n",
            "Test Accuracy: 0.9183\n",
            "Test F1 Score: 0.8524\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.3011\n",
            "Epoch [2/10], Average Loss: 1.9041\n",
            "Epoch [3/10], Average Loss: 1.7040\n",
            "Epoch [4/10], Average Loss: 1.5538\n",
            "Epoch [5/10], Average Loss: 1.3435\n",
            "Epoch [6/10], Average Loss: 1.1731\n",
            "Epoch [7/10], Average Loss: 1.0618\n",
            "Epoch [8/10], Average Loss: 0.9725\n",
            "Epoch [9/10], Average Loss: 0.8899\n",
            "Epoch [10/10], Average Loss: 0.7992\n",
            "Test Accuracy: 0.7558\n",
            "Test F1 Score: 0.5198\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.2998\n",
            "Epoch [2/10], Average Loss: 1.5541\n",
            "Epoch [3/10], Average Loss: 1.2191\n",
            "Epoch [4/10], Average Loss: 1.0253\n",
            "Epoch [5/10], Average Loss: 0.8763\n",
            "Epoch [6/10], Average Loss: 0.7602\n",
            "Epoch [7/10], Average Loss: 0.6685\n",
            "Epoch [8/10], Average Loss: 0.5758\n",
            "Epoch [9/10], Average Loss: 0.4797\n",
            "Epoch [10/10], Average Loss: 0.4024\n",
            "Test Accuracy: 0.8916\n",
            "Test F1 Score: 0.7882\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2132\n",
            "Epoch [2/10], Average Loss: 1.6798\n",
            "Epoch [3/10], Average Loss: 1.4463\n",
            "Epoch [4/10], Average Loss: 1.2684\n",
            "Epoch [5/10], Average Loss: 1.1088\n",
            "Epoch [6/10], Average Loss: 0.9495\n",
            "Epoch [7/10], Average Loss: 0.8005\n",
            "Epoch [8/10], Average Loss: 0.7068\n",
            "Epoch [9/10], Average Loss: 0.6393\n",
            "Epoch [10/10], Average Loss: 0.5852\n",
            "Test Accuracy: 0.7870\n",
            "Test F1 Score: 0.6335\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.1837\n",
            "Epoch [2/10], Average Loss: 1.1663\n",
            "Epoch [3/10], Average Loss: 0.7598\n",
            "Epoch [4/10], Average Loss: 0.5769\n",
            "Epoch [5/10], Average Loss: 0.4523\n",
            "Epoch [6/10], Average Loss: 0.3578\n",
            "Epoch [7/10], Average Loss: 0.2906\n",
            "Epoch [8/10], Average Loss: 0.2444\n",
            "Epoch [9/10], Average Loss: 0.2096\n",
            "Epoch [10/10], Average Loss: 0.1825\n",
            "Test Accuracy: 0.9123\n",
            "Test F1 Score: 0.8319\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6789\n",
            "Epoch [2/10], Average Loss: 2.4136\n",
            "Epoch [3/10], Average Loss: 2.3174\n",
            "Epoch [4/10], Average Loss: 2.2589\n",
            "Epoch [5/10], Average Loss: 2.2129\n",
            "Epoch [6/10], Average Loss: 2.1737\n",
            "Epoch [7/10], Average Loss: 2.1353\n",
            "Epoch [8/10], Average Loss: 2.0983\n",
            "Epoch [9/10], Average Loss: 2.0642\n",
            "Epoch [10/10], Average Loss: 2.0293\n",
            "Test Accuracy: 0.3700\n",
            "Test F1 Score: 0.1582\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6968\n",
            "Epoch [2/10], Average Loss: 2.5208\n",
            "Epoch [3/10], Average Loss: 2.4291\n",
            "Epoch [4/10], Average Loss: 2.2512\n",
            "Epoch [5/10], Average Loss: 2.1740\n",
            "Epoch [6/10], Average Loss: 2.1146\n",
            "Epoch [7/10], Average Loss: 2.0503\n",
            "Epoch [8/10], Average Loss: 1.9772\n",
            "Epoch [9/10], Average Loss: 1.9057\n",
            "Epoch [10/10], Average Loss: 1.8418\n",
            "Test Accuracy: 0.4748\n",
            "Test F1 Score: 0.2248\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6111\n",
            "Epoch [2/10], Average Loss: 2.3837\n",
            "Epoch [3/10], Average Loss: 2.3123\n",
            "Epoch [4/10], Average Loss: 2.2615\n",
            "Epoch [5/10], Average Loss: 2.2142\n",
            "Epoch [6/10], Average Loss: 2.1670\n",
            "Epoch [7/10], Average Loss: 2.1223\n",
            "Epoch [8/10], Average Loss: 2.0856\n",
            "Epoch [9/10], Average Loss: 2.0473\n",
            "Epoch [10/10], Average Loss: 2.0095\n",
            "Test Accuracy: 0.4214\n",
            "Test F1 Score: 0.1736\n",
            "Testing parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6386\n",
            "Epoch [2/10], Average Loss: 2.4570\n",
            "Epoch [3/10], Average Loss: 2.3088\n",
            "Epoch [4/10], Average Loss: 2.0961\n",
            "Epoch [5/10], Average Loss: 1.9609\n",
            "Epoch [6/10], Average Loss: 1.8327\n",
            "Epoch [7/10], Average Loss: 1.7164\n",
            "Epoch [8/10], Average Loss: 1.6112\n",
            "Epoch [9/10], Average Loss: 1.5258\n",
            "Epoch [10/10], Average Loss: 1.4496\n",
            "Test Accuracy: 0.6176\n",
            "Test F1 Score: 0.3377\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.2652\n",
            "Epoch [2/10], Average Loss: 0.5906\n",
            "Epoch [3/10], Average Loss: 0.4191\n",
            "Epoch [4/10], Average Loss: 0.3381\n",
            "Epoch [5/10], Average Loss: 0.2848\n",
            "Epoch [6/10], Average Loss: 0.2525\n",
            "Epoch [7/10], Average Loss: 0.2261\n",
            "Epoch [8/10], Average Loss: 0.2149\n",
            "Epoch [9/10], Average Loss: 0.1940\n",
            "Epoch [10/10], Average Loss: 0.1955\n",
            "Test Accuracy: 0.9064\n",
            "Test F1 Score: 0.8284\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.2166\n",
            "Epoch [2/10], Average Loss: 0.3648\n",
            "Epoch [3/10], Average Loss: 0.2250\n",
            "Epoch [4/10], Average Loss: 0.1705\n",
            "Epoch [5/10], Average Loss: 0.1385\n",
            "Epoch [6/10], Average Loss: 0.1181\n",
            "Epoch [7/10], Average Loss: 0.1062\n",
            "Epoch [8/10], Average Loss: 0.0964\n",
            "Epoch [9/10], Average Loss: 0.0903\n",
            "Epoch [10/10], Average Loss: 0.0836\n",
            "Test Accuracy: 0.9156\n",
            "Test F1 Score: 0.8533\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.2028\n",
            "Epoch [2/10], Average Loss: 0.5569\n",
            "Epoch [3/10], Average Loss: 0.4210\n",
            "Epoch [4/10], Average Loss: 0.3442\n",
            "Epoch [5/10], Average Loss: 0.2958\n",
            "Epoch [6/10], Average Loss: 0.2589\n",
            "Epoch [7/10], Average Loss: 0.2325\n",
            "Epoch [8/10], Average Loss: 0.2134\n",
            "Epoch [9/10], Average Loss: 0.2020\n",
            "Epoch [10/10], Average Loss: 0.1863\n",
            "Test Accuracy: 0.9049\n",
            "Test F1 Score: 0.8314\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.0334\n",
            "Epoch [2/10], Average Loss: 0.2844\n",
            "Epoch [3/10], Average Loss: 0.1855\n",
            "Epoch [4/10], Average Loss: 0.1432\n",
            "Epoch [5/10], Average Loss: 0.1141\n",
            "Epoch [6/10], Average Loss: 0.0973\n",
            "Epoch [7/10], Average Loss: 0.0855\n",
            "Epoch [8/10], Average Loss: 0.0782\n",
            "Epoch [9/10], Average Loss: 0.0773\n",
            "Epoch [10/10], Average Loss: 0.0728\n",
            "Test Accuracy: 0.9189\n",
            "Test F1 Score: 0.8493\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2936\n",
            "Epoch [2/10], Average Loss: 1.8900\n",
            "Epoch [3/10], Average Loss: 1.6852\n",
            "Epoch [4/10], Average Loss: 1.5353\n",
            "Epoch [5/10], Average Loss: 1.3739\n",
            "Epoch [6/10], Average Loss: 1.1776\n",
            "Epoch [7/10], Average Loss: 1.0533\n",
            "Epoch [8/10], Average Loss: 0.9547\n",
            "Epoch [9/10], Average Loss: 0.8800\n",
            "Epoch [10/10], Average Loss: 0.7991\n",
            "Test Accuracy: 0.7604\n",
            "Test F1 Score: 0.5247\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.2823\n",
            "Epoch [2/10], Average Loss: 1.7940\n",
            "Epoch [3/10], Average Loss: 1.4788\n",
            "Epoch [4/10], Average Loss: 1.2073\n",
            "Epoch [5/10], Average Loss: 0.9971\n",
            "Epoch [6/10], Average Loss: 0.8655\n",
            "Epoch [7/10], Average Loss: 0.7207\n",
            "Epoch [8/10], Average Loss: 0.5978\n",
            "Epoch [9/10], Average Loss: 0.5100\n",
            "Epoch [10/10], Average Loss: 0.4505\n",
            "Test Accuracy: 0.8237\n",
            "Test F1 Score: 0.7267\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2342\n",
            "Epoch [2/10], Average Loss: 1.7841\n",
            "Epoch [3/10], Average Loss: 1.4924\n",
            "Epoch [4/10], Average Loss: 1.1960\n",
            "Epoch [5/10], Average Loss: 0.9513\n",
            "Epoch [6/10], Average Loss: 0.8171\n",
            "Epoch [7/10], Average Loss: 0.7128\n",
            "Epoch [8/10], Average Loss: 0.6337\n",
            "Epoch [9/10], Average Loss: 0.5512\n",
            "Epoch [10/10], Average Loss: 0.4756\n",
            "Test Accuracy: 0.8607\n",
            "Test F1 Score: 0.7124\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.1614\n",
            "Epoch [2/10], Average Loss: 1.1155\n",
            "Epoch [3/10], Average Loss: 0.7249\n",
            "Epoch [4/10], Average Loss: 0.5503\n",
            "Epoch [5/10], Average Loss: 0.4326\n",
            "Epoch [6/10], Average Loss: 0.3437\n",
            "Epoch [7/10], Average Loss: 0.2818\n",
            "Epoch [8/10], Average Loss: 0.2344\n",
            "Epoch [9/10], Average Loss: 0.2014\n",
            "Epoch [10/10], Average Loss: 0.1721\n",
            "Test Accuracy: 0.9079\n",
            "Test F1 Score: 0.8365\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6654\n",
            "Epoch [2/10], Average Loss: 2.4116\n",
            "Epoch [3/10], Average Loss: 2.3167\n",
            "Epoch [4/10], Average Loss: 2.2599\n",
            "Epoch [5/10], Average Loss: 2.2158\n",
            "Epoch [6/10], Average Loss: 2.1770\n",
            "Epoch [7/10], Average Loss: 2.1396\n",
            "Epoch [8/10], Average Loss: 2.1061\n",
            "Epoch [9/10], Average Loss: 2.0713\n",
            "Epoch [10/10], Average Loss: 2.0389\n",
            "Test Accuracy: 0.3762\n",
            "Test F1 Score: 0.1521\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6565\n",
            "Epoch [2/10], Average Loss: 2.3933\n",
            "Epoch [3/10], Average Loss: 2.2914\n",
            "Epoch [4/10], Average Loss: 2.2306\n",
            "Epoch [5/10], Average Loss: 2.1820\n",
            "Epoch [6/10], Average Loss: 2.1376\n",
            "Epoch [7/10], Average Loss: 2.1012\n",
            "Epoch [8/10], Average Loss: 2.0658\n",
            "Epoch [9/10], Average Loss: 2.0285\n",
            "Epoch [10/10], Average Loss: 1.9897\n",
            "Test Accuracy: 0.3950\n",
            "Test F1 Score: 0.1652\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.5833\n",
            "Epoch [2/10], Average Loss: 2.3572\n",
            "Epoch [3/10], Average Loss: 2.2758\n",
            "Epoch [4/10], Average Loss: 2.2128\n",
            "Epoch [5/10], Average Loss: 2.1614\n",
            "Epoch [6/10], Average Loss: 2.1167\n",
            "Epoch [7/10], Average Loss: 2.0731\n",
            "Epoch [8/10], Average Loss: 2.0314\n",
            "Epoch [9/10], Average Loss: 1.9946\n",
            "Epoch [10/10], Average Loss: 1.9541\n",
            "Test Accuracy: 0.4278\n",
            "Test F1 Score: 0.1818\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.5764\n",
            "Epoch [2/10], Average Loss: 2.3118\n",
            "Epoch [3/10], Average Loss: 2.2182\n",
            "Epoch [4/10], Average Loss: 2.1562\n",
            "Epoch [5/10], Average Loss: 2.1042\n",
            "Epoch [6/10], Average Loss: 2.0523\n",
            "Epoch [7/10], Average Loss: 1.9904\n",
            "Epoch [8/10], Average Loss: 1.9169\n",
            "Epoch [9/10], Average Loss: 1.8516\n",
            "Epoch [10/10], Average Loss: 1.7960\n",
            "Test Accuracy: 0.4515\n",
            "Test F1 Score: 0.1925\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.3867\n",
            "Epoch [2/10], Average Loss: 0.7035\n",
            "Epoch [3/10], Average Loss: 0.4567\n",
            "Epoch [4/10], Average Loss: 0.3600\n",
            "Epoch [5/10], Average Loss: 0.3006\n",
            "Epoch [6/10], Average Loss: 0.2593\n",
            "Epoch [7/10], Average Loss: 0.2311\n",
            "Epoch [8/10], Average Loss: 0.2112\n",
            "Epoch [9/10], Average Loss: 0.1953\n",
            "Epoch [10/10], Average Loss: 0.1874\n",
            "Test Accuracy: 0.9073\n",
            "Test F1 Score: 0.8285\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.3858\n",
            "Epoch [2/10], Average Loss: 0.4613\n",
            "Epoch [3/10], Average Loss: 0.2343\n",
            "Epoch [4/10], Average Loss: 0.1671\n",
            "Epoch [5/10], Average Loss: 0.1334\n",
            "Epoch [6/10], Average Loss: 0.1110\n",
            "Epoch [7/10], Average Loss: 0.0950\n",
            "Epoch [8/10], Average Loss: 0.0847\n",
            "Epoch [9/10], Average Loss: 0.0758\n",
            "Epoch [10/10], Average Loss: 0.0688\n",
            "Test Accuracy: 0.9151\n",
            "Test F1 Score: 0.8494\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.5527\n",
            "Epoch [2/10], Average Loss: 0.6063\n",
            "Epoch [3/10], Average Loss: 0.4649\n",
            "Epoch [4/10], Average Loss: 0.3862\n",
            "Epoch [5/10], Average Loss: 0.3387\n",
            "Epoch [6/10], Average Loss: 0.2937\n",
            "Epoch [7/10], Average Loss: 0.2649\n",
            "Epoch [8/10], Average Loss: 0.2433\n",
            "Epoch [9/10], Average Loss: 0.2234\n",
            "Epoch [10/10], Average Loss: 0.2109\n",
            "Test Accuracy: 0.9029\n",
            "Test F1 Score: 0.8312\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.2213\n",
            "Epoch [2/10], Average Loss: 0.3234\n",
            "Epoch [3/10], Average Loss: 0.1979\n",
            "Epoch [4/10], Average Loss: 0.1470\n",
            "Epoch [5/10], Average Loss: 0.1180\n",
            "Epoch [6/10], Average Loss: 0.0986\n",
            "Epoch [7/10], Average Loss: 0.0846\n",
            "Epoch [8/10], Average Loss: 0.0769\n",
            "Epoch [9/10], Average Loss: 0.0642\n",
            "Epoch [10/10], Average Loss: 0.0645\n",
            "Test Accuracy: 0.9183\n",
            "Test F1 Score: 0.8408\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2812\n",
            "Epoch [2/10], Average Loss: 1.8783\n",
            "Epoch [3/10], Average Loss: 1.6707\n",
            "Epoch [4/10], Average Loss: 1.5099\n",
            "Epoch [5/10], Average Loss: 1.2942\n",
            "Epoch [6/10], Average Loss: 1.1257\n",
            "Epoch [7/10], Average Loss: 1.0147\n",
            "Epoch [8/10], Average Loss: 0.9270\n",
            "Epoch [9/10], Average Loss: 0.8500\n",
            "Epoch [10/10], Average Loss: 0.7638\n",
            "Test Accuracy: 0.7647\n",
            "Test F1 Score: 0.5457\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.2674\n",
            "Epoch [2/10], Average Loss: 1.8012\n",
            "Epoch [3/10], Average Loss: 1.4881\n",
            "Epoch [4/10], Average Loss: 1.2107\n",
            "Epoch [5/10], Average Loss: 0.9940\n",
            "Epoch [6/10], Average Loss: 0.8473\n",
            "Epoch [7/10], Average Loss: 0.6859\n",
            "Epoch [8/10], Average Loss: 0.5690\n",
            "Epoch [9/10], Average Loss: 0.4895\n",
            "Epoch [10/10], Average Loss: 0.4309\n",
            "Test Accuracy: 0.8256\n",
            "Test F1 Score: 0.7341\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2019\n",
            "Epoch [2/10], Average Loss: 1.7747\n",
            "Epoch [3/10], Average Loss: 1.4914\n",
            "Epoch [4/10], Average Loss: 1.2522\n",
            "Epoch [5/10], Average Loss: 1.0797\n",
            "Epoch [6/10], Average Loss: 0.9073\n",
            "Epoch [7/10], Average Loss: 0.7678\n",
            "Epoch [8/10], Average Loss: 0.6785\n",
            "Epoch [9/10], Average Loss: 0.6072\n",
            "Epoch [10/10], Average Loss: 0.5544\n",
            "Test Accuracy: 0.7867\n",
            "Test F1 Score: 0.6457\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.0746\n",
            "Epoch [2/10], Average Loss: 1.1644\n",
            "Epoch [3/10], Average Loss: 0.7261\n",
            "Epoch [4/10], Average Loss: 0.5341\n",
            "Epoch [5/10], Average Loss: 0.4108\n",
            "Epoch [6/10], Average Loss: 0.3264\n",
            "Epoch [7/10], Average Loss: 0.2655\n",
            "Epoch [8/10], Average Loss: 0.2213\n",
            "Epoch [9/10], Average Loss: 0.1880\n",
            "Epoch [10/10], Average Loss: 0.1617\n",
            "Test Accuracy: 0.9029\n",
            "Test F1 Score: 0.8140\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6553\n",
            "Epoch [2/10], Average Loss: 2.4058\n",
            "Epoch [3/10], Average Loss: 2.3146\n",
            "Epoch [4/10], Average Loss: 2.2568\n",
            "Epoch [5/10], Average Loss: 2.2084\n",
            "Epoch [6/10], Average Loss: 2.1687\n",
            "Epoch [7/10], Average Loss: 2.1316\n",
            "Epoch [8/10], Average Loss: 2.0983\n",
            "Epoch [9/10], Average Loss: 2.0641\n",
            "Epoch [10/10], Average Loss: 2.0319\n",
            "Test Accuracy: 0.3674\n",
            "Test F1 Score: 0.1485\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6582\n",
            "Epoch [2/10], Average Loss: 2.3951\n",
            "Epoch [3/10], Average Loss: 2.2920\n",
            "Epoch [4/10], Average Loss: 2.2282\n",
            "Epoch [5/10], Average Loss: 2.1802\n",
            "Epoch [6/10], Average Loss: 2.1377\n",
            "Epoch [7/10], Average Loss: 2.1006\n",
            "Epoch [8/10], Average Loss: 2.0674\n",
            "Epoch [9/10], Average Loss: 2.0315\n",
            "Epoch [10/10], Average Loss: 1.9941\n",
            "Test Accuracy: 0.3626\n",
            "Test F1 Score: 0.1454\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.5862\n",
            "Epoch [2/10], Average Loss: 2.3586\n",
            "Epoch [3/10], Average Loss: 2.2828\n",
            "Epoch [4/10], Average Loss: 2.2286\n",
            "Epoch [5/10], Average Loss: 2.1822\n",
            "Epoch [6/10], Average Loss: 2.1388\n",
            "Epoch [7/10], Average Loss: 2.0959\n",
            "Epoch [8/10], Average Loss: 2.0561\n",
            "Epoch [9/10], Average Loss: 2.0168\n",
            "Epoch [10/10], Average Loss: 1.9783\n",
            "Test Accuracy: 0.3786\n",
            "Test F1 Score: 0.1683\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.5888\n",
            "Epoch [2/10], Average Loss: 2.3093\n",
            "Epoch [3/10], Average Loss: 2.2119\n",
            "Epoch [4/10], Average Loss: 2.1507\n",
            "Epoch [5/10], Average Loss: 2.0984\n",
            "Epoch [6/10], Average Loss: 2.0415\n",
            "Epoch [7/10], Average Loss: 1.9703\n",
            "Epoch [8/10], Average Loss: 1.8929\n",
            "Epoch [9/10], Average Loss: 1.8308\n",
            "Epoch [10/10], Average Loss: 1.7786\n",
            "Test Accuracy: 0.4709\n",
            "Test F1 Score: 0.2157\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.4549\n",
            "Epoch [2/10], Average Loss: 0.7186\n",
            "Epoch [3/10], Average Loss: 0.5033\n",
            "Epoch [4/10], Average Loss: 0.4112\n",
            "Epoch [5/10], Average Loss: 0.3462\n",
            "Epoch [6/10], Average Loss: 0.3036\n",
            "Epoch [7/10], Average Loss: 0.2806\n",
            "Epoch [8/10], Average Loss: 0.2609\n",
            "Epoch [9/10], Average Loss: 0.2588\n",
            "Epoch [10/10], Average Loss: 0.2247\n",
            "Test Accuracy: 0.9064\n",
            "Test F1 Score: 0.8273\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.2825\n",
            "Epoch [2/10], Average Loss: 0.3424\n",
            "Epoch [3/10], Average Loss: 0.2243\n",
            "Epoch [4/10], Average Loss: 0.1788\n",
            "Epoch [5/10], Average Loss: 0.1483\n",
            "Epoch [6/10], Average Loss: 0.1279\n",
            "Epoch [7/10], Average Loss: 0.1146\n",
            "Epoch [8/10], Average Loss: 0.1053\n",
            "Epoch [9/10], Average Loss: 0.0977\n",
            "Epoch [10/10], Average Loss: 0.0921\n",
            "Test Accuracy: 0.9205\n",
            "Test F1 Score: 0.8506\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.3528\n",
            "Epoch [2/10], Average Loss: 0.6641\n",
            "Epoch [3/10], Average Loss: 0.5138\n",
            "Epoch [4/10], Average Loss: 0.4395\n",
            "Epoch [5/10], Average Loss: 0.3621\n",
            "Epoch [6/10], Average Loss: 0.3139\n",
            "Epoch [7/10], Average Loss: 0.2888\n",
            "Epoch [8/10], Average Loss: 0.2781\n",
            "Epoch [9/10], Average Loss: 0.2721\n",
            "Epoch [10/10], Average Loss: 0.2427\n",
            "Test Accuracy: 0.9070\n",
            "Test F1 Score: 0.8204\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.0131\n",
            "Epoch [2/10], Average Loss: 0.2790\n",
            "Epoch [3/10], Average Loss: 0.1923\n",
            "Epoch [4/10], Average Loss: 0.1488\n",
            "Epoch [5/10], Average Loss: 0.1227\n",
            "Epoch [6/10], Average Loss: 0.1084\n",
            "Epoch [7/10], Average Loss: 0.1010\n",
            "Epoch [8/10], Average Loss: 0.0875\n",
            "Epoch [9/10], Average Loss: 0.0836\n",
            "Epoch [10/10], Average Loss: 0.0792\n",
            "Test Accuracy: 0.9212\n",
            "Test F1 Score: 0.8477\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2853\n",
            "Epoch [2/10], Average Loss: 1.8883\n",
            "Epoch [3/10], Average Loss: 1.6864\n",
            "Epoch [4/10], Average Loss: 1.5330\n",
            "Epoch [5/10], Average Loss: 1.3482\n",
            "Epoch [6/10], Average Loss: 1.1628\n",
            "Epoch [7/10], Average Loss: 1.0441\n",
            "Epoch [8/10], Average Loss: 0.9521\n",
            "Epoch [9/10], Average Loss: 0.8593\n",
            "Epoch [10/10], Average Loss: 0.7680\n",
            "Test Accuracy: 0.7614\n",
            "Test F1 Score: 0.5530\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.2656\n",
            "Epoch [2/10], Average Loss: 1.7856\n",
            "Epoch [3/10], Average Loss: 1.4581\n",
            "Epoch [4/10], Average Loss: 1.1719\n",
            "Epoch [5/10], Average Loss: 1.0160\n",
            "Epoch [6/10], Average Loss: 0.9078\n",
            "Epoch [7/10], Average Loss: 0.7571\n",
            "Epoch [8/10], Average Loss: 0.6300\n",
            "Epoch [9/10], Average Loss: 0.5325\n",
            "Epoch [10/10], Average Loss: 0.4654\n",
            "Test Accuracy: 0.8272\n",
            "Test F1 Score: 0.7200\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2460\n",
            "Epoch [2/10], Average Loss: 1.8143\n",
            "Epoch [3/10], Average Loss: 1.5655\n",
            "Epoch [4/10], Average Loss: 1.2909\n",
            "Epoch [5/10], Average Loss: 1.1065\n",
            "Epoch [6/10], Average Loss: 0.9311\n",
            "Epoch [7/10], Average Loss: 0.7914\n",
            "Epoch [8/10], Average Loss: 0.6991\n",
            "Epoch [9/10], Average Loss: 0.6327\n",
            "Epoch [10/10], Average Loss: 0.5825\n",
            "Test Accuracy: 0.7885\n",
            "Test F1 Score: 0.6420\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.1583\n",
            "Epoch [2/10], Average Loss: 1.1420\n",
            "Epoch [3/10], Average Loss: 0.7385\n",
            "Epoch [4/10], Average Loss: 0.5606\n",
            "Epoch [5/10], Average Loss: 0.4411\n",
            "Epoch [6/10], Average Loss: 0.3493\n",
            "Epoch [7/10], Average Loss: 0.2851\n",
            "Epoch [8/10], Average Loss: 0.2429\n",
            "Epoch [9/10], Average Loss: 0.2073\n",
            "Epoch [10/10], Average Loss: 0.1830\n",
            "Test Accuracy: 0.9113\n",
            "Test F1 Score: 0.8281\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6625\n",
            "Epoch [2/10], Average Loss: 2.4110\n",
            "Epoch [3/10], Average Loss: 2.3149\n",
            "Epoch [4/10], Average Loss: 2.2544\n",
            "Epoch [5/10], Average Loss: 2.2091\n",
            "Epoch [6/10], Average Loss: 2.1703\n",
            "Epoch [7/10], Average Loss: 2.1342\n",
            "Epoch [8/10], Average Loss: 2.0997\n",
            "Epoch [9/10], Average Loss: 2.0685\n",
            "Epoch [10/10], Average Loss: 2.0367\n",
            "Test Accuracy: 0.3716\n",
            "Test F1 Score: 0.1540\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6609\n",
            "Epoch [2/10], Average Loss: 2.3891\n",
            "Epoch [3/10], Average Loss: 2.2781\n",
            "Epoch [4/10], Average Loss: 2.2139\n",
            "Epoch [5/10], Average Loss: 2.1673\n",
            "Epoch [6/10], Average Loss: 2.1253\n",
            "Epoch [7/10], Average Loss: 2.0875\n",
            "Epoch [8/10], Average Loss: 2.0505\n",
            "Epoch [9/10], Average Loss: 2.0109\n",
            "Epoch [10/10], Average Loss: 1.9679\n",
            "Test Accuracy: 0.4115\n",
            "Test F1 Score: 0.1735\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6038\n",
            "Epoch [2/10], Average Loss: 2.3716\n",
            "Epoch [3/10], Average Loss: 2.2891\n",
            "Epoch [4/10], Average Loss: 2.2269\n",
            "Epoch [5/10], Average Loss: 2.1803\n",
            "Epoch [6/10], Average Loss: 2.1386\n",
            "Epoch [7/10], Average Loss: 2.1023\n",
            "Epoch [8/10], Average Loss: 2.0706\n",
            "Epoch [9/10], Average Loss: 2.0370\n",
            "Epoch [10/10], Average Loss: 2.0025\n",
            "Test Accuracy: 0.3583\n",
            "Test F1 Score: 0.1364\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.5835\n",
            "Epoch [2/10], Average Loss: 2.3235\n",
            "Epoch [3/10], Average Loss: 2.2310\n",
            "Epoch [4/10], Average Loss: 2.1689\n",
            "Epoch [5/10], Average Loss: 2.1207\n",
            "Epoch [6/10], Average Loss: 2.0713\n",
            "Epoch [7/10], Average Loss: 2.0152\n",
            "Epoch [8/10], Average Loss: 1.9461\n",
            "Epoch [9/10], Average Loss: 1.8775\n",
            "Epoch [10/10], Average Loss: 1.8246\n",
            "Test Accuracy: 0.4405\n",
            "Test F1 Score: 0.1796\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.6225\n",
            "Epoch [2/10], Average Loss: 0.8968\n",
            "Epoch [3/10], Average Loss: 0.6482\n",
            "Epoch [4/10], Average Loss: 0.4997\n",
            "Epoch [5/10], Average Loss: 0.4052\n",
            "Epoch [6/10], Average Loss: 0.3470\n",
            "Epoch [7/10], Average Loss: 0.3085\n",
            "Epoch [8/10], Average Loss: 0.2783\n",
            "Epoch [9/10], Average Loss: 0.2550\n",
            "Epoch [10/10], Average Loss: 0.2441\n",
            "Test Accuracy: 0.9059\n",
            "Test F1 Score: 0.8141\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.1621\n",
            "Epoch [2/10], Average Loss: 0.3296\n",
            "Epoch [3/10], Average Loss: 0.2247\n",
            "Epoch [4/10], Average Loss: 0.1796\n",
            "Epoch [5/10], Average Loss: 0.1513\n",
            "Epoch [6/10], Average Loss: 0.1352\n",
            "Epoch [7/10], Average Loss: 0.1192\n",
            "Epoch [8/10], Average Loss: 0.1095\n",
            "Epoch [9/10], Average Loss: 0.1035\n",
            "Epoch [10/10], Average Loss: 0.0950\n",
            "Test Accuracy: 0.9160\n",
            "Test F1 Score: 0.8451\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.1306\n",
            "Epoch [2/10], Average Loss: 0.5637\n",
            "Epoch [3/10], Average Loss: 0.4615\n",
            "Epoch [4/10], Average Loss: 0.4026\n",
            "Epoch [5/10], Average Loss: 0.3571\n",
            "Epoch [6/10], Average Loss: 0.3332\n",
            "Epoch [7/10], Average Loss: 0.3052\n",
            "Epoch [8/10], Average Loss: 0.4271\n",
            "Epoch [9/10], Average Loss: 0.4418\n",
            "Epoch [10/10], Average Loss: 0.4019\n",
            "Test Accuracy: 0.8777\n",
            "Test F1 Score: 0.7903\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.0171\n",
            "Epoch [2/10], Average Loss: 0.2904\n",
            "Epoch [3/10], Average Loss: 0.1967\n",
            "Epoch [4/10], Average Loss: 0.1544\n",
            "Epoch [5/10], Average Loss: 0.1295\n",
            "Epoch [6/10], Average Loss: 0.1142\n",
            "Epoch [7/10], Average Loss: 0.1026\n",
            "Epoch [8/10], Average Loss: 0.0947\n",
            "Epoch [9/10], Average Loss: 0.0885\n",
            "Epoch [10/10], Average Loss: 0.0849\n",
            "Test Accuracy: 0.9165\n",
            "Test F1 Score: 0.8444\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2848\n",
            "Epoch [2/10], Average Loss: 1.8854\n",
            "Epoch [3/10], Average Loss: 1.6865\n",
            "Epoch [4/10], Average Loss: 1.5241\n",
            "Epoch [5/10], Average Loss: 1.3147\n",
            "Epoch [6/10], Average Loss: 1.1537\n",
            "Epoch [7/10], Average Loss: 1.0386\n",
            "Epoch [8/10], Average Loss: 0.9475\n",
            "Epoch [9/10], Average Loss: 0.8640\n",
            "Epoch [10/10], Average Loss: 0.7767\n",
            "Test Accuracy: 0.7576\n",
            "Test F1 Score: 0.5256\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.2887\n",
            "Epoch [2/10], Average Loss: 1.8585\n",
            "Epoch [3/10], Average Loss: 1.5438\n",
            "Epoch [4/10], Average Loss: 1.3128\n",
            "Epoch [5/10], Average Loss: 1.0659\n",
            "Epoch [6/10], Average Loss: 0.9208\n",
            "Epoch [7/10], Average Loss: 0.7909\n",
            "Epoch [8/10], Average Loss: 0.6539\n",
            "Epoch [9/10], Average Loss: 0.5584\n",
            "Epoch [10/10], Average Loss: 0.4892\n",
            "Test Accuracy: 0.8218\n",
            "Test F1 Score: 0.7142\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2359\n",
            "Epoch [2/10], Average Loss: 1.6887\n",
            "Epoch [3/10], Average Loss: 1.4227\n",
            "Epoch [4/10], Average Loss: 1.1905\n",
            "Epoch [5/10], Average Loss: 0.9724\n",
            "Epoch [6/10], Average Loss: 0.8383\n",
            "Epoch [7/10], Average Loss: 0.7404\n",
            "Epoch [8/10], Average Loss: 0.6368\n",
            "Epoch [9/10], Average Loss: 0.5711\n",
            "Epoch [10/10], Average Loss: 0.5221\n",
            "Test Accuracy: 0.8324\n",
            "Test F1 Score: 0.6703\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.1567\n",
            "Epoch [2/10], Average Loss: 1.3770\n",
            "Epoch [3/10], Average Loss: 0.9803\n",
            "Epoch [4/10], Average Loss: 0.7578\n",
            "Epoch [5/10], Average Loss: 0.5786\n",
            "Epoch [6/10], Average Loss: 0.4724\n",
            "Epoch [7/10], Average Loss: 0.3762\n",
            "Epoch [8/10], Average Loss: 0.2940\n",
            "Epoch [9/10], Average Loss: 0.2430\n",
            "Epoch [10/10], Average Loss: 0.2078\n",
            "Test Accuracy: 0.9085\n",
            "Test F1 Score: 0.8307\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6624\n",
            "Epoch [2/10], Average Loss: 2.4286\n",
            "Epoch [3/10], Average Loss: 2.3402\n",
            "Epoch [4/10], Average Loss: 2.2821\n",
            "Epoch [5/10], Average Loss: 2.2354\n",
            "Epoch [6/10], Average Loss: 2.1952\n",
            "Epoch [7/10], Average Loss: 2.1589\n",
            "Epoch [8/10], Average Loss: 2.1260\n",
            "Epoch [9/10], Average Loss: 2.0920\n",
            "Epoch [10/10], Average Loss: 2.0588\n",
            "Test Accuracy: 0.3612\n",
            "Test F1 Score: 0.1461\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6652\n",
            "Epoch [2/10], Average Loss: 2.4128\n",
            "Epoch [3/10], Average Loss: 2.3086\n",
            "Epoch [4/10], Average Loss: 2.2462\n",
            "Epoch [5/10], Average Loss: 2.1971\n",
            "Epoch [6/10], Average Loss: 2.1531\n",
            "Epoch [7/10], Average Loss: 2.1181\n",
            "Epoch [8/10], Average Loss: 2.0848\n",
            "Epoch [9/10], Average Loss: 2.0536\n",
            "Epoch [10/10], Average Loss: 2.0215\n",
            "Test Accuracy: 0.3618\n",
            "Test F1 Score: 0.1542\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.5983\n",
            "Epoch [2/10], Average Loss: 2.3725\n",
            "Epoch [3/10], Average Loss: 2.2913\n",
            "Epoch [4/10], Average Loss: 2.2324\n",
            "Epoch [5/10], Average Loss: 2.1848\n",
            "Epoch [6/10], Average Loss: 2.1420\n",
            "Epoch [7/10], Average Loss: 2.1021\n",
            "Epoch [8/10], Average Loss: 2.0621\n",
            "Epoch [9/10], Average Loss: 2.0244\n",
            "Epoch [10/10], Average Loss: 1.9864\n",
            "Test Accuracy: 0.4085\n",
            "Test F1 Score: 0.1684\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.1, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.5904\n",
            "Epoch [2/10], Average Loss: 2.3412\n",
            "Epoch [3/10], Average Loss: 2.2499\n",
            "Epoch [4/10], Average Loss: 2.1876\n",
            "Epoch [5/10], Average Loss: 2.1401\n",
            "Epoch [6/10], Average Loss: 2.0923\n",
            "Epoch [7/10], Average Loss: 2.0382\n",
            "Epoch [8/10], Average Loss: 1.9679\n",
            "Epoch [9/10], Average Loss: 1.8983\n",
            "Epoch [10/10], Average Loss: 1.8389\n",
            "Test Accuracy: 0.4332\n",
            "Test F1 Score: 0.1825\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.3866\n",
            "Epoch [2/10], Average Loss: 0.6746\n",
            "Epoch [3/10], Average Loss: 0.4378\n",
            "Epoch [4/10], Average Loss: 0.3412\n",
            "Epoch [5/10], Average Loss: 0.2889\n",
            "Epoch [6/10], Average Loss: 0.2526\n",
            "Epoch [7/10], Average Loss: 0.2314\n",
            "Epoch [8/10], Average Loss: 0.2144\n",
            "Epoch [9/10], Average Loss: 0.1941\n",
            "Epoch [10/10], Average Loss: 0.1902\n",
            "Test Accuracy: 0.9095\n",
            "Test F1 Score: 0.8237\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.2977\n",
            "Epoch [2/10], Average Loss: 0.3776\n",
            "Epoch [3/10], Average Loss: 0.2353\n",
            "Epoch [4/10], Average Loss: 0.1809\n",
            "Epoch [5/10], Average Loss: 0.1503\n",
            "Epoch [6/10], Average Loss: 0.1322\n",
            "Epoch [7/10], Average Loss: 0.1198\n",
            "Epoch [8/10], Average Loss: 0.1072\n",
            "Epoch [9/10], Average Loss: 0.0886\n",
            "Epoch [10/10], Average Loss: 0.0794\n",
            "Test Accuracy: 0.9183\n",
            "Test F1 Score: 0.8383\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.0955\n",
            "Epoch [2/10], Average Loss: 0.5161\n",
            "Epoch [3/10], Average Loss: 0.4199\n",
            "Epoch [4/10], Average Loss: 0.3568\n",
            "Epoch [5/10], Average Loss: 0.3217\n",
            "Epoch [6/10], Average Loss: 0.2817\n",
            "Epoch [7/10], Average Loss: 0.2902\n",
            "Epoch [8/10], Average Loss: 0.3721\n",
            "Epoch [9/10], Average Loss: 0.3447\n",
            "Epoch [10/10], Average Loss: 0.3225\n",
            "Test Accuracy: 0.8909\n",
            "Test F1 Score: 0.7981\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.1627\n",
            "Epoch [2/10], Average Loss: 0.3067\n",
            "Epoch [3/10], Average Loss: 0.2033\n",
            "Epoch [4/10], Average Loss: 0.1566\n",
            "Epoch [5/10], Average Loss: 0.1247\n",
            "Epoch [6/10], Average Loss: 0.1039\n",
            "Epoch [7/10], Average Loss: 0.0901\n",
            "Epoch [8/10], Average Loss: 0.0772\n",
            "Epoch [9/10], Average Loss: 0.0695\n",
            "Epoch [10/10], Average Loss: 0.0637\n",
            "Test Accuracy: 0.9189\n",
            "Test F1 Score: 0.8512\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2788\n",
            "Epoch [2/10], Average Loss: 1.8759\n",
            "Epoch [3/10], Average Loss: 1.6753\n",
            "Epoch [4/10], Average Loss: 1.5229\n",
            "Epoch [5/10], Average Loss: 1.3593\n",
            "Epoch [6/10], Average Loss: 1.1670\n",
            "Epoch [7/10], Average Loss: 1.0362\n",
            "Epoch [8/10], Average Loss: 0.9424\n",
            "Epoch [9/10], Average Loss: 0.8667\n",
            "Epoch [10/10], Average Loss: 0.7814\n",
            "Test Accuracy: 0.7625\n",
            "Test F1 Score: 0.5344\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.2581\n",
            "Epoch [2/10], Average Loss: 1.6706\n",
            "Epoch [3/10], Average Loss: 1.3098\n",
            "Epoch [4/10], Average Loss: 1.1122\n",
            "Epoch [5/10], Average Loss: 0.9571\n",
            "Epoch [6/10], Average Loss: 0.8384\n",
            "Epoch [7/10], Average Loss: 0.6320\n",
            "Epoch [8/10], Average Loss: 0.4895\n",
            "Epoch [9/10], Average Loss: 0.4145\n",
            "Epoch [10/10], Average Loss: 0.3599\n",
            "Test Accuracy: 0.8710\n",
            "Test F1 Score: 0.7670\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2296\n",
            "Epoch [2/10], Average Loss: 1.6771\n",
            "Epoch [3/10], Average Loss: 1.4195\n",
            "Epoch [4/10], Average Loss: 1.2161\n",
            "Epoch [5/10], Average Loss: 0.9885\n",
            "Epoch [6/10], Average Loss: 0.8483\n",
            "Epoch [7/10], Average Loss: 0.7515\n",
            "Epoch [8/10], Average Loss: 0.6769\n",
            "Epoch [9/10], Average Loss: 0.5832\n",
            "Epoch [10/10], Average Loss: 0.5253\n",
            "Test Accuracy: 0.8397\n",
            "Test F1 Score: 0.6902\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.2361\n",
            "Epoch [2/10], Average Loss: 1.5304\n",
            "Epoch [3/10], Average Loss: 1.1942\n",
            "Epoch [4/10], Average Loss: 0.9618\n",
            "Epoch [5/10], Average Loss: 0.7454\n",
            "Epoch [6/10], Average Loss: 0.6095\n",
            "Epoch [7/10], Average Loss: 0.4764\n",
            "Epoch [8/10], Average Loss: 0.3833\n",
            "Epoch [9/10], Average Loss: 0.3125\n",
            "Epoch [10/10], Average Loss: 0.2585\n",
            "Test Accuracy: 0.9051\n",
            "Test F1 Score: 0.8204\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6720\n",
            "Epoch [2/10], Average Loss: 2.4221\n",
            "Epoch [3/10], Average Loss: 2.3298\n",
            "Epoch [4/10], Average Loss: 2.2672\n",
            "Epoch [5/10], Average Loss: 2.2206\n",
            "Epoch [6/10], Average Loss: 2.1828\n",
            "Epoch [7/10], Average Loss: 2.1498\n",
            "Epoch [8/10], Average Loss: 2.1191\n",
            "Epoch [9/10], Average Loss: 2.0866\n",
            "Epoch [10/10], Average Loss: 2.0582\n",
            "Test Accuracy: 0.3600\n",
            "Test F1 Score: 0.1376\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6609\n",
            "Epoch [2/10], Average Loss: 2.4141\n",
            "Epoch [3/10], Average Loss: 2.3050\n",
            "Epoch [4/10], Average Loss: 2.2343\n",
            "Epoch [5/10], Average Loss: 2.1844\n",
            "Epoch [6/10], Average Loss: 2.1420\n",
            "Epoch [7/10], Average Loss: 2.1050\n",
            "Epoch [8/10], Average Loss: 2.0714\n",
            "Epoch [9/10], Average Loss: 2.0366\n",
            "Epoch [10/10], Average Loss: 1.9990\n",
            "Test Accuracy: 0.3637\n",
            "Test F1 Score: 0.1494\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.5932\n",
            "Epoch [2/10], Average Loss: 2.3670\n",
            "Epoch [3/10], Average Loss: 2.2856\n",
            "Epoch [4/10], Average Loss: 2.2248\n",
            "Epoch [5/10], Average Loss: 2.1761\n",
            "Epoch [6/10], Average Loss: 2.1303\n",
            "Epoch [7/10], Average Loss: 2.0907\n",
            "Epoch [8/10], Average Loss: 2.0536\n",
            "Epoch [9/10], Average Loss: 2.0146\n",
            "Epoch [10/10], Average Loss: 1.9802\n",
            "Test Accuracy: 0.3984\n",
            "Test F1 Score: 0.1703\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.5903\n",
            "Epoch [2/10], Average Loss: 2.3215\n",
            "Epoch [3/10], Average Loss: 2.2230\n",
            "Epoch [4/10], Average Loss: 2.1590\n",
            "Epoch [5/10], Average Loss: 2.1052\n",
            "Epoch [6/10], Average Loss: 2.0509\n",
            "Epoch [7/10], Average Loss: 1.9846\n",
            "Epoch [8/10], Average Loss: 1.9143\n",
            "Epoch [9/10], Average Loss: 1.8555\n",
            "Epoch [10/10], Average Loss: 1.8037\n",
            "Test Accuracy: 0.4696\n",
            "Test F1 Score: 0.2008\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.3448\n",
            "Epoch [2/10], Average Loss: 0.6061\n",
            "Epoch [3/10], Average Loss: 0.4139\n",
            "Epoch [4/10], Average Loss: 0.3293\n",
            "Epoch [5/10], Average Loss: 0.2768\n",
            "Epoch [6/10], Average Loss: 0.2438\n",
            "Epoch [7/10], Average Loss: 0.2171\n",
            "Epoch [8/10], Average Loss: 0.2030\n",
            "Epoch [9/10], Average Loss: 0.1922\n",
            "Epoch [10/10], Average Loss: 0.1811\n",
            "Test Accuracy: 0.9095\n",
            "Test F1 Score: 0.8323\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.3938\n",
            "Epoch [2/10], Average Loss: 0.4349\n",
            "Epoch [3/10], Average Loss: 0.2421\n",
            "Epoch [4/10], Average Loss: 0.1825\n",
            "Epoch [5/10], Average Loss: 0.1504\n",
            "Epoch [6/10], Average Loss: 0.1253\n",
            "Epoch [7/10], Average Loss: 0.1076\n",
            "Epoch [8/10], Average Loss: 0.0957\n",
            "Epoch [9/10], Average Loss: 0.0934\n",
            "Epoch [10/10], Average Loss: 0.0851\n",
            "Test Accuracy: 0.9157\n",
            "Test F1 Score: 0.8519\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.6279\n",
            "Epoch [2/10], Average Loss: 0.8548\n",
            "Epoch [3/10], Average Loss: 0.6880\n",
            "Epoch [4/10], Average Loss: 0.5899\n",
            "Epoch [5/10], Average Loss: 0.5030\n",
            "Epoch [6/10], Average Loss: 0.4197\n",
            "Epoch [7/10], Average Loss: 0.3744\n",
            "Epoch [8/10], Average Loss: 0.3667\n",
            "Epoch [9/10], Average Loss: 0.3472\n",
            "Epoch [10/10], Average Loss: 0.3086\n",
            "Test Accuracy: 0.8830\n",
            "Test F1 Score: 0.7977\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.0090\n",
            "Epoch [2/10], Average Loss: 0.2812\n",
            "Epoch [3/10], Average Loss: 0.1851\n",
            "Epoch [4/10], Average Loss: 0.1435\n",
            "Epoch [5/10], Average Loss: 0.1190\n",
            "Epoch [6/10], Average Loss: 0.1017\n",
            "Epoch [7/10], Average Loss: 0.0880\n",
            "Epoch [8/10], Average Loss: 0.0795\n",
            "Epoch [9/10], Average Loss: 0.0749\n",
            "Epoch [10/10], Average Loss: 0.0678\n",
            "Test Accuracy: 0.9163\n",
            "Test F1 Score: 0.8548\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2770\n",
            "Epoch [2/10], Average Loss: 1.7984\n",
            "Epoch [3/10], Average Loss: 1.5526\n",
            "Epoch [4/10], Average Loss: 1.4081\n",
            "Epoch [5/10], Average Loss: 1.2718\n",
            "Epoch [6/10], Average Loss: 1.1385\n",
            "Epoch [7/10], Average Loss: 1.0252\n",
            "Epoch [8/10], Average Loss: 0.9411\n",
            "Epoch [9/10], Average Loss: 0.8757\n",
            "Epoch [10/10], Average Loss: 0.8069\n",
            "Test Accuracy: 0.7524\n",
            "Test F1 Score: 0.5221\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.3356\n",
            "Epoch [2/10], Average Loss: 1.5763\n",
            "Epoch [3/10], Average Loss: 1.2491\n",
            "Epoch [4/10], Average Loss: 1.0351\n",
            "Epoch [5/10], Average Loss: 0.8849\n",
            "Epoch [6/10], Average Loss: 0.7478\n",
            "Epoch [7/10], Average Loss: 0.6444\n",
            "Epoch [8/10], Average Loss: 0.5710\n",
            "Epoch [9/10], Average Loss: 0.4770\n",
            "Epoch [10/10], Average Loss: 0.4050\n",
            "Test Accuracy: 0.8812\n",
            "Test F1 Score: 0.7735\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2358\n",
            "Epoch [2/10], Average Loss: 1.6799\n",
            "Epoch [3/10], Average Loss: 1.4113\n",
            "Epoch [4/10], Average Loss: 1.1790\n",
            "Epoch [5/10], Average Loss: 0.9552\n",
            "Epoch [6/10], Average Loss: 0.8284\n",
            "Epoch [7/10], Average Loss: 0.7388\n",
            "Epoch [8/10], Average Loss: 0.6700\n",
            "Epoch [9/10], Average Loss: 0.5922\n",
            "Epoch [10/10], Average Loss: 0.5094\n",
            "Test Accuracy: 0.8484\n",
            "Test F1 Score: 0.7001\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.1676\n",
            "Epoch [2/10], Average Loss: 1.4101\n",
            "Epoch [3/10], Average Loss: 0.9052\n",
            "Epoch [4/10], Average Loss: 0.6742\n",
            "Epoch [5/10], Average Loss: 0.5012\n",
            "Epoch [6/10], Average Loss: 0.3951\n",
            "Epoch [7/10], Average Loss: 0.3178\n",
            "Epoch [8/10], Average Loss: 0.2569\n",
            "Epoch [9/10], Average Loss: 0.2136\n",
            "Epoch [10/10], Average Loss: 0.1814\n",
            "Test Accuracy: 0.9062\n",
            "Test F1 Score: 0.8231\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6569\n",
            "Epoch [2/10], Average Loss: 2.4046\n",
            "Epoch [3/10], Average Loss: 2.3059\n",
            "Epoch [4/10], Average Loss: 2.2482\n",
            "Epoch [5/10], Average Loss: 2.2051\n",
            "Epoch [6/10], Average Loss: 2.1677\n",
            "Epoch [7/10], Average Loss: 2.1343\n",
            "Epoch [8/10], Average Loss: 2.1020\n",
            "Epoch [9/10], Average Loss: 2.0695\n",
            "Epoch [10/10], Average Loss: 2.0414\n",
            "Test Accuracy: 0.3720\n",
            "Test F1 Score: 0.1492\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6491\n",
            "Epoch [2/10], Average Loss: 2.3998\n",
            "Epoch [3/10], Average Loss: 2.2951\n",
            "Epoch [4/10], Average Loss: 2.2307\n",
            "Epoch [5/10], Average Loss: 2.1832\n",
            "Epoch [6/10], Average Loss: 2.1438\n",
            "Epoch [7/10], Average Loss: 2.1074\n",
            "Epoch [8/10], Average Loss: 2.0720\n",
            "Epoch [9/10], Average Loss: 2.0348\n",
            "Epoch [10/10], Average Loss: 1.9974\n",
            "Test Accuracy: 0.3776\n",
            "Test F1 Score: 0.1618\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.5924\n",
            "Epoch [2/10], Average Loss: 2.3672\n",
            "Epoch [3/10], Average Loss: 2.2869\n",
            "Epoch [4/10], Average Loss: 2.2279\n",
            "Epoch [5/10], Average Loss: 2.1804\n",
            "Epoch [6/10], Average Loss: 2.1390\n",
            "Epoch [7/10], Average Loss: 2.0971\n",
            "Epoch [8/10], Average Loss: 2.0583\n",
            "Epoch [9/10], Average Loss: 2.0196\n",
            "Epoch [10/10], Average Loss: 1.9834\n",
            "Test Accuracy: 0.4063\n",
            "Test F1 Score: 0.1768\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.5838\n",
            "Epoch [2/10], Average Loss: 2.3246\n",
            "Epoch [3/10], Average Loss: 2.2260\n",
            "Epoch [4/10], Average Loss: 2.1629\n",
            "Epoch [5/10], Average Loss: 2.1130\n",
            "Epoch [6/10], Average Loss: 2.0643\n",
            "Epoch [7/10], Average Loss: 2.0048\n",
            "Epoch [8/10], Average Loss: 1.9361\n",
            "Epoch [9/10], Average Loss: 1.8700\n",
            "Epoch [10/10], Average Loss: 1.8131\n",
            "Test Accuracy: 0.4375\n",
            "Test F1 Score: 0.1799\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.3289\n",
            "Epoch [2/10], Average Loss: 0.6678\n",
            "Epoch [3/10], Average Loss: 0.4994\n",
            "Epoch [4/10], Average Loss: 0.3916\n",
            "Epoch [5/10], Average Loss: 0.3280\n",
            "Epoch [6/10], Average Loss: 0.2916\n",
            "Epoch [7/10], Average Loss: 0.2597\n",
            "Epoch [8/10], Average Loss: 0.2426\n",
            "Epoch [9/10], Average Loss: 0.2243\n",
            "Epoch [10/10], Average Loss: 0.2154\n",
            "Test Accuracy: 0.9093\n",
            "Test F1 Score: 0.8287\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.4554\n",
            "Epoch [2/10], Average Loss: 0.4307\n",
            "Epoch [3/10], Average Loss: 0.2465\n",
            "Epoch [4/10], Average Loss: 0.1866\n",
            "Epoch [5/10], Average Loss: 0.1564\n",
            "Epoch [6/10], Average Loss: 0.1321\n",
            "Epoch [7/10], Average Loss: 0.1181\n",
            "Epoch [8/10], Average Loss: 0.1067\n",
            "Epoch [9/10], Average Loss: 0.0975\n",
            "Epoch [10/10], Average Loss: 0.0896\n",
            "Test Accuracy: 0.9179\n",
            "Test F1 Score: 0.8552\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.3747\n",
            "Epoch [2/10], Average Loss: 0.6268\n",
            "Epoch [3/10], Average Loss: 0.4726\n",
            "Epoch [4/10], Average Loss: 0.3810\n",
            "Epoch [5/10], Average Loss: 0.3302\n",
            "Epoch [6/10], Average Loss: 0.2986\n",
            "Epoch [7/10], Average Loss: 0.2784\n",
            "Epoch [8/10], Average Loss: 0.2566\n",
            "Epoch [9/10], Average Loss: 0.2333\n",
            "Epoch [10/10], Average Loss: 0.2254\n",
            "Test Accuracy: 0.9055\n",
            "Test F1 Score: 0.8248\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.4899\n",
            "Epoch [2/10], Average Loss: 0.3937\n",
            "Epoch [3/10], Average Loss: 0.2473\n",
            "Epoch [4/10], Average Loss: 0.1924\n",
            "Epoch [5/10], Average Loss: 0.1619\n",
            "Epoch [6/10], Average Loss: 0.1374\n",
            "Epoch [7/10], Average Loss: 0.1179\n",
            "Epoch [8/10], Average Loss: 0.1066\n",
            "Epoch [9/10], Average Loss: 0.0962\n",
            "Epoch [10/10], Average Loss: 0.0892\n",
            "Test Accuracy: 0.9189\n",
            "Test F1 Score: 0.8472\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2907\n",
            "Epoch [2/10], Average Loss: 1.8871\n",
            "Epoch [3/10], Average Loss: 1.6968\n",
            "Epoch [4/10], Average Loss: 1.5217\n",
            "Epoch [5/10], Average Loss: 1.3100\n",
            "Epoch [6/10], Average Loss: 1.1587\n",
            "Epoch [7/10], Average Loss: 1.0458\n",
            "Epoch [8/10], Average Loss: 0.9602\n",
            "Epoch [9/10], Average Loss: 0.8732\n",
            "Epoch [10/10], Average Loss: 0.7784\n",
            "Test Accuracy: 0.7630\n",
            "Test F1 Score: 0.5519\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.3116\n",
            "Epoch [2/10], Average Loss: 1.8882\n",
            "Epoch [3/10], Average Loss: 1.5876\n",
            "Epoch [4/10], Average Loss: 1.3840\n",
            "Epoch [5/10], Average Loss: 1.1532\n",
            "Epoch [6/10], Average Loss: 0.9734\n",
            "Epoch [7/10], Average Loss: 0.8397\n",
            "Epoch [8/10], Average Loss: 0.6978\n",
            "Epoch [9/10], Average Loss: 0.5959\n",
            "Epoch [10/10], Average Loss: 0.5243\n",
            "Test Accuracy: 0.8148\n",
            "Test F1 Score: 0.6958\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2646\n",
            "Epoch [2/10], Average Loss: 1.8223\n",
            "Epoch [3/10], Average Loss: 1.5187\n",
            "Epoch [4/10], Average Loss: 1.2865\n",
            "Epoch [5/10], Average Loss: 1.1206\n",
            "Epoch [6/10], Average Loss: 0.9608\n",
            "Epoch [7/10], Average Loss: 0.8195\n",
            "Epoch [8/10], Average Loss: 0.7234\n",
            "Epoch [9/10], Average Loss: 0.6539\n",
            "Epoch [10/10], Average Loss: 0.5986\n",
            "Test Accuracy: 0.7842\n",
            "Test F1 Score: 0.6411\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.1323\n",
            "Epoch [2/10], Average Loss: 1.2354\n",
            "Epoch [3/10], Average Loss: 0.7965\n",
            "Epoch [4/10], Average Loss: 0.6086\n",
            "Epoch [5/10], Average Loss: 0.4886\n",
            "Epoch [6/10], Average Loss: 0.3978\n",
            "Epoch [7/10], Average Loss: 0.3209\n",
            "Epoch [8/10], Average Loss: 0.2673\n",
            "Epoch [9/10], Average Loss: 0.2272\n",
            "Epoch [10/10], Average Loss: 0.1975\n",
            "Test Accuracy: 0.9088\n",
            "Test F1 Score: 0.8348\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6707\n",
            "Epoch [2/10], Average Loss: 2.4231\n",
            "Epoch [3/10], Average Loss: 2.3279\n",
            "Epoch [4/10], Average Loss: 2.2691\n",
            "Epoch [5/10], Average Loss: 2.2237\n",
            "Epoch [6/10], Average Loss: 2.1846\n",
            "Epoch [7/10], Average Loss: 2.1493\n",
            "Epoch [8/10], Average Loss: 2.1167\n",
            "Epoch [9/10], Average Loss: 2.0822\n",
            "Epoch [10/10], Average Loss: 2.0471\n",
            "Test Accuracy: 0.3654\n",
            "Test F1 Score: 0.1413\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6647\n",
            "Epoch [2/10], Average Loss: 2.4028\n",
            "Epoch [3/10], Average Loss: 2.3017\n",
            "Epoch [4/10], Average Loss: 2.2375\n",
            "Epoch [5/10], Average Loss: 2.1898\n",
            "Epoch [6/10], Average Loss: 2.1476\n",
            "Epoch [7/10], Average Loss: 2.1114\n",
            "Epoch [8/10], Average Loss: 2.0767\n",
            "Epoch [9/10], Average Loss: 2.0424\n",
            "Epoch [10/10], Average Loss: 2.0034\n",
            "Test Accuracy: 0.3595\n",
            "Test F1 Score: 0.1457\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6069\n",
            "Epoch [2/10], Average Loss: 2.3844\n",
            "Epoch [3/10], Average Loss: 2.3066\n",
            "Epoch [4/10], Average Loss: 2.2437\n",
            "Epoch [5/10], Average Loss: 2.1929\n",
            "Epoch [6/10], Average Loss: 2.1519\n",
            "Epoch [7/10], Average Loss: 2.1127\n",
            "Epoch [8/10], Average Loss: 2.0743\n",
            "Epoch [9/10], Average Loss: 2.0407\n",
            "Epoch [10/10], Average Loss: 2.0040\n",
            "Test Accuracy: 0.3863\n",
            "Test F1 Score: 0.1697\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.1, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.5822\n",
            "Epoch [2/10], Average Loss: 2.3291\n",
            "Epoch [3/10], Average Loss: 2.2433\n",
            "Epoch [4/10], Average Loss: 2.1798\n",
            "Epoch [5/10], Average Loss: 2.1252\n",
            "Epoch [6/10], Average Loss: 2.0597\n",
            "Epoch [7/10], Average Loss: 1.9824\n",
            "Epoch [8/10], Average Loss: 1.9094\n",
            "Epoch [9/10], Average Loss: 1.8461\n",
            "Epoch [10/10], Average Loss: 1.7913\n",
            "Test Accuracy: 0.4698\n",
            "Test F1 Score: 0.2109\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.3397\n",
            "Epoch [2/10], Average Loss: 0.6573\n",
            "Epoch [3/10], Average Loss: 0.4954\n",
            "Epoch [4/10], Average Loss: 0.3994\n",
            "Epoch [5/10], Average Loss: 0.3518\n",
            "Epoch [6/10], Average Loss: 0.3064\n",
            "Epoch [7/10], Average Loss: 0.2846\n",
            "Epoch [8/10], Average Loss: 0.2602\n",
            "Epoch [9/10], Average Loss: 0.2523\n",
            "Epoch [10/10], Average Loss: 0.2403\n",
            "Test Accuracy: 0.9044\n",
            "Test F1 Score: 0.8236\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.0403\n",
            "Epoch [2/10], Average Loss: 0.7896\n",
            "Epoch [3/10], Average Loss: 0.4206\n",
            "Epoch [4/10], Average Loss: 0.2867\n",
            "Epoch [5/10], Average Loss: 0.2297\n",
            "Epoch [6/10], Average Loss: 0.1945\n",
            "Epoch [7/10], Average Loss: 0.1699\n",
            "Epoch [8/10], Average Loss: 0.1509\n",
            "Epoch [9/10], Average Loss: 0.1353\n",
            "Epoch [10/10], Average Loss: 0.1247\n",
            "Test Accuracy: 0.9156\n",
            "Test F1 Score: 0.8455\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 1.1572\n",
            "Epoch [2/10], Average Loss: 0.5466\n",
            "Epoch [3/10], Average Loss: 0.4255\n",
            "Epoch [4/10], Average Loss: 0.3506\n",
            "Epoch [5/10], Average Loss: 0.3030\n",
            "Epoch [6/10], Average Loss: 0.2736\n",
            "Epoch [7/10], Average Loss: 0.2499\n",
            "Epoch [8/10], Average Loss: 0.2330\n",
            "Epoch [9/10], Average Loss: 0.2221\n",
            "Epoch [10/10], Average Loss: 0.2137\n",
            "Test Accuracy: 0.9088\n",
            "Test F1 Score: 0.8335\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 1.7830\n",
            "Epoch [2/10], Average Loss: 1.2940\n",
            "Epoch [3/10], Average Loss: 0.5290\n",
            "Epoch [4/10], Average Loss: 0.3519\n",
            "Epoch [5/10], Average Loss: 0.2665\n",
            "Epoch [6/10], Average Loss: 0.2250\n",
            "Epoch [7/10], Average Loss: 0.1920\n",
            "Epoch [8/10], Average Loss: 0.1715\n",
            "Epoch [9/10], Average Loss: 0.1575\n",
            "Epoch [10/10], Average Loss: 0.1386\n",
            "Test Accuracy: 0.9111\n",
            "Test F1 Score: 0.8431\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2975\n",
            "Epoch [2/10], Average Loss: 1.8966\n",
            "Epoch [3/10], Average Loss: 1.7017\n",
            "Epoch [4/10], Average Loss: 1.5531\n",
            "Epoch [5/10], Average Loss: 1.3999\n",
            "Epoch [6/10], Average Loss: 1.2144\n",
            "Epoch [7/10], Average Loss: 1.0827\n",
            "Epoch [8/10], Average Loss: 0.9846\n",
            "Epoch [9/10], Average Loss: 0.8959\n",
            "Epoch [10/10], Average Loss: 0.8069\n",
            "Test Accuracy: 0.7501\n",
            "Test F1 Score: 0.5116\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.2809\n",
            "Epoch [2/10], Average Loss: 1.8168\n",
            "Epoch [3/10], Average Loss: 1.5189\n",
            "Epoch [4/10], Average Loss: 1.2845\n",
            "Epoch [5/10], Average Loss: 1.0502\n",
            "Epoch [6/10], Average Loss: 0.9129\n",
            "Epoch [7/10], Average Loss: 0.7952\n",
            "Epoch [8/10], Average Loss: 0.6559\n",
            "Epoch [9/10], Average Loss: 0.5595\n",
            "Epoch [10/10], Average Loss: 0.4895\n",
            "Test Accuracy: 0.8246\n",
            "Test F1 Score: 0.7119\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.2436\n",
            "Epoch [2/10], Average Loss: 1.8103\n",
            "Epoch [3/10], Average Loss: 1.5454\n",
            "Epoch [4/10], Average Loss: 1.2852\n",
            "Epoch [5/10], Average Loss: 1.1077\n",
            "Epoch [6/10], Average Loss: 0.9351\n",
            "Epoch [7/10], Average Loss: 0.7954\n",
            "Epoch [8/10], Average Loss: 0.6989\n",
            "Epoch [9/10], Average Loss: 0.6345\n",
            "Epoch [10/10], Average Loss: 0.5840\n",
            "Test Accuracy: 0.7927\n",
            "Test F1 Score: 0.6480\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.2775\n",
            "Epoch [2/10], Average Loss: 1.2522\n",
            "Epoch [3/10], Average Loss: 0.8023\n",
            "Epoch [4/10], Average Loss: 0.6148\n",
            "Epoch [5/10], Average Loss: 0.4899\n",
            "Epoch [6/10], Average Loss: 0.3950\n",
            "Epoch [7/10], Average Loss: 0.3239\n",
            "Epoch [8/10], Average Loss: 0.2760\n",
            "Epoch [9/10], Average Loss: 0.2382\n",
            "Epoch [10/10], Average Loss: 0.2084\n",
            "Test Accuracy: 0.9079\n",
            "Test F1 Score: 0.8323\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.6697\n",
            "Epoch [2/10], Average Loss: 2.4256\n",
            "Epoch [3/10], Average Loss: 2.3272\n",
            "Epoch [4/10], Average Loss: 2.2640\n",
            "Epoch [5/10], Average Loss: 2.2153\n",
            "Epoch [6/10], Average Loss: 2.1744\n",
            "Epoch [7/10], Average Loss: 2.1373\n",
            "Epoch [8/10], Average Loss: 2.1021\n",
            "Epoch [9/10], Average Loss: 2.0679\n",
            "Epoch [10/10], Average Loss: 2.0380\n",
            "Test Accuracy: 0.3650\n",
            "Test F1 Score: 0.1391\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 128, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.6727\n",
            "Epoch [2/10], Average Loss: 2.4378\n",
            "Epoch [3/10], Average Loss: 2.3311\n",
            "Epoch [4/10], Average Loss: 2.2632\n",
            "Epoch [5/10], Average Loss: 2.2149\n",
            "Epoch [6/10], Average Loss: 2.1734\n",
            "Epoch [7/10], Average Loss: 2.1381\n",
            "Epoch [8/10], Average Loss: 2.1034\n",
            "Epoch [9/10], Average Loss: 2.0676\n",
            "Epoch [10/10], Average Loss: 2.0315\n",
            "Test Accuracy: 0.3756\n",
            "Test F1 Score: 0.1544\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 1}\n",
            "Epoch [1/10], Average Loss: 2.5877\n",
            "Epoch [2/10], Average Loss: 2.3389\n",
            "Epoch [3/10], Average Loss: 2.2533\n",
            "Epoch [4/10], Average Loss: 2.1953\n",
            "Epoch [5/10], Average Loss: 2.1509\n",
            "Epoch [6/10], Average Loss: 2.1079\n",
            "Epoch [7/10], Average Loss: 2.0645\n",
            "Epoch [8/10], Average Loss: 2.0223\n",
            "Epoch [9/10], Average Loss: 1.9811\n",
            "Epoch [10/10], Average Loss: 1.9399\n",
            "Test Accuracy: 0.3964\n",
            "Test F1 Score: 0.1615\n",
            "Testing parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Training with parameters: {'attention_hidden_sizes': [256, 128], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.2, 'dropout_prob_out': 0.2, 'learning_rate': 0.0001, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Epoch [1/10], Average Loss: 2.5868\n",
            "Epoch [2/10], Average Loss: 2.3265\n",
            "Epoch [3/10], Average Loss: 2.2256\n",
            "Epoch [4/10], Average Loss: 2.1598\n",
            "Epoch [5/10], Average Loss: 2.1079\n",
            "Epoch [6/10], Average Loss: 2.0483\n",
            "Epoch [7/10], Average Loss: 1.9756\n",
            "Epoch [8/10], Average Loss: 1.9056\n",
            "Epoch [9/10], Average Loss: 1.8480\n",
            "Epoch [10/10], Average Loss: 1.7925\n",
            "Test Accuracy: 0.4792\n",
            "Test F1 Score: 0.2098\n",
            "Best hyperparameters: {'attention_hidden_sizes': [128, 64], 'dropout_prob_att': 0.2, 'dropout_prob_emb': 0.1, 'dropout_prob_out': 0.1, 'learning_rate': 0.01, 'lstm_hidden_dim': 256, 'lstm_stacks': 2}\n",
            "Best development macro F1 score: 0.8565\n"
          ]
        }
      ],
      "source": [
        "# hyperparameter grid\n",
        "param_grid = {\n",
        "    \"lstm_hidden_dim\": [128, 256],  # LSTM hidden layer sizes\n",
        "    \"dropout_prob_emb\": [0.1, 0.2],  # Dropout probability for embeddings\n",
        "    \"dropout_prob_att\": [0.1, 0.2],  # Dropout probability for attention layer\n",
        "    \"dropout_prob_out\": [0.1, 0.2],  # Dropout probability for output layer\n",
        "    \"attention_hidden_sizes\": [[128, 64], [256, 128]],  # Attention MLP sizes\n",
        "    \"learning_rate\": [0.01,0.001,0.0001],  # Learning rates\n",
        "    \"lstm_stacks\": [1, 2],  # Number of LSTM layers\n",
        "}\n",
        "\n",
        "best_score = 0\n",
        "best_params = None\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "    print(f\"Testing parameters: {params}\")\n",
        "\n",
        "    lstm_hidden_dim = params[\"lstm_hidden_dim\"]\n",
        "    dropout_prob_emb = params[\"dropout_prob_emb\"]\n",
        "    dropout_prob_att = params[\"dropout_prob_att\"]\n",
        "    dropout_prob_out = params[\"dropout_prob_out\"]\n",
        "    attention_hidden_sizes = params[\"attention_hidden_sizes\"]\n",
        "    learning_rate = params[\"learning_rate\"]\n",
        "    lstm_stacks = params[\"lstm_stacks\"]\n",
        "    epochs = 10\n",
        "\n",
        "    model = BiLSTM_Attention(\n",
        "        input_dim=embedding_dim,\n",
        "        n_classes=len(tag2idx),\n",
        "        dropout_prob_emb=dropout_prob_emb,\n",
        "        dropout_prob_att=dropout_prob_att,\n",
        "        dropout_prob_out=dropout_prob_out,\n",
        "        hidden_dim=128,\n",
        "        lstm_hidden_dim=lstm_hidden_dim,\n",
        "        lstm_stacks=lstm_stacks,\n",
        "        attention_hidden_sizes=attention_hidden_sizes,\n",
        "        max_words=len(word2idx),\n",
        "        matrix_embeddings=embedding_matrix,\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    print(f\"Training with parameters: {params}\")\n",
        "    dev_accuracy, dev_macroF1_score = UtilsBILSTM.train_and_evaluate(model, optimizer, criterion, device, epochs, train_loader, dev_loader)\n",
        "\n",
        "    # Update best parameters if current macro F1 score is better\n",
        "    if dev_macroF1_score > best_score:\n",
        "        best_score = dev_macroF1_score\n",
        "        best_params = params\n",
        "\n",
        "print(f\"Best hyperparameters: {best_params}\")\n",
        "print(f\"Best development macro F1 score: {best_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJR6wrP-nc8s",
        "outputId": "57fe2a7c-1c60-4191-94cf-4840eabb6796"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Training Loss: 1.2964, Accuracy: 0.6233, F1 Score: 0.6068\n",
            "Validation Loss: 0.5637, Accuracy: 0.8486, F1 Score: 0.8465\n",
            "Epoch 2/10, Training Loss: 0.3353, Accuracy: 0.9154, F1 Score: 0.9127\n",
            "Validation Loss: 0.3599, Accuracy: 0.8969, F1 Score: 0.8989\n",
            "Epoch 3/10, Training Loss: 0.2029, Accuracy: 0.9460, F1 Score: 0.9452\n",
            "Validation Loss: 0.3287, Accuracy: 0.9064, F1 Score: 0.9074\n",
            "Epoch 4/10, Training Loss: 0.1503, Accuracy: 0.9588, F1 Score: 0.9584\n",
            "Validation Loss: 0.3408, Accuracy: 0.9055, F1 Score: 0.9072\n",
            "Epoch 5/10, Training Loss: 0.1186, Accuracy: 0.9671, F1 Score: 0.9668\n",
            "Validation Loss: 0.3807, Accuracy: 0.9076, F1 Score: 0.9096\n",
            "Epoch 6/10, Training Loss: 0.0924, Accuracy: 0.9743, F1 Score: 0.9741\n",
            "Validation Loss: 0.3706, Accuracy: 0.9072, F1 Score: 0.9082\n",
            "Epoch 7/10, Training Loss: 0.0826, Accuracy: 0.9771, F1 Score: 0.9770\n",
            "Validation Loss: 0.4079, Accuracy: 0.9106, F1 Score: 0.9108\n",
            "Epoch 8/10, Training Loss: 0.0740, Accuracy: 0.9797, F1 Score: 0.9795\n",
            "Validation Loss: 0.3900, Accuracy: 0.9136, F1 Score: 0.9137\n",
            "Epoch 9/10, Training Loss: 0.0611, Accuracy: 0.9831, F1 Score: 0.9830\n",
            "Validation Loss: 0.4049, Accuracy: 0.9111, F1 Score: 0.9112\n",
            "Epoch 10/10, Training Loss: 0.0475, Accuracy: 0.9869, F1 Score: 0.9868\n",
            "Validation Loss: 0.4381, Accuracy: 0.9101, F1 Score: 0.9101\n",
            "Training complete! Total time: 73.19 seconds\n",
            "Best model saved at: bilstm_best_model_epoch_3.pth\n"
          ]
        }
      ],
      "source": [
        "best_lstm_hidden_dim = best_params[\"lstm_hidden_dim\"]\n",
        "best_dropout_prob_emb = best_params[\"dropout_prob_emb\"]\n",
        "best_dropout_prob_att = best_params[\"dropout_prob_att\"]\n",
        "best_dropout_prob_out = best_params[\"dropout_prob_out\"]\n",
        "best_attention_hidden_sizes = best_params[\"attention_hidden_sizes\"]\n",
        "best_learning_rate = best_params[\"learning_rate\"]\n",
        "best_lstm_stacks = best_params[\"lstm_stacks\"]\n",
        "\n",
        "\n",
        "model2 = BiLSTM_Attention(\n",
        "        input_dim=embedding_dim,\n",
        "        n_classes=len(tag2idx),\n",
        "        dropout_prob_emb=best_dropout_prob_emb,\n",
        "        dropout_prob_att=best_dropout_prob_att,\n",
        "        dropout_prob_out=best_dropout_prob_out,\n",
        "        hidden_dim=128,\n",
        "        lstm_hidden_dim=best_lstm_hidden_dim,\n",
        "        lstm_stacks=best_lstm_stacks,\n",
        "        attention_hidden_sizes=best_attention_hidden_sizes,\n",
        "        max_words=len(word2idx),\n",
        "        matrix_embeddings=embedding_matrix,\n",
        "    )\n",
        "model2.to(device)\n",
        "\n",
        "optimizer = Adam(model2.parameters(), lr=best_learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "\n",
        "# Train and validate\n",
        "epochs_for_val = 10\n",
        "history, best_model_path = UtilsBILSTM.train_and_validate(model2, optimizer, criterion, device, epochs_for_val, train_loader, dev_loader)\n",
        "print(f\"Best model saved at: {best_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LfEMOWapnc8s",
        "outputId": "56cb4c08-1455-470d-ec6b-e1a83a9001de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12544"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_train_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjx0EG5-nc8s",
        "outputId": "20ed2d6e-d669-4f03-b58d-e73646588f58"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "Report.plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xsjSjCUnc8s"
      },
      "source": [
        "## Preparing Features and Labels (Tags) for POS Tagging with MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9Vdv-v1nc8t"
      },
      "source": [
        "We prepare data for training a Multi-Layer Perceptron (MLP) model to predict Part-of-Speech (POS) tags using sliding window word embeddings.\n",
        "\n",
        "We use the pre-trained `word2vec-google-news-300` model from Gensim's API to obtain 300-dimensional word embeddings for the words in our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfSfap19nc8t"
      },
      "source": [
        "**Processing Sentences**:\n",
        "   - The `UtilsPOSTagging.words` function extracts words from the input sentences, and `UtilsPOSTagging.tags` extracts the corresponding POS tags.\n",
        "   - For each word in a sentence, we retrieve its embedding using `UtilsPOSTagging.get_word_embedding`.\n",
        "   - The `UtilsPOSTagging.sliding_window` function creates feature vectors by applying a sliding window of a specified size (`window_size`=3) over word embeddings in each sentence.\n",
        "   - *Padding*: Zero vectors are added at the start and end of each sentence to ensure the sliding window operates fully on all words.\n",
        "   - For each word, the embeddings from its surrounding context (based on the window size) are concatenated to form a single feature vector.\n",
        "   - The POS tag of the center word in the window is used as the label.\n",
        "\n",
        "**Outputs**:\n",
        "   - **Features**: A 2D NumPy array where each row is a flattened feature vector of embeddings for a word and its context. Shape: `(number_of_words, window_size * embedding_dim)`.\n",
        "   - **Tags**: A 1D NumPy array containing the corresponding POS tags (labels) for the center words. Shape: `(number_of_words,)`.\n",
        "\n",
        "By the end of this process, we have a dataset ready for training the MLP model, with features representing the context of each word and labels indicating their corresponding POS tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW3NjZjLnc8t"
      },
      "outputs": [],
      "source": [
        "window_size = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz-286L4nc8t"
      },
      "source": [
        "\n",
        "Preprocess and encode tags for sequence labeling tasks using `LabelEncoder` from `sklearn`.\n",
        "\n",
        "- **Flatten Tag Sequences**: The tag sequences from the training data are flattened to create a single list of all tags.\n",
        "- **Encode Tags**: The encoder is fitted to generate a mapping from tags to numerical labels.\n",
        "- **Transform Datasets**: Tags in the training, development, and test sets are converted into numerical formats for model compatibility.\n",
        "- **Create Tag-to-Integer Mapping**: A dictionary (`tag_to_int`) maps each tag to its corresponding numerical value, making it easier to interpret and debug.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJgkFxQYnc8t",
        "outputId": "8fa93f75-4acd-44c3-cd8f-562047c5fbf6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_tags = [tag for sentence in train_as_tags for tag in sentence]\n",
        "tag_encoder = LabelEncoder()\n",
        "tag_encoder.fit(all_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCpIVy7Onc8t",
        "outputId": "8ac724d9-833f-4389-ab4a-6e6845811716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ADJ': 0, 'ADP': 1, 'ADV': 2, 'AUX': 3, 'CCONJ': 4, 'DET': 5, 'INTJ': 6, 'NOUN': 7, 'NUM': 8, 'PART': 9, 'PRON': 10, 'PROPN': 11, 'PUNCT': 12, 'SCONJ': 13, 'SYM': 14, 'VERB': 15, 'X': 16}\n"
          ]
        }
      ],
      "source": [
        "# Convert tags to numerical labels for the training set\n",
        "train_labels_numerical = [tag_encoder.transform(sentence) for sentence in train_as_tags]\n",
        "dev_labels_numerical = [tag_encoder.transform(sentence) for sentence in dev_as_tags]\n",
        "test_labels_numerical = [tag_encoder.transform(sentence) for sentence in test_as_tags]\n",
        "\n",
        "tag_to_int = dict(zip(tag_encoder.classes_, tag_encoder.transform(tag_encoder.classes_)))\n",
        "print(tag_to_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opeCUtjLnc8t"
      },
      "outputs": [],
      "source": [
        "train_features, train_labels = UtilsPOSTagging.sliding_window(train_as_embed, train_as_tags, window_size=3)\n",
        "dev_features, dev_labels = UtilsPOSTagging.sliding_window(dev_as_embed, dev_as_tags, window_size=3)\n",
        "test_features, test_labels = UtilsPOSTagging.sliding_window(test_as_embed, test_as_tags, window_size=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmijiBafnc8t"
      },
      "source": [
        "### **POS Tagging with MLP Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2KLOEVGnc8t"
      },
      "source": [
        "**Experiment Setup: Initial Testing with Arbitrary Hyperparameters**\n",
        "\n",
        "In this experiment, we aim to evaluate the performance of our POS Tagger classifier. To start, we set arbitrary hyperparameter values. This initial testing allows us to observe baseline performance and helps us identify potential adjustments for further tuning. Subsequent steps will involve tuning these parameters for optimal performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZuko7RPnc8u",
        "outputId": "f0bfa9c9-35c6-4308-f090-aa95e8d0de7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLPModel(\n",
            "  (layers): ModuleList(\n",
            "    (0): Linear(in_features=900, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "  )\n",
            "  (dropouts): ModuleList(\n",
            "    (0-1): 2 x Dropout(p=0.5, inplace=False)\n",
            "  )\n",
            "  (output_layer): Linear(in_features=256, out_features=17, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "input_size = 900\n",
        "# Model instance\n",
        "model = MLPModel(input_size=input_size, hidden_sizes=[512, 256], output_size=len(tag_encoder.classes_), dropout_probs=0.5)\n",
        "print(model)\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Optimizer and loss function\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Number of epochs\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_djogSGonc8u"
      },
      "outputs": [],
      "source": [
        "train_labels_flattened = [label for sentence in train_labels_numerical for label in sentence]\n",
        "dev_labels_flattened = [label for sentence in dev_labels_numerical for label in sentence]\n",
        "test_labels_flattened = [label for sentence in test_labels_numerical for label in sentence]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ju8OcNenc8u"
      },
      "outputs": [],
      "source": [
        "train_features_tensor = torch.tensor(train_features, dtype=torch.float32)\n",
        "train_labels_tensor = torch.tensor(train_labels_flattened, dtype=torch.long)\n",
        "train_dataset = TensorDataset(train_features_tensor, train_labels_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "dev_features_tensor = torch.tensor(dev_features, dtype=torch.float32)\n",
        "dev_labels_tensor = torch.tensor(dev_labels_flattened, dtype=torch.long)\n",
        "dev_dataset = TensorDataset(dev_features_tensor, dev_labels_tensor)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=32)\n",
        "\n",
        "test_features_tensor = torch.tensor(test_features, dtype=torch.float32)\n",
        "test_labels_tensor = torch.tensor(test_labels_flattened, dtype=torch.long)\n",
        "test_dataset = TensorDataset(test_features_tensor, test_labels_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFty0t6znc8u",
        "outputId": "8e9f48fd-4358-4faf-dbf1-49d6921f488e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5, Training Loss: 0.5630, Accuracy: 0.8243, F1 Score: 0.8197\n",
            "Validation Loss: 0.4538, Accuracy: 0.8552, F1 Score: 0.8503\n",
            "Epoch 2/5, Training Loss: 0.4407, Accuracy: 0.8593, F1 Score: 0.8560\n",
            "Validation Loss: 0.4310, Accuracy: 0.8601, F1 Score: 0.8543\n",
            "Epoch 3/5, Training Loss: 0.4076, Accuracy: 0.8684, F1 Score: 0.8656\n",
            "Validation Loss: 0.4246, Accuracy: 0.8633, F1 Score: 0.8607\n",
            "Epoch 4/5, Training Loss: 0.3870, Accuracy: 0.8737, F1 Score: 0.8712\n",
            "Validation Loss: 0.4298, Accuracy: 0.8624, F1 Score: 0.8580\n",
            "Epoch 5/5, Training Loss: 0.3711, Accuracy: 0.8779, F1 Score: 0.8757\n",
            "Validation Loss: 0.4202, Accuracy: 0.8638, F1 Score: 0.8594\n",
            "Training complete! Total time: 39.91 seconds\n"
          ]
        }
      ],
      "source": [
        "# Train and evaluate the MLP Classifier (Before hyperparameter tuning)\n",
        "test_accuracy, macro_f1 = UtilsMLP.train_and_validate(model, optimizer, criterion, device, epochs, train_loader, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euHYDRxSnc8u"
      },
      "source": [
        "**Hyperparameter Tuning for MLP Classifier**\n",
        "\n",
        "The hyperparameter tuning is performed using a grid search approach.\n",
        "\n",
        "**Hyperparameters Tuned**\n",
        "- **Hidden Layer Sizes**: The architecture of the MLP, specifying the number of hidden layers and their sizes.\n",
        "- **Dropout Probabilities**: Regularization parameter controlling the fraction of neurons dropped during training.\n",
        "- **Learning Rate**: The step size for the optimizer during weight updates.\n",
        "\n",
        "**Procedure**\n",
        "1. **Data Preparation**:\n",
        "   - For each configuration, the input data is reduced to a specific dimensionality, converted into tensors, and prepared using data loaders.\n",
        "2. **Model Initialization**:\n",
        "   - An MLP is created using the specified parameters, including input size, hidden layer sizes, and dropout probabilities.\n",
        "3. **Training**:\n",
        "   - The model is trained with the Adam optimizer and cross-entropy loss for a fixed number of epochs (50 epochs).\n",
        "4. **Evaluation**:\n",
        "   - The model is evaluated on the development set, with the macro F1 score used as the performance metric.\n",
        "\n",
        "The best-performing set of hyperparameters is identified by iterating through all possible combinations and selecting the one that maximizes the development set's macro F1 score. The final results include the best hyperparameters and their associated performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-r5T6aFnc8u",
        "outputId": "5fce85b4-330c-4d7a-d4e9-cfb9efdf498b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing parameters: {'dropout_probs': 0.2, 'hidden_sizes': [256, 128], 'learning_rate': 0.001}\n",
            "Epoch [1/10], Average Loss: 0.5224\n",
            "Epoch [2/10], Average Loss: 0.3856\n",
            "Epoch [3/10], Average Loss: 0.3421\n",
            "Epoch [4/10], Average Loss: 0.3111\n",
            "Epoch [5/10], Average Loss: 0.2886\n",
            "Epoch [6/10], Average Loss: 0.2701\n",
            "Epoch [7/10], Average Loss: 0.2556\n",
            "Epoch [8/10], Average Loss: 0.2459\n",
            "Epoch [9/10], Average Loss: 0.2352\n",
            "Epoch [10/10], Average Loss: 0.2277\n",
            "Development Accuracy: 0.8665\n",
            "Development F1 Score: 0.7764\n",
            "Testing parameters: {'dropout_probs': 0.2, 'hidden_sizes': [256, 128], 'learning_rate': 0.01}\n",
            "Epoch [1/10], Average Loss: 0.7351\n",
            "Epoch [2/10], Average Loss: 0.6776\n",
            "Epoch [3/10], Average Loss: 0.6573\n",
            "Epoch [4/10], Average Loss: 0.6469\n",
            "Epoch [5/10], Average Loss: 0.6409\n",
            "Epoch [6/10], Average Loss: 0.6429\n",
            "Epoch [7/10], Average Loss: 0.6414\n",
            "Epoch [8/10], Average Loss: 0.6377\n",
            "Epoch [9/10], Average Loss: 0.6313\n",
            "Epoch [10/10], Average Loss: 0.6302\n",
            "Development Accuracy: 0.8287\n",
            "Development F1 Score: 0.7123\n",
            "Testing parameters: {'dropout_probs': 0.2, 'hidden_sizes': [512, 256], 'learning_rate': 0.001}\n",
            "Epoch [1/10], Average Loss: 0.4927\n",
            "Epoch [2/10], Average Loss: 0.3615\n",
            "Epoch [3/10], Average Loss: 0.3096\n",
            "Epoch [4/10], Average Loss: 0.2737\n",
            "Epoch [5/10], Average Loss: 0.2475\n",
            "Epoch [6/10], Average Loss: 0.2277\n",
            "Epoch [7/10], Average Loss: 0.2159\n",
            "Epoch [8/10], Average Loss: 0.2044\n",
            "Epoch [9/10], Average Loss: 0.1953\n",
            "Epoch [10/10], Average Loss: 0.1904\n",
            "Development Accuracy: 0.8708\n",
            "Development F1 Score: 0.7881\n",
            "Testing parameters: {'dropout_probs': 0.2, 'hidden_sizes': [512, 256], 'learning_rate': 0.01}\n",
            "Epoch [1/10], Average Loss: 0.7964\n",
            "Epoch [2/10], Average Loss: 0.7467\n",
            "Epoch [3/10], Average Loss: 0.7342\n",
            "Epoch [4/10], Average Loss: 0.7244\n",
            "Epoch [5/10], Average Loss: 0.7371\n",
            "Epoch [6/10], Average Loss: 0.7286\n",
            "Epoch [7/10], Average Loss: 0.7263\n",
            "Epoch [8/10], Average Loss: 0.7577\n",
            "Epoch [9/10], Average Loss: 0.7525\n",
            "Epoch [10/10], Average Loss: 0.7632\n",
            "Development Accuracy: 0.8072\n",
            "Development F1 Score: 0.6959\n",
            "Testing parameters: {'dropout_probs': 0.2, 'hidden_sizes': [512, 256, 128], 'learning_rate': 0.001}\n",
            "Epoch [1/10], Average Loss: 0.5331\n",
            "Epoch [2/10], Average Loss: 0.3848\n",
            "Epoch [3/10], Average Loss: 0.3360\n",
            "Epoch [4/10], Average Loss: 0.3018\n",
            "Epoch [5/10], Average Loss: 0.2769\n",
            "Epoch [6/10], Average Loss: 0.2603\n",
            "Epoch [7/10], Average Loss: 0.2442\n",
            "Epoch [8/10], Average Loss: 0.2329\n",
            "Epoch [9/10], Average Loss: 0.2235\n",
            "Epoch [10/10], Average Loss: 0.2163\n",
            "Development Accuracy: 0.8673\n",
            "Development F1 Score: 0.7786\n",
            "Testing parameters: {'dropout_probs': 0.2, 'hidden_sizes': [512, 256, 128], 'learning_rate': 0.01}\n",
            "Epoch [1/10], Average Loss: 0.9769\n",
            "Epoch [2/10], Average Loss: 0.9655\n",
            "Epoch [3/10], Average Loss: 1.0026\n",
            "Epoch [4/10], Average Loss: 1.0052\n",
            "Epoch [5/10], Average Loss: 1.0177\n",
            "Epoch [6/10], Average Loss: 1.0655\n",
            "Epoch [7/10], Average Loss: 1.0368\n",
            "Epoch [8/10], Average Loss: 1.0485\n",
            "Epoch [9/10], Average Loss: 1.0853\n",
            "Epoch [10/10], Average Loss: 1.1393\n",
            "Development Accuracy: 0.7686\n",
            "Development F1 Score: 0.5580\n",
            "Testing parameters: {'dropout_probs': 0.2, 'hidden_sizes': [512, 256, 128, 64], 'learning_rate': 0.001}\n",
            "Epoch [1/10], Average Loss: 0.5923\n",
            "Epoch [2/10], Average Loss: 0.4179\n",
            "Epoch [3/10], Average Loss: 0.3659\n",
            "Epoch [4/10], Average Loss: 0.3334\n",
            "Epoch [5/10], Average Loss: 0.3084\n",
            "Epoch [6/10], Average Loss: 0.2893\n",
            "Epoch [7/10], Average Loss: 0.2759\n",
            "Epoch [8/10], Average Loss: 0.2621\n",
            "Epoch [9/10], Average Loss: 0.2532\n",
            "Epoch [10/10], Average Loss: 0.2436\n",
            "Development Accuracy: 0.8688\n",
            "Development F1 Score: 0.7864\n",
            "Testing parameters: {'dropout_probs': 0.2, 'hidden_sizes': [512, 256, 128, 64], 'learning_rate': 0.01}\n",
            "Epoch [1/10], Average Loss: 1.1720\n",
            "Epoch [2/10], Average Loss: 1.2363\n",
            "Epoch [3/10], Average Loss: 1.6270\n",
            "Epoch [4/10], Average Loss: 1.9305\n",
            "Epoch [5/10], Average Loss: 2.1477\n",
            "Epoch [6/10], Average Loss: 2.1984\n",
            "Epoch [7/10], Average Loss: 2.3255\n",
            "Epoch [8/10], Average Loss: 2.3000\n",
            "Epoch [9/10], Average Loss: 2.3721\n",
            "Epoch [10/10], Average Loss: 2.3622\n",
            "Development Accuracy: 0.2701\n",
            "Development F1 Score: 0.0529\n",
            "Testing parameters: {'dropout_probs': 0.3, 'hidden_sizes': [256, 128], 'learning_rate': 0.001}\n",
            "Epoch [1/10], Average Loss: 0.5475\n",
            "Epoch [2/10], Average Loss: 0.4122\n",
            "Epoch [3/10], Average Loss: 0.3719\n",
            "Epoch [4/10], Average Loss: 0.3476\n",
            "Epoch [5/10], Average Loss: 0.3266\n",
            "Epoch [6/10], Average Loss: 0.3113\n",
            "Epoch [7/10], Average Loss: 0.2972\n",
            "Epoch [8/10], Average Loss: 0.2890\n",
            "Epoch [9/10], Average Loss: 0.2819\n",
            "Epoch [10/10], Average Loss: 0.2721\n",
            "Development Accuracy: 0.8718\n",
            "Development F1 Score: 0.7837\n",
            "Testing parameters: {'dropout_probs': 0.3, 'hidden_sizes': [256, 128], 'learning_rate': 0.01}\n",
            "Epoch [1/10], Average Loss: 0.8229\n",
            "Epoch [2/10], Average Loss: 0.7844\n",
            "Epoch [3/10], Average Loss: 0.7663\n",
            "Epoch [4/10], Average Loss: 0.7625\n",
            "Epoch [5/10], Average Loss: 0.7535\n",
            "Epoch [6/10], Average Loss: 0.7460\n",
            "Epoch [7/10], Average Loss: 0.7405\n",
            "Epoch [8/10], Average Loss: 0.7374\n",
            "Epoch [9/10], Average Loss: 0.7471\n",
            "Epoch [10/10], Average Loss: 0.7390\n",
            "Development Accuracy: 0.8206\n",
            "Development F1 Score: 0.7088\n",
            "Testing parameters: {'dropout_probs': 0.3, 'hidden_sizes': [512, 256], 'learning_rate': 0.001}\n",
            "Epoch [1/10], Average Loss: 0.5091\n",
            "Epoch [2/10], Average Loss: 0.3801\n",
            "Epoch [3/10], Average Loss: 0.3369\n",
            "Epoch [4/10], Average Loss: 0.3053\n",
            "Epoch [5/10], Average Loss: 0.2838\n",
            "Epoch [6/10], Average Loss: 0.2663\n",
            "Epoch [7/10], Average Loss: 0.2550\n",
            "Epoch [8/10], Average Loss: 0.2435\n",
            "Epoch [9/10], Average Loss: 0.2361\n",
            "Epoch [10/10], Average Loss: 0.2273\n",
            "Development Accuracy: 0.8699\n",
            "Development F1 Score: 0.7835\n",
            "Testing parameters: {'dropout_probs': 0.3, 'hidden_sizes': [512, 256], 'learning_rate': 0.01}\n",
            "Epoch [1/10], Average Loss: 0.9010\n",
            "Epoch [2/10], Average Loss: 0.8774\n",
            "Epoch [3/10], Average Loss: 0.8681\n",
            "Epoch [4/10], Average Loss: 0.8867\n",
            "Epoch [5/10], Average Loss: 0.8875\n",
            "Epoch [6/10], Average Loss: 0.9028\n",
            "Epoch [7/10], Average Loss: 0.9149\n",
            "Epoch [8/10], Average Loss: 0.9501\n",
            "Epoch [9/10], Average Loss: 0.9735\n",
            "Epoch [10/10], Average Loss: 0.9807\n",
            "Development Accuracy: 0.7892\n",
            "Development F1 Score: 0.6584\n",
            "Testing parameters: {'dropout_probs': 0.3, 'hidden_sizes': [512, 256, 128], 'learning_rate': 0.001}\n",
            "Epoch [1/10], Average Loss: 0.5592\n",
            "Epoch [2/10], Average Loss: 0.4133\n",
            "Epoch [3/10], Average Loss: 0.3699\n",
            "Epoch [4/10], Average Loss: 0.3399\n",
            "Epoch [5/10], Average Loss: 0.3203\n",
            "Epoch [6/10], Average Loss: 0.3054\n",
            "Epoch [7/10], Average Loss: 0.2899\n",
            "Epoch [8/10], Average Loss: 0.2810\n",
            "Epoch [9/10], Average Loss: 0.2697\n",
            "Epoch [10/10], Average Loss: 0.2615\n",
            "Development Accuracy: 0.8705\n",
            "Development F1 Score: 0.7868\n",
            "Testing parameters: {'dropout_probs': 0.3, 'hidden_sizes': [512, 256, 128], 'learning_rate': 0.01}\n",
            "Epoch [1/10], Average Loss: 1.2499\n",
            "Epoch [2/10], Average Loss: 1.2434\n",
            "Epoch [3/10], Average Loss: 1.2813\n",
            "Epoch [4/10], Average Loss: 1.2968\n",
            "Epoch [5/10], Average Loss: 1.3136\n",
            "Epoch [6/10], Average Loss: 1.3818\n",
            "Epoch [7/10], Average Loss: 1.4083\n",
            "Epoch [8/10], Average Loss: 1.4814\n",
            "Epoch [9/10], Average Loss: 1.5426\n",
            "Epoch [10/10], Average Loss: 1.5285\n",
            "Development Accuracy: 0.6151\n",
            "Development F1 Score: 0.3677\n",
            "Testing parameters: {'dropout_probs': 0.3, 'hidden_sizes': [512, 256, 128, 64], 'learning_rate': 0.001}\n",
            "Epoch [1/10], Average Loss: 0.6363\n",
            "Epoch [2/10], Average Loss: 0.4551\n",
            "Epoch [3/10], Average Loss: 0.4063\n",
            "Epoch [4/10], Average Loss: 0.3780\n",
            "Epoch [5/10], Average Loss: 0.3584\n",
            "Epoch [6/10], Average Loss: 0.3440\n",
            "Epoch [7/10], Average Loss: 0.3310\n",
            "Epoch [8/10], Average Loss: 0.3205\n",
            "Epoch [9/10], Average Loss: 0.3133\n",
            "Epoch [10/10], Average Loss: 0.3048\n",
            "Development Accuracy: 0.8707\n",
            "Development F1 Score: 0.7795\n",
            "Testing parameters: {'dropout_probs': 0.3, 'hidden_sizes': [512, 256, 128, 64], 'learning_rate': 0.01}\n",
            "Epoch [1/10], Average Loss: 1.5475\n",
            "Epoch [2/10], Average Loss: 1.8723\n",
            "Epoch [3/10], Average Loss: 2.1845\n",
            "Epoch [4/10], Average Loss: 2.3901\n",
            "Epoch [5/10], Average Loss: 2.4121\n",
            "Epoch [6/10], Average Loss: 2.4244\n",
            "Epoch [7/10], Average Loss: 2.4068\n",
            "Epoch [8/10], Average Loss: 2.4132\n",
            "Epoch [9/10], Average Loss: 2.4205\n",
            "Epoch [10/10], Average Loss: 2.4108\n",
            "Development Accuracy: 0.2136\n",
            "Development F1 Score: 0.0417\n",
            "Testing parameters: {'dropout_probs': 0.5, 'hidden_sizes': [256, 128], 'learning_rate': 0.001}\n",
            "Epoch [1/10], Average Loss: 0.6236\n",
            "Epoch [2/10], Average Loss: 0.4804\n",
            "Epoch [3/10], Average Loss: 0.4501\n",
            "Epoch [4/10], Average Loss: 0.4300\n",
            "Epoch [5/10], Average Loss: 0.4174\n",
            "Epoch [6/10], Average Loss: 0.4059\n",
            "Epoch [7/10], Average Loss: 0.3933\n",
            "Epoch [8/10], Average Loss: 0.3857\n",
            "Epoch [9/10], Average Loss: 0.3807\n",
            "Epoch [10/10], Average Loss: 0.3725\n",
            "Development Accuracy: 0.8674\n",
            "Development F1 Score: 0.7707\n",
            "Testing parameters: {'dropout_probs': 0.5, 'hidden_sizes': [256, 128], 'learning_rate': 0.01}\n",
            "Epoch [1/10], Average Loss: 1.0780\n",
            "Epoch [2/10], Average Loss: 1.0505\n",
            "Epoch [3/10], Average Loss: 1.0541\n",
            "Epoch [4/10], Average Loss: 1.0502\n",
            "Epoch [5/10], Average Loss: 1.0441\n",
            "Epoch [6/10], Average Loss: 1.0369\n",
            "Epoch [7/10], Average Loss: 1.0439\n",
            "Epoch [8/10], Average Loss: 1.0472\n",
            "Epoch [9/10], Average Loss: 1.0538\n",
            "Epoch [10/10], Average Loss: 1.0631\n",
            "Development Accuracy: 0.7754\n",
            "Development F1 Score: 0.6438\n",
            "Testing parameters: {'dropout_probs': 0.5, 'hidden_sizes': [512, 256], 'learning_rate': 0.001}\n",
            "Epoch [1/10], Average Loss: 0.5630\n",
            "Epoch [2/10], Average Loss: 0.4401\n",
            "Epoch [3/10], Average Loss: 0.4066\n",
            "Epoch [4/10], Average Loss: 0.3867\n",
            "Epoch [5/10], Average Loss: 0.3731\n",
            "Epoch [6/10], Average Loss: 0.3586\n",
            "Epoch [7/10], Average Loss: 0.3484\n",
            "Epoch [8/10], Average Loss: 0.3395\n",
            "Epoch [9/10], Average Loss: 0.3322\n",
            "Epoch [10/10], Average Loss: 0.3263\n",
            "Development Accuracy: 0.8725\n",
            "Development F1 Score: 0.7774\n",
            "Testing parameters: {'dropout_probs': 0.5, 'hidden_sizes': [512, 256], 'learning_rate': 0.01}\n",
            "Epoch [1/10], Average Loss: 1.2715\n",
            "Epoch [2/10], Average Loss: 1.3245\n",
            "Epoch [3/10], Average Loss: 1.3622\n",
            "Epoch [4/10], Average Loss: 1.3816\n",
            "Epoch [5/10], Average Loss: 1.4376\n",
            "Epoch [6/10], Average Loss: 1.4794\n",
            "Epoch [7/10], Average Loss: 1.5693\n",
            "Epoch [8/10], Average Loss: 1.6278\n",
            "Epoch [9/10], Average Loss: 1.7207\n",
            "Epoch [10/10], Average Loss: 1.8741\n",
            "Development Accuracy: 0.5792\n",
            "Development F1 Score: 0.4404\n",
            "Testing parameters: {'dropout_probs': 0.5, 'hidden_sizes': [512, 256, 128], 'learning_rate': 0.001}\n",
            "Epoch [1/10], Average Loss: 0.6512\n",
            "Epoch [2/10], Average Loss: 0.4963\n",
            "Epoch [3/10], Average Loss: 0.4619\n",
            "Epoch [4/10], Average Loss: 0.4416\n",
            "Epoch [5/10], Average Loss: 0.4252\n",
            "Epoch [6/10], Average Loss: 0.4142\n",
            "Epoch [7/10], Average Loss: 0.4038\n",
            "Epoch [8/10], Average Loss: 0.3932\n",
            "Epoch [9/10], Average Loss: 0.3853\n",
            "Epoch [10/10], Average Loss: 0.3807\n",
            "Development Accuracy: 0.8699\n",
            "Development F1 Score: 0.7759\n",
            "Testing parameters: {'dropout_probs': 0.5, 'hidden_sizes': [512, 256, 128], 'learning_rate': 0.01}\n",
            "Epoch [1/10], Average Loss: 2.2663\n",
            "Epoch [2/10], Average Loss: 2.5220\n",
            "Epoch [3/10], Average Loss: 2.5160\n",
            "Epoch [4/10], Average Loss: 2.5149\n",
            "Epoch [5/10], Average Loss: 2.5147\n",
            "Epoch [6/10], Average Loss: 2.5147\n",
            "Epoch [7/10], Average Loss: 2.5146\n",
            "Epoch [8/10], Average Loss: 2.5148\n",
            "Epoch [9/10], Average Loss: 2.5185\n",
            "Epoch [10/10], Average Loss: 2.5155\n",
            "Development Accuracy: 0.1675\n",
            "Development F1 Score: 0.0169\n",
            "Testing parameters: {'dropout_probs': 0.5, 'hidden_sizes': [512, 256, 128, 64], 'learning_rate': 0.001}\n",
            "Epoch [1/10], Average Loss: 0.8230\n",
            "Epoch [2/10], Average Loss: 0.5931\n",
            "Epoch [3/10], Average Loss: 0.5467\n",
            "Epoch [4/10], Average Loss: 0.5208\n",
            "Epoch [5/10], Average Loss: 0.5041\n",
            "Epoch [6/10], Average Loss: 0.4887\n",
            "Epoch [7/10], Average Loss: 0.4803\n",
            "Epoch [8/10], Average Loss: 0.4681\n",
            "Epoch [9/10], Average Loss: 0.4623\n",
            "Epoch [10/10], Average Loss: 0.4563\n",
            "Development Accuracy: 0.8653\n",
            "Development F1 Score: 0.7734\n",
            "Testing parameters: {'dropout_probs': 0.5, 'hidden_sizes': [512, 256, 128, 64], 'learning_rate': 0.01}\n",
            "Epoch [1/10], Average Loss: 2.4765\n",
            "Epoch [2/10], Average Loss: 2.5174\n",
            "Epoch [3/10], Average Loss: 2.5149\n",
            "Epoch [4/10], Average Loss: 2.5147\n",
            "Epoch [5/10], Average Loss: 2.5145\n",
            "Epoch [6/10], Average Loss: 2.5146\n",
            "Epoch [7/10], Average Loss: 2.5148\n",
            "Epoch [8/10], Average Loss: 2.5147\n",
            "Epoch [9/10], Average Loss: 2.5150\n",
            "Epoch [10/10], Average Loss: 2.5147\n",
            "Development Accuracy: 0.1675\n",
            "Development F1 Score: 0.0169\n",
            "Best hyperparameters: {'dropout_probs': 0.2, 'hidden_sizes': [512, 256], 'learning_rate': 0.001}\n",
            "Best development macro f1: 0.7881\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    \"hidden_sizes\": [[256, 128], [512, 256], [512, 256, 128], [512, 256, 128, 64]],\n",
        "    \"dropout_probs\": [0.2, 0.3,0.5],\n",
        "    \"learning_rate\": [0.001, 0.01]\n",
        "}\n",
        "\n",
        "best_score = 0\n",
        "best_params = None\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "    print(f\"Testing parameters: {params}\")\n",
        "\n",
        "    # Unpack parameters\n",
        "    hidden_sizes = params[\"hidden_sizes\"]\n",
        "    dropout_probs = params[\"dropout_probs\"]\n",
        "    learning_rate = params[\"learning_rate\"]\n",
        "    epochs = 10\n",
        "\n",
        "    model = MLPModel(input_size=input_size, hidden_sizes=hidden_sizes, output_size=len(tag_encoder.classes_), dropout_probs=dropout_probs)\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train and evaluate\n",
        "    dev_accuracy, dev_macroF1_score = UtilsMLP.train_and_evaluate(model, optimizer, criterion, device, epochs, train_loader, dev_loader, title='Development')\n",
        "\n",
        "    # Update best parameters if current model is better, according to macro f1 score\n",
        "    if dev_macroF1_score > best_score:\n",
        "        best_score = dev_macroF1_score\n",
        "        best_params = params\n",
        "\n",
        "print(f\"Best hyperparameters: {best_params}\")\n",
        "print(f\"Best development macro f1: {best_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04L_CyyDnc8u",
        "outputId": "9eb9de44-5ccd-430f-af63-eeee10d6b1d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLPModel(\n",
            "  (layers): ModuleList(\n",
            "    (0): Linear(in_features=900, out_features=512, bias=True)\n",
            "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
            "  )\n",
            "  (dropouts): ModuleList(\n",
            "    (0-1): 2 x Dropout(p=0.2, inplace=False)\n",
            "  )\n",
            "  (output_layer): Linear(in_features=256, out_features=17, bias=True)\n",
            ")\n",
            "Epoch 1/100, Training Loss: 0.4933, Accuracy: 0.8425, F1 Score: 0.8388\n",
            "Validation Loss: 0.4280, Accuracy: 0.8613, F1 Score: 0.8581\n",
            "Epoch 2/100, Training Loss: 0.3619, Accuracy: 0.8806, F1 Score: 0.8781\n",
            "Validation Loss: 0.4168, Accuracy: 0.8666, F1 Score: 0.8628\n",
            "Epoch 3/100, Training Loss: 0.3099, Accuracy: 0.8950, F1 Score: 0.8935\n",
            "Validation Loss: 0.4239, Accuracy: 0.8677, F1 Score: 0.8655\n",
            "Epoch 4/100, Training Loss: 0.2742, Accuracy: 0.9059, F1 Score: 0.9049\n",
            "Validation Loss: 0.4371, Accuracy: 0.8688, F1 Score: 0.8652\n",
            "Epoch 5/100, Training Loss: 0.2482, Accuracy: 0.9146, F1 Score: 0.9139\n",
            "Validation Loss: 0.4799, Accuracy: 0.8676, F1 Score: 0.8627\n",
            "Epoch 6/100, Training Loss: 0.2294, Accuracy: 0.9198, F1 Score: 0.9193\n",
            "Validation Loss: 0.4893, Accuracy: 0.8705, F1 Score: 0.8689\n",
            "Epoch 7/100, Training Loss: 0.2150, Accuracy: 0.9238, F1 Score: 0.9234\n",
            "Validation Loss: 0.5079, Accuracy: 0.8703, F1 Score: 0.8667\n",
            "Epoch 8/100, Training Loss: 0.2053, Accuracy: 0.9275, F1 Score: 0.9272\n",
            "Validation Loss: 0.5393, Accuracy: 0.8703, F1 Score: 0.8681\n",
            "Epoch 9/100, Training Loss: 0.1967, Accuracy: 0.9305, F1 Score: 0.9303\n",
            "Validation Loss: 0.5558, Accuracy: 0.8701, F1 Score: 0.8683\n",
            "Epoch 10/100, Training Loss: 0.1901, Accuracy: 0.9327, F1 Score: 0.9325\n",
            "Validation Loss: 0.5777, Accuracy: 0.8668, F1 Score: 0.8652\n",
            "Epoch 11/100, Training Loss: 0.1837, Accuracy: 0.9357, F1 Score: 0.9356\n",
            "Validation Loss: 0.6112, Accuracy: 0.8684, F1 Score: 0.8666\n",
            "Epoch 12/100, Training Loss: 0.1786, Accuracy: 0.9370, F1 Score: 0.9369\n",
            "Validation Loss: 0.6247, Accuracy: 0.8670, F1 Score: 0.8656\n",
            "Epoch 13/100, Training Loss: 0.1747, Accuracy: 0.9385, F1 Score: 0.9385\n",
            "Validation Loss: 0.6768, Accuracy: 0.8682, F1 Score: 0.8664\n",
            "Epoch 14/100, Training Loss: 0.1723, Accuracy: 0.9394, F1 Score: 0.9393\n",
            "Validation Loss: 0.6786, Accuracy: 0.8669, F1 Score: 0.8648\n",
            "Epoch 15/100, Training Loss: 0.1677, Accuracy: 0.9414, F1 Score: 0.9413\n",
            "Validation Loss: 0.6881, Accuracy: 0.8670, F1 Score: 0.8655\n",
            "Epoch 16/100, Training Loss: 0.1670, Accuracy: 0.9416, F1 Score: 0.9416\n",
            "Validation Loss: 0.7104, Accuracy: 0.8687, F1 Score: 0.8662\n",
            "Epoch 17/100, Training Loss: 0.1639, Accuracy: 0.9420, F1 Score: 0.9419\n",
            "Validation Loss: 0.7025, Accuracy: 0.8663, F1 Score: 0.8642\n",
            "Epoch 18/100, Training Loss: 0.1613, Accuracy: 0.9438, F1 Score: 0.9438\n",
            "Validation Loss: 0.7514, Accuracy: 0.8678, F1 Score: 0.8654\n",
            "Epoch 19/100, Training Loss: 0.1580, Accuracy: 0.9448, F1 Score: 0.9448\n",
            "Validation Loss: 0.7488, Accuracy: 0.8665, F1 Score: 0.8654\n",
            "Epoch 20/100, Training Loss: 0.1568, Accuracy: 0.9452, F1 Score: 0.9452\n",
            "Validation Loss: 0.7811, Accuracy: 0.8653, F1 Score: 0.8635\n",
            "Epoch 21/100, Training Loss: 0.1569, Accuracy: 0.9449, F1 Score: 0.9449\n",
            "Validation Loss: 0.7840, Accuracy: 0.8621, F1 Score: 0.8613\n",
            "Epoch 22/100, Training Loss: 0.1539, Accuracy: 0.9463, F1 Score: 0.9463\n",
            "Validation Loss: 0.8187, Accuracy: 0.8668, F1 Score: 0.8647\n",
            "Epoch 23/100, Training Loss: 0.1529, Accuracy: 0.9468, F1 Score: 0.9468\n",
            "Validation Loss: 0.8079, Accuracy: 0.8657, F1 Score: 0.8642\n",
            "Epoch 24/100, Training Loss: 0.1514, Accuracy: 0.9473, F1 Score: 0.9473\n",
            "Validation Loss: 0.8154, Accuracy: 0.8662, F1 Score: 0.8646\n",
            "Epoch 25/100, Training Loss: 0.1505, Accuracy: 0.9477, F1 Score: 0.9477\n",
            "Validation Loss: 0.8405, Accuracy: 0.8666, F1 Score: 0.8652\n",
            "Epoch 26/100, Training Loss: 0.1482, Accuracy: 0.9477, F1 Score: 0.9477\n",
            "Validation Loss: 0.8788, Accuracy: 0.8679, F1 Score: 0.8664\n",
            "Epoch 27/100, Training Loss: 0.1494, Accuracy: 0.9483, F1 Score: 0.9483\n",
            "Validation Loss: 0.8986, Accuracy: 0.8652, F1 Score: 0.8643\n",
            "Epoch 28/100, Training Loss: 0.1456, Accuracy: 0.9490, F1 Score: 0.9491\n",
            "Validation Loss: 0.9312, Accuracy: 0.8672, F1 Score: 0.8657\n",
            "Epoch 29/100, Training Loss: 0.1463, Accuracy: 0.9491, F1 Score: 0.9491\n",
            "Validation Loss: 0.9015, Accuracy: 0.8656, F1 Score: 0.8643\n",
            "Epoch 30/100, Training Loss: 0.1470, Accuracy: 0.9488, F1 Score: 0.9488\n",
            "Validation Loss: 0.9423, Accuracy: 0.8653, F1 Score: 0.8644\n",
            "Epoch 31/100, Training Loss: 0.1446, Accuracy: 0.9497, F1 Score: 0.9497\n",
            "Validation Loss: 0.9812, Accuracy: 0.8643, F1 Score: 0.8631\n",
            "Epoch 32/100, Training Loss: 0.1441, Accuracy: 0.9496, F1 Score: 0.9496\n",
            "Validation Loss: 0.9724, Accuracy: 0.8656, F1 Score: 0.8638\n",
            "Epoch 33/100, Training Loss: 0.1435, Accuracy: 0.9500, F1 Score: 0.9500\n",
            "Validation Loss: 0.9695, Accuracy: 0.8653, F1 Score: 0.8641\n",
            "Epoch 34/100, Training Loss: 0.1432, Accuracy: 0.9506, F1 Score: 0.9507\n",
            "Validation Loss: 0.9964, Accuracy: 0.8665, F1 Score: 0.8653\n",
            "Epoch 35/100, Training Loss: 0.1427, Accuracy: 0.9506, F1 Score: 0.9506\n",
            "Validation Loss: 1.0221, Accuracy: 0.8659, F1 Score: 0.8645\n",
            "Epoch 36/100, Training Loss: 0.1400, Accuracy: 0.9511, F1 Score: 0.9512\n",
            "Validation Loss: 1.0607, Accuracy: 0.8661, F1 Score: 0.8650\n",
            "Epoch 37/100, Training Loss: 0.1405, Accuracy: 0.9509, F1 Score: 0.9510\n",
            "Validation Loss: 1.0554, Accuracy: 0.8676, F1 Score: 0.8667\n",
            "Epoch 38/100, Training Loss: 0.1409, Accuracy: 0.9516, F1 Score: 0.9516\n",
            "Validation Loss: 1.0853, Accuracy: 0.8653, F1 Score: 0.8635\n",
            "Epoch 39/100, Training Loss: 0.1405, Accuracy: 0.9518, F1 Score: 0.9518\n",
            "Validation Loss: 1.0746, Accuracy: 0.8649, F1 Score: 0.8637\n",
            "Epoch 40/100, Training Loss: 0.1387, Accuracy: 0.9517, F1 Score: 0.9517\n",
            "Validation Loss: 1.0374, Accuracy: 0.8681, F1 Score: 0.8664\n",
            "Epoch 41/100, Training Loss: 0.1386, Accuracy: 0.9523, F1 Score: 0.9523\n",
            "Validation Loss: 1.1591, Accuracy: 0.8660, F1 Score: 0.8640\n",
            "Epoch 42/100, Training Loss: 0.1374, Accuracy: 0.9523, F1 Score: 0.9524\n",
            "Validation Loss: 1.1242, Accuracy: 0.8670, F1 Score: 0.8655\n",
            "Epoch 43/100, Training Loss: 0.1380, Accuracy: 0.9526, F1 Score: 0.9527\n",
            "Validation Loss: 1.1828, Accuracy: 0.8675, F1 Score: 0.8647\n",
            "Epoch 44/100, Training Loss: 0.1378, Accuracy: 0.9527, F1 Score: 0.9527\n",
            "Validation Loss: 1.1623, Accuracy: 0.8631, F1 Score: 0.8613\n",
            "Epoch 45/100, Training Loss: 0.1381, Accuracy: 0.9525, F1 Score: 0.9525\n",
            "Validation Loss: 1.1759, Accuracy: 0.8657, F1 Score: 0.8638\n",
            "Epoch 46/100, Training Loss: 0.1383, Accuracy: 0.9526, F1 Score: 0.9527\n",
            "Validation Loss: 1.1529, Accuracy: 0.8638, F1 Score: 0.8620\n",
            "Epoch 47/100, Training Loss: 0.1358, Accuracy: 0.9530, F1 Score: 0.9530\n",
            "Validation Loss: 1.1642, Accuracy: 0.8630, F1 Score: 0.8619\n",
            "Epoch 48/100, Training Loss: 0.1359, Accuracy: 0.9533, F1 Score: 0.9534\n",
            "Validation Loss: 1.2571, Accuracy: 0.8660, F1 Score: 0.8641\n",
            "Epoch 49/100, Training Loss: 0.1359, Accuracy: 0.9532, F1 Score: 0.9533\n",
            "Validation Loss: 1.2588, Accuracy: 0.8655, F1 Score: 0.8640\n",
            "Epoch 50/100, Training Loss: 0.1371, Accuracy: 0.9530, F1 Score: 0.9530\n",
            "Validation Loss: 1.2750, Accuracy: 0.8648, F1 Score: 0.8633\n",
            "Epoch 51/100, Training Loss: 0.1362, Accuracy: 0.9535, F1 Score: 0.9535\n",
            "Validation Loss: 1.2639, Accuracy: 0.8647, F1 Score: 0.8631\n",
            "Epoch 52/100, Training Loss: 0.1345, Accuracy: 0.9543, F1 Score: 0.9543\n",
            "Validation Loss: 1.3206, Accuracy: 0.8668, F1 Score: 0.8650\n",
            "Epoch 53/100, Training Loss: 0.1338, Accuracy: 0.9545, F1 Score: 0.9545\n",
            "Validation Loss: 1.3119, Accuracy: 0.8648, F1 Score: 0.8627\n",
            "Epoch 54/100, Training Loss: 0.1319, Accuracy: 0.9551, F1 Score: 0.9551\n",
            "Validation Loss: 1.3567, Accuracy: 0.8664, F1 Score: 0.8652\n",
            "Epoch 55/100, Training Loss: 0.1361, Accuracy: 0.9542, F1 Score: 0.9542\n",
            "Validation Loss: 1.3306, Accuracy: 0.8654, F1 Score: 0.8643\n",
            "Epoch 56/100, Training Loss: 0.1359, Accuracy: 0.9534, F1 Score: 0.9535\n",
            "Validation Loss: 1.3245, Accuracy: 0.8653, F1 Score: 0.8637\n",
            "Epoch 57/100, Training Loss: 0.1319, Accuracy: 0.9546, F1 Score: 0.9546\n",
            "Validation Loss: 1.4013, Accuracy: 0.8673, F1 Score: 0.8652\n",
            "Epoch 58/100, Training Loss: 0.1331, Accuracy: 0.9542, F1 Score: 0.9542\n",
            "Validation Loss: 1.4388, Accuracy: 0.8655, F1 Score: 0.8636\n",
            "Epoch 59/100, Training Loss: 0.1340, Accuracy: 0.9544, F1 Score: 0.9544\n",
            "Validation Loss: 1.3781, Accuracy: 0.8655, F1 Score: 0.8642\n",
            "Epoch 60/100, Training Loss: 0.1322, Accuracy: 0.9546, F1 Score: 0.9547\n",
            "Validation Loss: 1.4496, Accuracy: 0.8669, F1 Score: 0.8648\n",
            "Epoch 61/100, Training Loss: 0.1330, Accuracy: 0.9547, F1 Score: 0.9547\n",
            "Validation Loss: 1.4487, Accuracy: 0.8673, F1 Score: 0.8645\n",
            "Epoch 62/100, Training Loss: 0.1312, Accuracy: 0.9555, F1 Score: 0.9556\n",
            "Validation Loss: 1.4312, Accuracy: 0.8662, F1 Score: 0.8643\n",
            "Epoch 63/100, Training Loss: 0.1330, Accuracy: 0.9548, F1 Score: 0.9548\n",
            "Validation Loss: 1.4891, Accuracy: 0.8641, F1 Score: 0.8624\n",
            "Epoch 64/100, Training Loss: 0.1315, Accuracy: 0.9552, F1 Score: 0.9553\n",
            "Validation Loss: 1.5390, Accuracy: 0.8659, F1 Score: 0.8645\n",
            "Epoch 65/100, Training Loss: 0.1334, Accuracy: 0.9550, F1 Score: 0.9551\n",
            "Validation Loss: 1.4874, Accuracy: 0.8665, F1 Score: 0.8648\n",
            "Epoch 66/100, Training Loss: 0.1318, Accuracy: 0.9550, F1 Score: 0.9551\n",
            "Validation Loss: 1.5578, Accuracy: 0.8645, F1 Score: 0.8630\n",
            "Epoch 67/100, Training Loss: 0.1319, Accuracy: 0.9556, F1 Score: 0.9556\n",
            "Validation Loss: 1.5703, Accuracy: 0.8644, F1 Score: 0.8634\n",
            "Epoch 68/100, Training Loss: 0.1331, Accuracy: 0.9555, F1 Score: 0.9556\n",
            "Validation Loss: 1.6252, Accuracy: 0.8636, F1 Score: 0.8612\n",
            "Epoch 69/100, Training Loss: 0.1311, Accuracy: 0.9559, F1 Score: 0.9559\n",
            "Validation Loss: 1.5844, Accuracy: 0.8658, F1 Score: 0.8634\n",
            "Epoch 70/100, Training Loss: 0.1327, Accuracy: 0.9553, F1 Score: 0.9554\n",
            "Validation Loss: 1.6639, Accuracy: 0.8649, F1 Score: 0.8632\n",
            "Epoch 71/100, Training Loss: 0.1329, Accuracy: 0.9553, F1 Score: 0.9554\n",
            "Validation Loss: 1.5777, Accuracy: 0.8654, F1 Score: 0.8636\n",
            "Epoch 72/100, Training Loss: 0.1314, Accuracy: 0.9561, F1 Score: 0.9562\n",
            "Validation Loss: 1.5854, Accuracy: 0.8666, F1 Score: 0.8645\n",
            "Epoch 73/100, Training Loss: 0.1305, Accuracy: 0.9559, F1 Score: 0.9560\n",
            "Validation Loss: 1.6431, Accuracy: 0.8674, F1 Score: 0.8653\n",
            "Epoch 74/100, Training Loss: 0.1293, Accuracy: 0.9560, F1 Score: 0.9561\n",
            "Validation Loss: 1.6556, Accuracy: 0.8674, F1 Score: 0.8652\n",
            "Epoch 75/100, Training Loss: 0.1297, Accuracy: 0.9561, F1 Score: 0.9562\n",
            "Validation Loss: 1.6782, Accuracy: 0.8649, F1 Score: 0.8640\n",
            "Epoch 76/100, Training Loss: 0.1325, Accuracy: 0.9559, F1 Score: 0.9560\n",
            "Validation Loss: 1.6975, Accuracy: 0.8638, F1 Score: 0.8624\n",
            "Epoch 77/100, Training Loss: 0.1302, Accuracy: 0.9560, F1 Score: 0.9560\n",
            "Validation Loss: 1.7585, Accuracy: 0.8659, F1 Score: 0.8634\n",
            "Epoch 78/100, Training Loss: 0.1331, Accuracy: 0.9561, F1 Score: 0.9561\n",
            "Validation Loss: 1.6371, Accuracy: 0.8655, F1 Score: 0.8639\n",
            "Epoch 79/100, Training Loss: 0.1308, Accuracy: 0.9561, F1 Score: 0.9562\n",
            "Validation Loss: 1.6808, Accuracy: 0.8644, F1 Score: 0.8619\n",
            "Epoch 80/100, Training Loss: 0.1302, Accuracy: 0.9558, F1 Score: 0.9559\n",
            "Validation Loss: 1.7488, Accuracy: 0.8652, F1 Score: 0.8633\n",
            "Epoch 81/100, Training Loss: 0.1308, Accuracy: 0.9559, F1 Score: 0.9559\n",
            "Validation Loss: 1.7896, Accuracy: 0.8655, F1 Score: 0.8633\n",
            "Epoch 82/100, Training Loss: 0.1317, Accuracy: 0.9555, F1 Score: 0.9555\n",
            "Validation Loss: 1.7530, Accuracy: 0.8653, F1 Score: 0.8630\n",
            "Epoch 83/100, Training Loss: 0.1285, Accuracy: 0.9566, F1 Score: 0.9566\n",
            "Validation Loss: 1.7898, Accuracy: 0.8682, F1 Score: 0.8666\n",
            "Epoch 84/100, Training Loss: 0.1294, Accuracy: 0.9567, F1 Score: 0.9568\n",
            "Validation Loss: 1.7618, Accuracy: 0.8664, F1 Score: 0.8641\n",
            "Epoch 85/100, Training Loss: 0.1305, Accuracy: 0.9565, F1 Score: 0.9566\n",
            "Validation Loss: 1.7473, Accuracy: 0.8649, F1 Score: 0.8631\n",
            "Epoch 86/100, Training Loss: 0.1289, Accuracy: 0.9570, F1 Score: 0.9570\n",
            "Validation Loss: 1.8698, Accuracy: 0.8653, F1 Score: 0.8631\n",
            "Epoch 87/100, Training Loss: 0.1286, Accuracy: 0.9568, F1 Score: 0.9569\n",
            "Validation Loss: 1.8768, Accuracy: 0.8664, F1 Score: 0.8650\n",
            "Epoch 88/100, Training Loss: 0.1306, Accuracy: 0.9563, F1 Score: 0.9564\n",
            "Validation Loss: 1.9221, Accuracy: 0.8679, F1 Score: 0.8656\n",
            "Epoch 89/100, Training Loss: 0.1308, Accuracy: 0.9564, F1 Score: 0.9565\n",
            "Validation Loss: 1.9023, Accuracy: 0.8665, F1 Score: 0.8643\n",
            "Epoch 90/100, Training Loss: 0.1296, Accuracy: 0.9569, F1 Score: 0.9569\n",
            "Validation Loss: 1.8893, Accuracy: 0.8663, F1 Score: 0.8642\n",
            "Epoch 91/100, Training Loss: 0.1283, Accuracy: 0.9572, F1 Score: 0.9573\n",
            "Validation Loss: 1.9536, Accuracy: 0.8685, F1 Score: 0.8657\n",
            "Epoch 92/100, Training Loss: 0.1292, Accuracy: 0.9570, F1 Score: 0.9571\n",
            "Validation Loss: 1.9366, Accuracy: 0.8678, F1 Score: 0.8667\n",
            "Epoch 93/100, Training Loss: 0.1299, Accuracy: 0.9569, F1 Score: 0.9570\n",
            "Validation Loss: 1.9957, Accuracy: 0.8659, F1 Score: 0.8642\n",
            "Epoch 94/100, Training Loss: 0.1302, Accuracy: 0.9569, F1 Score: 0.9570\n",
            "Validation Loss: 2.0063, Accuracy: 0.8669, F1 Score: 0.8650\n",
            "Epoch 95/100, Training Loss: 0.1311, Accuracy: 0.9565, F1 Score: 0.9566\n",
            "Validation Loss: 2.0506, Accuracy: 0.8654, F1 Score: 0.8635\n",
            "Epoch 96/100, Training Loss: 0.1301, Accuracy: 0.9571, F1 Score: 0.9571\n",
            "Validation Loss: 2.0800, Accuracy: 0.8665, F1 Score: 0.8643\n",
            "Epoch 97/100, Training Loss: 0.1280, Accuracy: 0.9574, F1 Score: 0.9575\n",
            "Validation Loss: 1.9715, Accuracy: 0.8676, F1 Score: 0.8661\n",
            "Epoch 98/100, Training Loss: 0.1293, Accuracy: 0.9569, F1 Score: 0.9570\n",
            "Validation Loss: 2.0092, Accuracy: 0.8682, F1 Score: 0.8661\n",
            "Epoch 99/100, Training Loss: 0.1320, Accuracy: 0.9565, F1 Score: 0.9566\n",
            "Validation Loss: 2.0418, Accuracy: 0.8665, F1 Score: 0.8644\n",
            "Epoch 100/100, Training Loss: 0.1293, Accuracy: 0.9570, F1 Score: 0.9571\n",
            "Validation Loss: 1.9468, Accuracy: 0.8669, F1 Score: 0.8654\n",
            "Training complete! Total time: 741.56 seconds\n",
            "Best model saved at: mlp_best_model_epoch_2.pth\n"
          ]
        }
      ],
      "source": [
        "\n",
        "best_hidden_sizes = best_params[\"hidden_sizes\"]\n",
        "best_dropout_probs = best_params[\"dropout_probs\"]\n",
        "best_learning_rate = best_params[\"learning_rate\"]\n",
        "best_epochs = epochs\n",
        "model = MLPModel(input_size, best_hidden_sizes, len(tag_encoder.classes_), best_dropout_probs)\n",
        "print(model)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=best_learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train and validate\n",
        "epochs_for_val = 100\n",
        "history, best_model_path = UtilsMLP.train_and_validate(model, optimizer, criterion, device, epochs_for_val, train_loader, dev_loader)\n",
        "print(f\"Best model saved at: {best_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lLS1n18nc8v",
        "outputId": "f2c818c7-2ac5-46f2-9d47-522d5084097f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "Report.plot_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE_EW_vJnc8v"
      },
      "source": [
        "## Comparisons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH669jWYnc8v"
      },
      "source": [
        "In this section, we evaluate the performance of three models:\n",
        "\n",
        "## 1. **Best BiLSTM Model**\n",
        "   - After performing hyperparameter tuning on the BiLSTM model with attention, we use the configuration that achieved the highest macro F1 score on the validation set.\n",
        "   - This model leverages pre-trained word embeddings (e.g., Word2Vec) and incorporates an attention mechanism to improve context understanding.\n",
        "   - The BiLSTM model captures long-term dependencies and sequential patterns, making it well-suited for POS tagging tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. **Best MLP Model**:\n",
        "   - After conducting hyperparameter tuning on the MLP model, we use the configuration that achieved the best performance on the validation set. This model utilizes sliding window word embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. **Baseline Classifier**:\n",
        "   - As a reference point, we implement a baseline classifier that operates as follows:\n",
        "     - For each word in the test set, it assigns the **most frequent tag** that the word had in the training data.\n",
        "     - For words not encountered during training (unseen words), the classifier assigns the **most frequent tag overall** (across all words) from the training data.\n",
        "\n",
        "This baseline provides a simple yet effective method for POS tagging and serves as a comparison to demonstrate the improvements achieved by the MLP model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieHOlijhnc8v"
      },
      "source": [
        "### **BILSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Np4ZGYrmnc8v"
      },
      "outputs": [],
      "source": [
        "model2 = BiLSTM_Attention(\n",
        "        input_dim=embedding_dim,\n",
        "        n_classes=len(tag2idx),\n",
        "        dropout_prob_emb=best_dropout_prob_emb,\n",
        "        dropout_prob_att=best_dropout_prob_att,\n",
        "        dropout_prob_out=best_dropout_prob_out,\n",
        "        hidden_dim=128,\n",
        "        lstm_hidden_dim=best_lstm_hidden_dim,\n",
        "        lstm_stacks=best_lstm_stacks,\n",
        "        attention_hidden_sizes=best_attention_hidden_sizes,\n",
        "        max_words=len(word2idx),\n",
        "        matrix_embeddings=embedding_matrix,\n",
        "    )\n",
        "model2.to(device)\n",
        "\n",
        "# Define optimizer and criterion\n",
        "optimizer = Adam(model2.parameters(), lr=best_learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4T6uOTHnc8v",
        "outputId": "34e7e5b9-efde-4299-87d2-363901ed06b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2, Loss: 0.0696\n",
            "Epoch 2/2, Loss: 0.0598\n"
          ]
        }
      ],
      "source": [
        "trained_model = TrainEvaluate.train_model(model2, optimizer, criterion, device, 2, train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V--0twpJnc8v",
        "outputId": "b9623fce-94f0-437a-f5af-3c52652700ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9890420332355816\n",
            "F1 Score: 0.9889772449536427\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      0.99      0.99     34755\n",
            "           2       0.99      0.99      0.99     17738\n",
            "           3       1.00      1.00      1.00     12817\n",
            "           4       0.99      0.94      0.97       695\n",
            "           5       0.99      1.00      1.00     23596\n",
            "           6       0.95      0.97      0.96      3822\n",
            "           7       0.93      0.69      0.79       399\n",
            "           8       0.99      0.99      0.99     22602\n",
            "           9       1.00      1.00      1.00     18677\n",
            "          10       0.97      0.96      0.97      4127\n",
            "          11       0.99      0.97      0.98     10117\n",
            "          12       1.00      0.99      1.00      5747\n",
            "          13       1.00      1.00      1.00      6687\n",
            "          14       1.00      1.00      1.00     16299\n",
            "          15       0.99      0.99      0.99     13187\n",
            "          16       0.96      0.97      0.97     12613\n",
            "          17       0.95      0.86      0.90       722\n",
            "\n",
            "    accuracy                           0.99    204600\n",
            "   macro avg       0.98      0.96      0.97    204600\n",
            "weighted avg       0.99      0.99      0.99    204600\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions = TrainEvaluate.predict(trained_model, X_train_padded, device)\n",
        "print(\"Predicted Labels:\")\n",
        "print(predictions)\n",
        "true_labels = y_train_padded.argmax(axis=-1).flatten()  # Convert from one-hot\n",
        "\n",
        "# Mask out padding tokens\n",
        "valid_true_labels = true_labels[true_labels != PAD_INDEX]\n",
        "valid_predictions = predictions[true_labels != PAD_INDEX]\n",
        "\n",
        "# Evaluate\n",
        "print(\"Accuracy:\", accuracy_score(valid_true_labels, valid_predictions))\n",
        "print(\"F1 Score:\", f1_score(valid_true_labels, valid_predictions, average='weighted'))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(valid_true_labels, valid_predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rS6JMr2gnc8w",
        "outputId": "e9a5dabf-9f99-48e6-db3b-32f04d1517c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2, Loss: 0.0754\n",
            "Epoch 2/2, Loss: 0.0648\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.93      0.97      0.95      2030\n",
            "           2       0.99      0.99      0.99       736\n",
            "           3       0.99      0.97      0.98      3096\n",
            "           4       0.96      0.96      0.96       649\n",
            "           5       0.92      0.87      0.90      1794\n",
            "           6       0.96      0.91      0.93      2606\n",
            "           7       0.88      0.81      0.84       121\n",
            "           8       0.92      0.90      0.91      1183\n",
            "           9       0.87      0.88      0.88      4123\n",
            "          10       0.00      0.00      0.00        42\n",
            "          11       0.99      0.98      0.99      1543\n",
            "          12       0.93      0.84      0.88       384\n",
            "          13       0.89      0.68      0.77       109\n",
            "          14       0.66      0.83      0.74      2076\n",
            "          15       0.87      0.49      0.63       542\n",
            "          16       0.98      0.99      0.99      1896\n",
            "          17       0.99      0.98      0.99      2166\n",
            "\n",
            "    accuracy                           0.92     25096\n",
            "   macro avg       0.87      0.83      0.84     25096\n",
            "weighted avg       0.92      0.92      0.92     25096\n",
            "\n",
            "Accuracy: 0.9152\n",
            "F1 Score: 0.9151\n"
          ]
        }
      ],
      "source": [
        "trained_model = TrainEvaluate.train_model(model2, optimizer, criterion, device, 2, train_loader)\n",
        "eval_report = TrainEvaluate.evaluate_model(trained_model, criterion, device, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42BjLGSPnc8w"
      },
      "source": [
        "### **MLP Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o61yMKw_nc8w",
        "outputId": "cc8bf58f-3be0-4ece-b3d5-cf7c751f35de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/2] - Training Loss: 0.4933\n",
            "Epoch [2/2] - Training Loss: 0.3608\n"
          ]
        }
      ],
      "source": [
        "model = MLPModel(input_size=input_size, hidden_sizes=best_hidden_sizes, output_size=len(tag_encoder.classes_), dropout_probs=best_dropout_probs)\n",
        "model.to(device)\n",
        "optimizer = Adam(model.parameters(), lr=best_learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "mlp = MLPClassifier(model, optimizer, criterion, device, epochs=2)\n",
        "_ = mlp.fit(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-daNzcgnc8w",
        "outputId": "202eee6f-c8b3-4cba-b353-fca37c8b3521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.91      0.92      1794\n",
            "           1       0.83      0.89      0.86      2030\n",
            "           2       0.94      0.90      0.92      1183\n",
            "           3       0.97      0.98      0.97      1543\n",
            "           4       0.62      0.47      0.53       736\n",
            "           5       0.91      0.93      0.92      1896\n",
            "           6       0.88      0.82      0.85       121\n",
            "           7       0.88      0.93      0.90      4123\n",
            "           8       0.78      0.49      0.60       542\n",
            "           9       0.84      0.86      0.85       649\n",
            "          10       0.98      0.99      0.98      2166\n",
            "          11       0.87      0.59      0.70      2076\n",
            "          12       0.68      0.87      0.76      3096\n",
            "          13       0.89      0.80      0.84       384\n",
            "          14       0.97      0.64      0.77       109\n",
            "          15       0.95      0.93      0.94      2606\n",
            "          16       0.50      0.02      0.05        42\n",
            "\n",
            "    accuracy                           0.87     25096\n",
            "   macro avg       0.85      0.77      0.79     25096\n",
            "weighted avg       0.87      0.87      0.86     25096\n",
            "\n"
          ]
        }
      ],
      "source": [
        "y_test_mlp = mlp.predict(test_features)\n",
        "print(classification_report(test_labels_flattened, y_test_mlp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT0BoMsDnc8w"
      },
      "source": [
        "### **Most Frequent Tag Classifier (Baseline)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbTWpK3mnc8w",
        "outputId": "5dd53415-9275-4360-859c-f138bd3fcc99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.21      0.35      1794\n",
            "           1       0.91      0.23      0.37      2030\n",
            "           2       0.96      0.27      0.42      1183\n",
            "           3       0.99      0.40      0.57      1543\n",
            "           4       0.59      0.17      0.27       736\n",
            "           5       0.97      0.30      0.46      1896\n",
            "           6       0.97      0.53      0.68       121\n",
            "           7       0.24      0.98      0.38      4123\n",
            "           8       0.60      0.21      0.31       542\n",
            "           9       0.95      0.33      0.49       649\n",
            "          10       0.99      0.51      0.67      2166\n",
            "          11       0.80      0.21      0.33      2076\n",
            "          12       0.71      0.54      0.61      3096\n",
            "          13       0.96      0.28      0.43       384\n",
            "          14       0.88      0.33      0.48       109\n",
            "          15       0.94      0.30      0.45      2606\n",
            "          16       0.25      0.02      0.04        42\n",
            "\n",
            "    accuracy                           0.44     25096\n",
            "   macro avg       0.80      0.34      0.43     25096\n",
            "weighted avg       0.78      0.44      0.45     25096\n",
            "\n"
          ]
        }
      ],
      "source": [
        "baseline_model = MostFrequentTagBaseline()\n",
        "baseline_model.fit(train_features, train_labels_flattened)\n",
        "\n",
        "# Predict for the dev set\n",
        "test_predicted_tags = baseline_model.predict(test_features)\n",
        "print(classification_report(test_labels_flattened, test_predicted_tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN04Mzctnc8w"
      },
      "outputs": [],
      "source": [
        "train_labels = y_train_padded.argmax(axis=-1).flatten()  # Convert from one-hot\n",
        "dev_labels = y_dev_padded.argmax(axis=-1).flatten()\n",
        "test_labels = y_test_padded.argmax(axis=-1).flatten()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPfJPBLZnc8w",
        "outputId": "e2ae5348-5f38-431b-b2d5-1373709482e4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_2e877 th {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_2e877 td {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_2e877 index {\n",
              "  display: none;\n",
              "}\n",
              "#T_2e877_row0_col0, #T_2e877_row0_col1, #T_2e877_row0_col2, #T_2e877_row0_col3, #T_2e877_row0_col4, #T_2e877_row0_col5, #T_2e877_row0_col6, #T_2e877_row0_col7, #T_2e877_row0_col8, #T_2e877_row0_col9, #T_2e877_row0_col10, #T_2e877_row18_col0, #T_2e877_row18_col1, #T_2e877_row18_col2, #T_2e877_row18_col3, #T_2e877_row18_col4, #T_2e877_row18_col5, #T_2e877_row18_col6, #T_2e877_row18_col7, #T_2e877_row18_col8, #T_2e877_row18_col9, #T_2e877_row18_col10, #T_2e877_row36_col0, #T_2e877_row36_col1, #T_2e877_row36_col2, #T_2e877_row36_col3, #T_2e877_row36_col4, #T_2e877_row36_col5, #T_2e877_row36_col6, #T_2e877_row36_col7, #T_2e877_row36_col8, #T_2e877_row36_col9, #T_2e877_row36_col10 {\n",
              "  border-top: 3px solid black;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_2e877\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_2e877_level0_col0\" class=\"col_heading level0 col0\" >Classifier</th>\n",
              "      <th id=\"T_2e877_level0_col1\" class=\"col_heading level0 col1\" >Subset</th>\n",
              "      <th id=\"T_2e877_level0_col2\" class=\"col_heading level0 col2\" >Class</th>\n",
              "      <th id=\"T_2e877_level0_col3\" class=\"col_heading level0 col3\" >Macro Precision</th>\n",
              "      <th id=\"T_2e877_level0_col4\" class=\"col_heading level0 col4\" >Macro Recall</th>\n",
              "      <th id=\"T_2e877_level0_col5\" class=\"col_heading level0 col5\" >Macro F1</th>\n",
              "      <th id=\"T_2e877_level0_col6\" class=\"col_heading level0 col6\" >Macro PR-AUC</th>\n",
              "      <th id=\"T_2e877_level0_col7\" class=\"col_heading level0 col7\" >Precision</th>\n",
              "      <th id=\"T_2e877_level0_col8\" class=\"col_heading level0 col8\" >Recall</th>\n",
              "      <th id=\"T_2e877_level0_col9\" class=\"col_heading level0 col9\" >F1</th>\n",
              "      <th id=\"T_2e877_level0_col10\" class=\"col_heading level0 col10\" >PR-AUC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_2e877_row0_col0\" class=\"data row0 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row0_col1\" class=\"data row0 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row0_col2\" class=\"data row0 col2\" >Macro Average</td>\n",
              "      <td id=\"T_2e877_row0_col3\" class=\"data row0 col3\" >0.892024</td>\n",
              "      <td id=\"T_2e877_row0_col4\" class=\"data row0 col4\" >0.819782</td>\n",
              "      <td id=\"T_2e877_row0_col5\" class=\"data row0 col5\" >0.842762</td>\n",
              "      <td id=\"T_2e877_row0_col6\" class=\"data row0 col6\" >0.900854</td>\n",
              "      <td id=\"T_2e877_row0_col7\" class=\"data row0 col7\" >nan</td>\n",
              "      <td id=\"T_2e877_row0_col8\" class=\"data row0 col8\" >nan</td>\n",
              "      <td id=\"T_2e877_row0_col9\" class=\"data row0 col9\" >nan</td>\n",
              "      <td id=\"T_2e877_row0_col10\" class=\"data row0 col10\" >nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_2e877_row1_col0\" class=\"data row1 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row1_col1\" class=\"data row1 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row1_col2\" class=\"data row1 col2\" >0</td>\n",
              "      <td id=\"T_2e877_row1_col3\" class=\"data row1 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row1_col4\" class=\"data row1 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row1_col5\" class=\"data row1 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row1_col6\" class=\"data row1 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row1_col7\" class=\"data row1 col7\" >0.960338</td>\n",
              "      <td id=\"T_2e877_row1_col8\" class=\"data row1 col8\" >0.938273</td>\n",
              "      <td id=\"T_2e877_row1_col9\" class=\"data row1 col9\" >0.949177</td>\n",
              "      <td id=\"T_2e877_row1_col10\" class=\"data row1 col10\" >0.985659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_2e877_row2_col0\" class=\"data row2 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row2_col1\" class=\"data row2 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row2_col2\" class=\"data row2 col2\" >1</td>\n",
              "      <td id=\"T_2e877_row2_col3\" class=\"data row2 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row2_col4\" class=\"data row2 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row2_col5\" class=\"data row2 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row2_col6\" class=\"data row2 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row2_col7\" class=\"data row2 col7\" >0.861011</td>\n",
              "      <td id=\"T_2e877_row2_col8\" class=\"data row2 col8\" >0.925080</td>\n",
              "      <td id=\"T_2e877_row2_col9\" class=\"data row2 col9\" >0.891896</td>\n",
              "      <td id=\"T_2e877_row2_col10\" class=\"data row2 col10\" >0.961534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_2e877_row3_col0\" class=\"data row3 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row3_col1\" class=\"data row3 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row3_col2\" class=\"data row3 col2\" >2</td>\n",
              "      <td id=\"T_2e877_row3_col3\" class=\"data row3 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row3_col4\" class=\"data row3 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row3_col5\" class=\"data row3 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row3_col6\" class=\"data row3 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row3_col7\" class=\"data row3 col7\" >0.955663</td>\n",
              "      <td id=\"T_2e877_row3_col8\" class=\"data row3 col8\" >0.918256</td>\n",
              "      <td id=\"T_2e877_row3_col9\" class=\"data row3 col9\" >0.936586</td>\n",
              "      <td id=\"T_2e877_row3_col10\" class=\"data row3 col10\" >0.981456</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_2e877_row4_col0\" class=\"data row4 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row4_col1\" class=\"data row4 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row4_col2\" class=\"data row4 col2\" >3</td>\n",
              "      <td id=\"T_2e877_row4_col3\" class=\"data row4 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row4_col4\" class=\"data row4 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row4_col5\" class=\"data row4 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row4_col6\" class=\"data row4 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row4_col7\" class=\"data row4 col7\" >0.983609</td>\n",
              "      <td id=\"T_2e877_row4_col8\" class=\"data row4 col8\" >0.983225</td>\n",
              "      <td id=\"T_2e877_row4_col9\" class=\"data row4 col9\" >0.983417</td>\n",
              "      <td id=\"T_2e877_row4_col10\" class=\"data row4 col10\" >0.997745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_2e877_row5_col0\" class=\"data row5 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row5_col1\" class=\"data row5 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row5_col2\" class=\"data row5 col2\" >4</td>\n",
              "      <td id=\"T_2e877_row5_col3\" class=\"data row5 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row5_col4\" class=\"data row5 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row5_col5\" class=\"data row5 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row5_col6\" class=\"data row5 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row5_col7\" class=\"data row5 col7\" >0.698323</td>\n",
              "      <td id=\"T_2e877_row5_col8\" class=\"data row5 col8\" >0.516824</td>\n",
              "      <td id=\"T_2e877_row5_col9\" class=\"data row5 col9\" >0.594019</td>\n",
              "      <td id=\"T_2e877_row5_col10\" class=\"data row5 col10\" >0.720782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_2e877_row6_col0\" class=\"data row6 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row6_col1\" class=\"data row6 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row6_col2\" class=\"data row6 col2\" >5</td>\n",
              "      <td id=\"T_2e877_row6_col3\" class=\"data row6 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row6_col4\" class=\"data row6 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row6_col5\" class=\"data row6 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row6_col6\" class=\"data row6 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row6_col7\" class=\"data row6 col7\" >0.947076</td>\n",
              "      <td id=\"T_2e877_row6_col8\" class=\"data row6 col8\" >0.962881</td>\n",
              "      <td id=\"T_2e877_row6_col9\" class=\"data row6 col9\" >0.954913</td>\n",
              "      <td id=\"T_2e877_row6_col10\" class=\"data row6 col10\" >0.991467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_2e877_row7_col0\" class=\"data row7 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row7_col1\" class=\"data row7 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row7_col2\" class=\"data row7 col2\" >6</td>\n",
              "      <td id=\"T_2e877_row7_col3\" class=\"data row7 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row7_col4\" class=\"data row7 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row7_col5\" class=\"data row7 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row7_col6\" class=\"data row7 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row7_col7\" class=\"data row7 col7\" >0.901989</td>\n",
              "      <td id=\"T_2e877_row7_col8\" class=\"data row7 col8\" >0.913669</td>\n",
              "      <td id=\"T_2e877_row7_col9\" class=\"data row7 col9\" >0.907791</td>\n",
              "      <td id=\"T_2e877_row7_col10\" class=\"data row7 col10\" >0.952935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_2e877_row8_col0\" class=\"data row8 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row8_col1\" class=\"data row8 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row8_col2\" class=\"data row8 col2\" >7</td>\n",
              "      <td id=\"T_2e877_row8_col3\" class=\"data row8 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row8_col4\" class=\"data row8 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row8_col5\" class=\"data row8 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row8_col6\" class=\"data row8 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row8_col7\" class=\"data row8 col7\" >0.934762</td>\n",
              "      <td id=\"T_2e877_row8_col8\" class=\"data row8 col8\" >0.957301</td>\n",
              "      <td id=\"T_2e877_row8_col9\" class=\"data row8 col9\" >0.945898</td>\n",
              "      <td id=\"T_2e877_row8_col10\" class=\"data row8 col10\" >0.985170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_2e877_row9_col0\" class=\"data row9 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row9_col1\" class=\"data row9 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row9_col2\" class=\"data row9 col2\" >8</td>\n",
              "      <td id=\"T_2e877_row9_col3\" class=\"data row9 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row9_col4\" class=\"data row9 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row9_col5\" class=\"data row9 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row9_col6\" class=\"data row9 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row9_col7\" class=\"data row9 col7\" >0.883619</td>\n",
              "      <td id=\"T_2e877_row9_col8\" class=\"data row9 col8\" >0.686213</td>\n",
              "      <td id=\"T_2e877_row9_col9\" class=\"data row9 col9\" >0.772504</td>\n",
              "      <td id=\"T_2e877_row9_col10\" class=\"data row9 col10\" >0.857080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "      <td id=\"T_2e877_row10_col0\" class=\"data row10 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row10_col1\" class=\"data row10 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row10_col2\" class=\"data row10 col2\" >9</td>\n",
              "      <td id=\"T_2e877_row10_col3\" class=\"data row10 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row10_col4\" class=\"data row10 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row10_col5\" class=\"data row10 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row10_col6\" class=\"data row10 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row10_col7\" class=\"data row10 col7\" >0.884978</td>\n",
              "      <td id=\"T_2e877_row10_col8\" class=\"data row10 col8\" >0.914231</td>\n",
              "      <td id=\"T_2e877_row10_col9\" class=\"data row10 col9\" >0.899367</td>\n",
              "      <td id=\"T_2e877_row10_col10\" class=\"data row10 col10\" >0.962290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "      <td id=\"T_2e877_row11_col0\" class=\"data row11 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row11_col1\" class=\"data row11 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row11_col2\" class=\"data row11 col2\" >10</td>\n",
              "      <td id=\"T_2e877_row11_col3\" class=\"data row11 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row11_col4\" class=\"data row11 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row11_col5\" class=\"data row11 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row11_col6\" class=\"data row11 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row11_col7\" class=\"data row11 col7\" >0.990277</td>\n",
              "      <td id=\"T_2e877_row11_col8\" class=\"data row11 col8\" >0.992451</td>\n",
              "      <td id=\"T_2e877_row11_col9\" class=\"data row11 col9\" >0.991362</td>\n",
              "      <td id=\"T_2e877_row11_col10\" class=\"data row11 col10\" >0.998825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "      <td id=\"T_2e877_row12_col0\" class=\"data row12 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row12_col1\" class=\"data row12 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row12_col2\" class=\"data row12 col2\" >11</td>\n",
              "      <td id=\"T_2e877_row12_col3\" class=\"data row12 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row12_col4\" class=\"data row12 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row12_col5\" class=\"data row12 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row12_col6\" class=\"data row12 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row12_col7\" class=\"data row12 col7\" >0.895987</td>\n",
              "      <td id=\"T_2e877_row12_col8\" class=\"data row12 col8\" >0.711365</td>\n",
              "      <td id=\"T_2e877_row12_col9\" class=\"data row12 col9\" >0.793073</td>\n",
              "      <td id=\"T_2e877_row12_col10\" class=\"data row12 col10\" >0.899595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "      <td id=\"T_2e877_row13_col0\" class=\"data row13 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row13_col1\" class=\"data row13 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row13_col2\" class=\"data row13 col2\" >12</td>\n",
              "      <td id=\"T_2e877_row13_col3\" class=\"data row13 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row13_col4\" class=\"data row13 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row13_col5\" class=\"data row13 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row13_col6\" class=\"data row13 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row13_col7\" class=\"data row13 col7\" >0.744455</td>\n",
              "      <td id=\"T_2e877_row13_col8\" class=\"data row13 col8\" >0.880531</td>\n",
              "      <td id=\"T_2e877_row13_col9\" class=\"data row13 col9\" >0.806795</td>\n",
              "      <td id=\"T_2e877_row13_col10\" class=\"data row13 col10\" >0.895897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "      <td id=\"T_2e877_row14_col0\" class=\"data row14 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row14_col1\" class=\"data row14 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row14_col2\" class=\"data row14 col2\" >13</td>\n",
              "      <td id=\"T_2e877_row14_col3\" class=\"data row14 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row14_col4\" class=\"data row14 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row14_col5\" class=\"data row14 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row14_col6\" class=\"data row14 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row14_col7\" class=\"data row14 col7\" >0.917690</td>\n",
              "      <td id=\"T_2e877_row14_col8\" class=\"data row14 col8\" >0.860544</td>\n",
              "      <td id=\"T_2e877_row14_col9\" class=\"data row14 col9\" >0.888199</td>\n",
              "      <td id=\"T_2e877_row14_col10\" class=\"data row14 col10\" >0.949038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
              "      <td id=\"T_2e877_row15_col0\" class=\"data row15 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row15_col1\" class=\"data row15 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row15_col2\" class=\"data row15 col2\" >14</td>\n",
              "      <td id=\"T_2e877_row15_col3\" class=\"data row15 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row15_col4\" class=\"data row15 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row15_col5\" class=\"data row15 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row15_col6\" class=\"data row15 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row15_col7\" class=\"data row15 col7\" >0.920082</td>\n",
              "      <td id=\"T_2e877_row15_col8\" class=\"data row15 col8\" >0.621884</td>\n",
              "      <td id=\"T_2e877_row15_col9\" class=\"data row15 col9\" >0.742149</td>\n",
              "      <td id=\"T_2e877_row15_col10\" class=\"data row15 col10\" >0.770628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
              "      <td id=\"T_2e877_row16_col0\" class=\"data row16 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row16_col1\" class=\"data row16 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row16_col2\" class=\"data row16 col2\" >15</td>\n",
              "      <td id=\"T_2e877_row16_col3\" class=\"data row16 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row16_col4\" class=\"data row16 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row16_col5\" class=\"data row16 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row16_col6\" class=\"data row16 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row16_col7\" class=\"data row16 col7\" >0.971586</td>\n",
              "      <td id=\"T_2e877_row16_col8\" class=\"data row16 col8\" >0.960582</td>\n",
              "      <td id=\"T_2e877_row16_col9\" class=\"data row16 col9\" >0.966053</td>\n",
              "      <td id=\"T_2e877_row16_col10\" class=\"data row16 col10\" >0.993173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
              "      <td id=\"T_2e877_row17_col0\" class=\"data row17 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_2e877_row17_col1\" class=\"data row17 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row17_col2\" class=\"data row17 col2\" >16</td>\n",
              "      <td id=\"T_2e877_row17_col3\" class=\"data row17 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row17_col4\" class=\"data row17 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row17_col5\" class=\"data row17 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row17_col6\" class=\"data row17 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row17_col7\" class=\"data row17 col7\" >0.712963</td>\n",
              "      <td id=\"T_2e877_row17_col8\" class=\"data row17 col8\" >0.192982</td>\n",
              "      <td id=\"T_2e877_row17_col9\" class=\"data row17 col9\" >0.303748</td>\n",
              "      <td id=\"T_2e877_row17_col10\" class=\"data row17 col10\" >0.411251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
              "      <td id=\"T_2e877_row18_col0\" class=\"data row18 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row18_col1\" class=\"data row18 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row18_col2\" class=\"data row18 col2\" >Macro Average</td>\n",
              "      <td id=\"T_2e877_row18_col3\" class=\"data row18 col3\" >0.964640</td>\n",
              "      <td id=\"T_2e877_row18_col4\" class=\"data row18 col4\" >0.938694</td>\n",
              "      <td id=\"T_2e877_row18_col5\" class=\"data row18 col5\" >0.950630</td>\n",
              "      <td id=\"T_2e877_row18_col6\" class=\"data row18 col6\" >0.981475</td>\n",
              "      <td id=\"T_2e877_row18_col7\" class=\"data row18 col7\" >nan</td>\n",
              "      <td id=\"T_2e877_row18_col8\" class=\"data row18 col8\" >nan</td>\n",
              "      <td id=\"T_2e877_row18_col9\" class=\"data row18 col9\" >nan</td>\n",
              "      <td id=\"T_2e877_row18_col10\" class=\"data row18 col10\" >nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
              "      <td id=\"T_2e877_row19_col0\" class=\"data row19 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row19_col1\" class=\"data row19 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row19_col2\" class=\"data row19 col2\" >0</td>\n",
              "      <td id=\"T_2e877_row19_col3\" class=\"data row19 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row19_col4\" class=\"data row19 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row19_col5\" class=\"data row19 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row19_col6\" class=\"data row19 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row19_col7\" class=\"data row19 col7\" >0.994069</td>\n",
              "      <td id=\"T_2e877_row19_col8\" class=\"data row19 col8\" >0.991431</td>\n",
              "      <td id=\"T_2e877_row19_col9\" class=\"data row19 col9\" >0.992748</td>\n",
              "      <td id=\"T_2e877_row19_col10\" class=\"data row19 col10\" >0.999617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
              "      <td id=\"T_2e877_row20_col0\" class=\"data row20 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row20_col1\" class=\"data row20 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row20_col2\" class=\"data row20 col2\" >1</td>\n",
              "      <td id=\"T_2e877_row20_col3\" class=\"data row20 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row20_col4\" class=\"data row20 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row20_col5\" class=\"data row20 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row20_col6\" class=\"data row20 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row20_col7\" class=\"data row20 col7\" >0.982353</td>\n",
              "      <td id=\"T_2e877_row20_col8\" class=\"data row20 col8\" >0.972828</td>\n",
              "      <td id=\"T_2e877_row20_col9\" class=\"data row20 col9\" >0.977568</td>\n",
              "      <td id=\"T_2e877_row20_col10\" class=\"data row20 col10\" >0.998242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
              "      <td id=\"T_2e877_row21_col0\" class=\"data row21 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row21_col1\" class=\"data row21 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row21_col2\" class=\"data row21 col2\" >2</td>\n",
              "      <td id=\"T_2e877_row21_col3\" class=\"data row21 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row21_col4\" class=\"data row21 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row21_col5\" class=\"data row21 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row21_col6\" class=\"data row21 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row21_col7\" class=\"data row21 col7\" >0.992362</td>\n",
              "      <td id=\"T_2e877_row21_col8\" class=\"data row21 col8\" >0.988831</td>\n",
              "      <td id=\"T_2e877_row21_col9\" class=\"data row21 col9\" >0.990593</td>\n",
              "      <td id=\"T_2e877_row21_col10\" class=\"data row21 col10\" >0.999578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
              "      <td id=\"T_2e877_row22_col0\" class=\"data row22 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row22_col1\" class=\"data row22 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row22_col2\" class=\"data row22 col2\" >3</td>\n",
              "      <td id=\"T_2e877_row22_col3\" class=\"data row22 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row22_col4\" class=\"data row22 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row22_col5\" class=\"data row22 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row22_col6\" class=\"data row22 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row22_col7\" class=\"data row22 col7\" >0.997335</td>\n",
              "      <td id=\"T_2e877_row22_col8\" class=\"data row22 col8\" >0.992744</td>\n",
              "      <td id=\"T_2e877_row22_col9\" class=\"data row22 col9\" >0.995034</td>\n",
              "      <td id=\"T_2e877_row22_col10\" class=\"data row22 col10\" >0.999873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
              "      <td id=\"T_2e877_row23_col0\" class=\"data row23 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row23_col1\" class=\"data row23 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row23_col2\" class=\"data row23 col2\" >4</td>\n",
              "      <td id=\"T_2e877_row23_col3\" class=\"data row23 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row23_col4\" class=\"data row23 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row23_col5\" class=\"data row23 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row23_col6\" class=\"data row23 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row23_col7\" class=\"data row23 col7\" >0.929619</td>\n",
              "      <td id=\"T_2e877_row23_col8\" class=\"data row23 col8\" >0.890833</td>\n",
              "      <td id=\"T_2e877_row23_col9\" class=\"data row23 col9\" >0.909813</td>\n",
              "      <td id=\"T_2e877_row23_col10\" class=\"data row23 col10\" >0.977373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
              "      <td id=\"T_2e877_row24_col0\" class=\"data row24 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row24_col1\" class=\"data row24 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row24_col2\" class=\"data row24 col2\" >5</td>\n",
              "      <td id=\"T_2e877_row24_col3\" class=\"data row24 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row24_col4\" class=\"data row24 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row24_col5\" class=\"data row24 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row24_col6\" class=\"data row24 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row24_col7\" class=\"data row24 col7\" >0.993767</td>\n",
              "      <td id=\"T_2e877_row24_col8\" class=\"data row24 col8\" >0.987975</td>\n",
              "      <td id=\"T_2e877_row24_col9\" class=\"data row24 col9\" >0.990862</td>\n",
              "      <td id=\"T_2e877_row24_col10\" class=\"data row24 col10\" >0.999434</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
              "      <td id=\"T_2e877_row25_col0\" class=\"data row25 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row25_col1\" class=\"data row25 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row25_col2\" class=\"data row25 col2\" >6</td>\n",
              "      <td id=\"T_2e877_row25_col3\" class=\"data row25 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row25_col4\" class=\"data row25 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row25_col5\" class=\"data row25 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row25_col6\" class=\"data row25 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row25_col7\" class=\"data row25 col7\" >0.976676</td>\n",
              "      <td id=\"T_2e877_row25_col8\" class=\"data row25 col8\" >0.964029</td>\n",
              "      <td id=\"T_2e877_row25_col9\" class=\"data row25 col9\" >0.970311</td>\n",
              "      <td id=\"T_2e877_row25_col10\" class=\"data row25 col10\" >0.990681</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
              "      <td id=\"T_2e877_row26_col0\" class=\"data row26 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row26_col1\" class=\"data row26 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row26_col2\" class=\"data row26 col2\" >7</td>\n",
              "      <td id=\"T_2e877_row26_col3\" class=\"data row26 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row26_col4\" class=\"data row26 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row26_col5\" class=\"data row26 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row26_col6\" class=\"data row26 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row26_col7\" class=\"data row26 col7\" >0.990079</td>\n",
              "      <td id=\"T_2e877_row26_col8\" class=\"data row26 col8\" >0.984865</td>\n",
              "      <td id=\"T_2e877_row26_col9\" class=\"data row26 col9\" >0.987465</td>\n",
              "      <td id=\"T_2e877_row26_col10\" class=\"data row26 col10\" >0.999235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
              "      <td id=\"T_2e877_row27_col0\" class=\"data row27 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row27_col1\" class=\"data row27 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row27_col2\" class=\"data row27 col2\" >8</td>\n",
              "      <td id=\"T_2e877_row27_col3\" class=\"data row27 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row27_col4\" class=\"data row27 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row27_col5\" class=\"data row27 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row27_col6\" class=\"data row27 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row27_col7\" class=\"data row27 col7\" >0.908052</td>\n",
              "      <td id=\"T_2e877_row27_col8\" class=\"data row27 col8\" >0.789678</td>\n",
              "      <td id=\"T_2e877_row27_col9\" class=\"data row27 col9\" >0.844738</td>\n",
              "      <td id=\"T_2e877_row27_col10\" class=\"data row27 col10\" >0.930614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
              "      <td id=\"T_2e877_row28_col0\" class=\"data row28 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row28_col1\" class=\"data row28 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row28_col2\" class=\"data row28 col2\" >9</td>\n",
              "      <td id=\"T_2e877_row28_col3\" class=\"data row28 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row28_col4\" class=\"data row28 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row28_col5\" class=\"data row28 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row28_col6\" class=\"data row28 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row28_col7\" class=\"data row28 col7\" >0.989317</td>\n",
              "      <td id=\"T_2e877_row28_col8\" class=\"data row28 col8\" >0.982777</td>\n",
              "      <td id=\"T_2e877_row28_col9\" class=\"data row28 col9\" >0.986036</td>\n",
              "      <td id=\"T_2e877_row28_col10\" class=\"data row28 col10\" >0.999047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
              "      <td id=\"T_2e877_row29_col0\" class=\"data row29 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row29_col1\" class=\"data row29 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row29_col2\" class=\"data row29 col2\" >10</td>\n",
              "      <td id=\"T_2e877_row29_col3\" class=\"data row29 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row29_col4\" class=\"data row29 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row29_col5\" class=\"data row29 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row29_col6\" class=\"data row29 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row29_col7\" class=\"data row29 col7\" >0.997861</td>\n",
              "      <td id=\"T_2e877_row29_col8\" class=\"data row29 col8\" >0.999197</td>\n",
              "      <td id=\"T_2e877_row29_col9\" class=\"data row29 col9\" >0.998529</td>\n",
              "      <td id=\"T_2e877_row29_col10\" class=\"data row29 col10\" >0.999988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
              "      <td id=\"T_2e877_row30_col0\" class=\"data row30 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row30_col1\" class=\"data row30 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row30_col2\" class=\"data row30 col2\" >11</td>\n",
              "      <td id=\"T_2e877_row30_col3\" class=\"data row30 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row30_col4\" class=\"data row30 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row30_col5\" class=\"data row30 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row30_col6\" class=\"data row30 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row30_col7\" class=\"data row30 col7\" >0.960959</td>\n",
              "      <td id=\"T_2e877_row30_col8\" class=\"data row30 col8\" >0.889523</td>\n",
              "      <td id=\"T_2e877_row30_col9\" class=\"data row30 col9\" >0.923862</td>\n",
              "      <td id=\"T_2e877_row30_col10\" class=\"data row30 col10\" >0.982439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
              "      <td id=\"T_2e877_row31_col0\" class=\"data row31 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row31_col1\" class=\"data row31 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row31_col2\" class=\"data row31 col2\" >12</td>\n",
              "      <td id=\"T_2e877_row31_col3\" class=\"data row31 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row31_col4\" class=\"data row31 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row31_col5\" class=\"data row31 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row31_col6\" class=\"data row31 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row31_col7\" class=\"data row31 col7\" >0.868081</td>\n",
              "      <td id=\"T_2e877_row31_col8\" class=\"data row31 col8\" >0.962409</td>\n",
              "      <td id=\"T_2e877_row31_col9\" class=\"data row31 col9\" >0.912815</td>\n",
              "      <td id=\"T_2e877_row31_col10\" class=\"data row31 col10\" >0.984034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
              "      <td id=\"T_2e877_row32_col0\" class=\"data row32 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row32_col1\" class=\"data row32 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row32_col2\" class=\"data row32 col2\" >13</td>\n",
              "      <td id=\"T_2e877_row32_col3\" class=\"data row32 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row32_col4\" class=\"data row32 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row32_col5\" class=\"data row32 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row32_col6\" class=\"data row32 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row32_col7\" class=\"data row32 col7\" >0.987569</td>\n",
              "      <td id=\"T_2e877_row32_col8\" class=\"data row32 col8\" >0.976975</td>\n",
              "      <td id=\"T_2e877_row32_col9\" class=\"data row32 col9\" >0.982244</td>\n",
              "      <td id=\"T_2e877_row32_col10\" class=\"data row32 col10\" >0.999082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
              "      <td id=\"T_2e877_row33_col0\" class=\"data row33 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row33_col1\" class=\"data row33 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row33_col2\" class=\"data row33 col2\" >14</td>\n",
              "      <td id=\"T_2e877_row33_col3\" class=\"data row33 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row33_col4\" class=\"data row33 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row33_col5\" class=\"data row33 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row33_col6\" class=\"data row33 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row33_col7\" class=\"data row33 col7\" >0.931464</td>\n",
              "      <td id=\"T_2e877_row33_col8\" class=\"data row33 col8\" >0.828255</td>\n",
              "      <td id=\"T_2e877_row33_col9\" class=\"data row33 col9\" >0.876833</td>\n",
              "      <td id=\"T_2e877_row33_col10\" class=\"data row33 col10\" >0.924258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
              "      <td id=\"T_2e877_row34_col0\" class=\"data row34 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row34_col1\" class=\"data row34 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row34_col2\" class=\"data row34 col2\" >15</td>\n",
              "      <td id=\"T_2e877_row34_col3\" class=\"data row34 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row34_col4\" class=\"data row34 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row34_col5\" class=\"data row34 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row34_col6\" class=\"data row34 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row34_col7\" class=\"data row34 col7\" >0.994553</td>\n",
              "      <td id=\"T_2e877_row34_col8\" class=\"data row34 col8\" >0.993541</td>\n",
              "      <td id=\"T_2e877_row34_col9\" class=\"data row34 col9\" >0.994047</td>\n",
              "      <td id=\"T_2e877_row34_col10\" class=\"data row34 col10\" >0.999866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
              "      <td id=\"T_2e877_row35_col0\" class=\"data row35 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_2e877_row35_col1\" class=\"data row35 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row35_col2\" class=\"data row35 col2\" >16</td>\n",
              "      <td id=\"T_2e877_row35_col3\" class=\"data row35 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row35_col4\" class=\"data row35 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row35_col5\" class=\"data row35 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row35_col6\" class=\"data row35 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row35_col7\" class=\"data row35 col7\" >0.904762</td>\n",
              "      <td id=\"T_2e877_row35_col8\" class=\"data row35 col8\" >0.761905</td>\n",
              "      <td id=\"T_2e877_row35_col9\" class=\"data row35 col9\" >0.827211</td>\n",
              "      <td id=\"T_2e877_row35_col10\" class=\"data row35 col10\" >0.901711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
              "      <td id=\"T_2e877_row36_col0\" class=\"data row36 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row36_col1\" class=\"data row36 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row36_col2\" class=\"data row36 col2\" >Macro Average</td>\n",
              "      <td id=\"T_2e877_row36_col3\" class=\"data row36 col3\" >0.856832</td>\n",
              "      <td id=\"T_2e877_row36_col4\" class=\"data row36 col4\" >0.906306</td>\n",
              "      <td id=\"T_2e877_row36_col5\" class=\"data row36 col5\" >0.854948</td>\n",
              "      <td id=\"T_2e877_row36_col6\" class=\"data row36 col6\" >0.978468</td>\n",
              "      <td id=\"T_2e877_row36_col7\" class=\"data row36 col7\" >nan</td>\n",
              "      <td id=\"T_2e877_row36_col8\" class=\"data row36 col8\" >nan</td>\n",
              "      <td id=\"T_2e877_row36_col9\" class=\"data row36 col9\" >nan</td>\n",
              "      <td id=\"T_2e877_row36_col10\" class=\"data row36 col10\" >nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
              "      <td id=\"T_2e877_row37_col0\" class=\"data row37 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row37_col1\" class=\"data row37 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row37_col2\" class=\"data row37 col2\" >0</td>\n",
              "      <td id=\"T_2e877_row37_col3\" class=\"data row37 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row37_col4\" class=\"data row37 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row37_col5\" class=\"data row37 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row37_col6\" class=\"data row37 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row37_col7\" class=\"data row37 col7\" >0.000000</td>\n",
              "      <td id=\"T_2e877_row37_col8\" class=\"data row37 col8\" >0.000000</td>\n",
              "      <td id=\"T_2e877_row37_col9\" class=\"data row37 col9\" >0.000000</td>\n",
              "      <td id=\"T_2e877_row37_col10\" class=\"data row37 col10\" >0.999700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
              "      <td id=\"T_2e877_row38_col0\" class=\"data row38 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row38_col1\" class=\"data row38 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row38_col2\" class=\"data row38 col2\" >1</td>\n",
              "      <td id=\"T_2e877_row38_col3\" class=\"data row38 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row38_col4\" class=\"data row38 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row38_col5\" class=\"data row38 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row38_col6\" class=\"data row38 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row38_col7\" class=\"data row38 col7\" >0.986737</td>\n",
              "      <td id=\"T_2e877_row38_col8\" class=\"data row38 col8\" >0.988951</td>\n",
              "      <td id=\"T_2e877_row38_col9\" class=\"data row38 col9\" >0.987843</td>\n",
              "      <td id=\"T_2e877_row38_col10\" class=\"data row38 col10\" >0.997630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
              "      <td id=\"T_2e877_row39_col0\" class=\"data row39 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row39_col1\" class=\"data row39 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row39_col2\" class=\"data row39 col2\" >2</td>\n",
              "      <td id=\"T_2e877_row39_col3\" class=\"data row39 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row39_col4\" class=\"data row39 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row39_col5\" class=\"data row39 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row39_col6\" class=\"data row39 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row39_col7\" class=\"data row39 col7\" >0.986562</td>\n",
              "      <td id=\"T_2e877_row39_col8\" class=\"data row39 col8\" >0.993348</td>\n",
              "      <td id=\"T_2e877_row39_col9\" class=\"data row39 col9\" >0.989943</td>\n",
              "      <td id=\"T_2e877_row39_col10\" class=\"data row39 col10\" >0.997936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
              "      <td id=\"T_2e877_row40_col0\" class=\"data row40 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row40_col1\" class=\"data row40 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row40_col2\" class=\"data row40 col2\" >3</td>\n",
              "      <td id=\"T_2e877_row40_col3\" class=\"data row40 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row40_col4\" class=\"data row40 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row40_col5\" class=\"data row40 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row40_col6\" class=\"data row40 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row40_col7\" class=\"data row40 col7\" >0.997349</td>\n",
              "      <td id=\"T_2e877_row40_col8\" class=\"data row40 col8\" >0.997815</td>\n",
              "      <td id=\"T_2e877_row40_col9\" class=\"data row40 col9\" >0.997582</td>\n",
              "      <td id=\"T_2e877_row40_col10\" class=\"data row40 col10\" >0.999530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
              "      <td id=\"T_2e877_row41_col0\" class=\"data row41 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row41_col1\" class=\"data row41 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row41_col2\" class=\"data row41 col2\" >4</td>\n",
              "      <td id=\"T_2e877_row41_col3\" class=\"data row41 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row41_col4\" class=\"data row41 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row41_col5\" class=\"data row41 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row41_col6\" class=\"data row41 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row41_col7\" class=\"data row41 col7\" >0.989442</td>\n",
              "      <td id=\"T_2e877_row41_col8\" class=\"data row41 col8\" >0.943885</td>\n",
              "      <td id=\"T_2e877_row41_col9\" class=\"data row41 col9\" >0.966127</td>\n",
              "      <td id=\"T_2e877_row41_col10\" class=\"data row41 col10\" >0.986899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
              "      <td id=\"T_2e877_row42_col0\" class=\"data row42 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row42_col1\" class=\"data row42 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row42_col2\" class=\"data row42 col2\" >5</td>\n",
              "      <td id=\"T_2e877_row42_col3\" class=\"data row42 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row42_col4\" class=\"data row42 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row42_col5\" class=\"data row42 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row42_col6\" class=\"data row42 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row42_col7\" class=\"data row42 col7\" >0.013824</td>\n",
              "      <td id=\"T_2e877_row42_col8\" class=\"data row42 col8\" >0.996228</td>\n",
              "      <td id=\"T_2e877_row42_col9\" class=\"data row42 col9\" >0.027269</td>\n",
              "      <td id=\"T_2e877_row42_col10\" class=\"data row42 col10\" >0.959782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
              "      <td id=\"T_2e877_row43_col0\" class=\"data row43 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row43_col1\" class=\"data row43 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row43_col2\" class=\"data row43 col2\" >6</td>\n",
              "      <td id=\"T_2e877_row43_col3\" class=\"data row43 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row43_col4\" class=\"data row43 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row43_col5\" class=\"data row43 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row43_col6\" class=\"data row43 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row43_col7\" class=\"data row43 col7\" >0.953273</td>\n",
              "      <td id=\"T_2e877_row43_col8\" class=\"data row43 col8\" >0.971481</td>\n",
              "      <td id=\"T_2e877_row43_col9\" class=\"data row43 col9\" >0.962291</td>\n",
              "      <td id=\"T_2e877_row43_col10\" class=\"data row43 col10\" >0.992161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
              "      <td id=\"T_2e877_row44_col0\" class=\"data row44 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row44_col1\" class=\"data row44 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row44_col2\" class=\"data row44 col2\" >7</td>\n",
              "      <td id=\"T_2e877_row44_col3\" class=\"data row44 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row44_col4\" class=\"data row44 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row44_col5\" class=\"data row44 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row44_col6\" class=\"data row44 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row44_col7\" class=\"data row44 col7\" >0.698734</td>\n",
              "      <td id=\"T_2e877_row44_col8\" class=\"data row44 col8\" >0.691729</td>\n",
              "      <td id=\"T_2e877_row44_col9\" class=\"data row44 col9\" >0.695214</td>\n",
              "      <td id=\"T_2e877_row44_col10\" class=\"data row44 col10\" >0.725935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
              "      <td id=\"T_2e877_row45_col0\" class=\"data row45 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row45_col1\" class=\"data row45 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row45_col2\" class=\"data row45 col2\" >8</td>\n",
              "      <td id=\"T_2e877_row45_col3\" class=\"data row45 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row45_col4\" class=\"data row45 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row45_col5\" class=\"data row45 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row45_col6\" class=\"data row45 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row45_col7\" class=\"data row45 col7\" >0.993721</td>\n",
              "      <td id=\"T_2e877_row45_col8\" class=\"data row45 col8\" >0.994248</td>\n",
              "      <td id=\"T_2e877_row45_col9\" class=\"data row45 col9\" >0.993984</td>\n",
              "      <td id=\"T_2e877_row45_col10\" class=\"data row45 col10\" >0.998949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
              "      <td id=\"T_2e877_row46_col0\" class=\"data row46 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row46_col1\" class=\"data row46 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row46_col2\" class=\"data row46 col2\" >9</td>\n",
              "      <td id=\"T_2e877_row46_col3\" class=\"data row46 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row46_col4\" class=\"data row46 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row46_col5\" class=\"data row46 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row46_col6\" class=\"data row46 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row46_col7\" class=\"data row46 col7\" >0.997695</td>\n",
              "      <td id=\"T_2e877_row46_col8\" class=\"data row46 col8\" >0.996627</td>\n",
              "      <td id=\"T_2e877_row46_col9\" class=\"data row46 col9\" >0.997161</td>\n",
              "      <td id=\"T_2e877_row46_col10\" class=\"data row46 col10\" >0.998695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
              "      <td id=\"T_2e877_row47_col0\" class=\"data row47 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row47_col1\" class=\"data row47 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row47_col2\" class=\"data row47 col2\" >10</td>\n",
              "      <td id=\"T_2e877_row47_col3\" class=\"data row47 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row47_col4\" class=\"data row47 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row47_col5\" class=\"data row47 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row47_col6\" class=\"data row47 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row47_col7\" class=\"data row47 col7\" >0.974284</td>\n",
              "      <td id=\"T_2e877_row47_col8\" class=\"data row47 col8\" >0.963896</td>\n",
              "      <td id=\"T_2e877_row47_col9\" class=\"data row47 col9\" >0.969062</td>\n",
              "      <td id=\"T_2e877_row47_col10\" class=\"data row47 col10\" >0.993149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
              "      <td id=\"T_2e877_row48_col0\" class=\"data row48 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row48_col1\" class=\"data row48 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row48_col2\" class=\"data row48 col2\" >11</td>\n",
              "      <td id=\"T_2e877_row48_col3\" class=\"data row48 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row48_col4\" class=\"data row48 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row48_col5\" class=\"data row48 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row48_col6\" class=\"data row48 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row48_col7\" class=\"data row48 col7\" >0.989309</td>\n",
              "      <td id=\"T_2e877_row48_col8\" class=\"data row48 col8\" >0.969556</td>\n",
              "      <td id=\"T_2e877_row48_col9\" class=\"data row48 col9\" >0.979333</td>\n",
              "      <td id=\"T_2e877_row48_col10\" class=\"data row48 col10\" >0.995967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
              "      <td id=\"T_2e877_row49_col0\" class=\"data row49 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row49_col1\" class=\"data row49 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row49_col2\" class=\"data row49 col2\" >12</td>\n",
              "      <td id=\"T_2e877_row49_col3\" class=\"data row49 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row49_col4\" class=\"data row49 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row49_col5\" class=\"data row49 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row49_col6\" class=\"data row49 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row49_col7\" class=\"data row49 col7\" >0.996858</td>\n",
              "      <td id=\"T_2e877_row49_col8\" class=\"data row49 col8\" >0.993562</td>\n",
              "      <td id=\"T_2e877_row49_col9\" class=\"data row49 col9\" >0.995207</td>\n",
              "      <td id=\"T_2e877_row49_col10\" class=\"data row49 col10\" >0.999194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row50\" class=\"row_heading level0 row50\" >50</th>\n",
              "      <td id=\"T_2e877_row50_col0\" class=\"data row50 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row50_col1\" class=\"data row50 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row50_col2\" class=\"data row50 col2\" >13</td>\n",
              "      <td id=\"T_2e877_row50_col3\" class=\"data row50 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row50_col4\" class=\"data row50 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row50_col5\" class=\"data row50 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row50_col6\" class=\"data row50 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row50_col7\" class=\"data row50 col7\" >0.997158</td>\n",
              "      <td id=\"T_2e877_row50_col8\" class=\"data row50 col8\" >0.996860</td>\n",
              "      <td id=\"T_2e877_row50_col9\" class=\"data row50 col9\" >0.997009</td>\n",
              "      <td id=\"T_2e877_row50_col10\" class=\"data row50 col10\" >0.999605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row51\" class=\"row_heading level0 row51\" >51</th>\n",
              "      <td id=\"T_2e877_row51_col0\" class=\"data row51 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row51_col1\" class=\"data row51 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row51_col2\" class=\"data row51 col2\" >14</td>\n",
              "      <td id=\"T_2e877_row51_col3\" class=\"data row51 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row51_col4\" class=\"data row51 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row51_col5\" class=\"data row51 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row51_col6\" class=\"data row51 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row51_col7\" class=\"data row51 col7\" >0.996633</td>\n",
              "      <td id=\"T_2e877_row51_col8\" class=\"data row51 col8\" >0.998896</td>\n",
              "      <td id=\"T_2e877_row51_col9\" class=\"data row51 col9\" >0.997763</td>\n",
              "      <td id=\"T_2e877_row51_col10\" class=\"data row51 col10\" >0.999781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row52\" class=\"row_heading level0 row52\" >52</th>\n",
              "      <td id=\"T_2e877_row52_col0\" class=\"data row52 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row52_col1\" class=\"data row52 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row52_col2\" class=\"data row52 col2\" >15</td>\n",
              "      <td id=\"T_2e877_row52_col3\" class=\"data row52 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row52_col4\" class=\"data row52 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row52_col5\" class=\"data row52 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row52_col6\" class=\"data row52 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row52_col7\" class=\"data row52 col7\" >0.986297</td>\n",
              "      <td id=\"T_2e877_row52_col8\" class=\"data row52 col8\" >0.987943</td>\n",
              "      <td id=\"T_2e877_row52_col9\" class=\"data row52 col9\" >0.987119</td>\n",
              "      <td id=\"T_2e877_row52_col10\" class=\"data row52 col10\" >0.997713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_2e877_level0_row53\" class=\"row_heading level0 row53\" >53</th>\n",
              "      <td id=\"T_2e877_row53_col0\" class=\"data row53 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_2e877_row53_col1\" class=\"data row53 col1\" >Train</td>\n",
              "      <td id=\"T_2e877_row53_col2\" class=\"data row53 col2\" >16</td>\n",
              "      <td id=\"T_2e877_row53_col3\" class=\"data row53 col3\" >nan</td>\n",
              "      <td id=\"T_2e877_row53_col4\" class=\"data row53 col4\" >nan</td>\n",
              "      <td id=\"T_2e877_row53_col5\" class=\"data row53 col5\" >nan</td>\n",
              "      <td id=\"T_2e877_row53_col6\" class=\"data row53 col6\" >nan</td>\n",
              "      <td id=\"T_2e877_row53_col7\" class=\"data row53 col7\" >0.961460</td>\n",
              "      <td id=\"T_2e877_row53_col8\" class=\"data row53 col8\" >0.971141</td>\n",
              "      <td id=\"T_2e877_row53_col9\" class=\"data row53 col9\" >0.966276</td>\n",
              "      <td id=\"T_2e877_row53_col10\" class=\"data row53 col10\" >0.991340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x76894e7001c0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_743be th {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_743be td {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_743be index {\n",
              "  display: none;\n",
              "}\n",
              "#T_743be_row0_col0, #T_743be_row0_col1, #T_743be_row0_col2, #T_743be_row0_col3, #T_743be_row0_col4, #T_743be_row0_col5, #T_743be_row0_col6, #T_743be_row0_col7, #T_743be_row0_col8, #T_743be_row0_col9, #T_743be_row0_col10, #T_743be_row18_col0, #T_743be_row18_col1, #T_743be_row18_col2, #T_743be_row18_col3, #T_743be_row18_col4, #T_743be_row18_col5, #T_743be_row18_col6, #T_743be_row18_col7, #T_743be_row18_col8, #T_743be_row18_col9, #T_743be_row18_col10, #T_743be_row36_col0, #T_743be_row36_col1, #T_743be_row36_col2, #T_743be_row36_col3, #T_743be_row36_col4, #T_743be_row36_col5, #T_743be_row36_col6, #T_743be_row36_col7, #T_743be_row36_col8, #T_743be_row36_col9, #T_743be_row36_col10 {\n",
              "  border-top: 3px solid black;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_743be\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_743be_level0_col0\" class=\"col_heading level0 col0\" >Classifier</th>\n",
              "      <th id=\"T_743be_level0_col1\" class=\"col_heading level0 col1\" >Subset</th>\n",
              "      <th id=\"T_743be_level0_col2\" class=\"col_heading level0 col2\" >Class</th>\n",
              "      <th id=\"T_743be_level0_col3\" class=\"col_heading level0 col3\" >Macro Precision</th>\n",
              "      <th id=\"T_743be_level0_col4\" class=\"col_heading level0 col4\" >Macro Recall</th>\n",
              "      <th id=\"T_743be_level0_col5\" class=\"col_heading level0 col5\" >Macro F1</th>\n",
              "      <th id=\"T_743be_level0_col6\" class=\"col_heading level0 col6\" >Macro PR-AUC</th>\n",
              "      <th id=\"T_743be_level0_col7\" class=\"col_heading level0 col7\" >Precision</th>\n",
              "      <th id=\"T_743be_level0_col8\" class=\"col_heading level0 col8\" >Recall</th>\n",
              "      <th id=\"T_743be_level0_col9\" class=\"col_heading level0 col9\" >F1</th>\n",
              "      <th id=\"T_743be_level0_col10\" class=\"col_heading level0 col10\" >PR-AUC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_743be_row0_col0\" class=\"data row0 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row0_col1\" class=\"data row0 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row0_col2\" class=\"data row0 col2\" >Macro Average</td>\n",
              "      <td id=\"T_743be_row0_col3\" class=\"data row0 col3\" >0.812434</td>\n",
              "      <td id=\"T_743be_row0_col4\" class=\"data row0 col4\" >0.751128</td>\n",
              "      <td id=\"T_743be_row0_col5\" class=\"data row0 col5\" >0.771179</td>\n",
              "      <td id=\"T_743be_row0_col6\" class=\"data row0 col6\" >0.832086</td>\n",
              "      <td id=\"T_743be_row0_col7\" class=\"data row0 col7\" >nan</td>\n",
              "      <td id=\"T_743be_row0_col8\" class=\"data row0 col8\" >nan</td>\n",
              "      <td id=\"T_743be_row0_col9\" class=\"data row0 col9\" >nan</td>\n",
              "      <td id=\"T_743be_row0_col10\" class=\"data row0 col10\" >nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_743be_row1_col0\" class=\"data row1 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row1_col1\" class=\"data row1 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row1_col2\" class=\"data row1 col2\" >0</td>\n",
              "      <td id=\"T_743be_row1_col3\" class=\"data row1 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row1_col4\" class=\"data row1 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row1_col5\" class=\"data row1 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row1_col6\" class=\"data row1 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row1_col7\" class=\"data row1 col7\" >0.932819</td>\n",
              "      <td id=\"T_743be_row1_col8\" class=\"data row1 col8\" >0.904431</td>\n",
              "      <td id=\"T_743be_row1_col9\" class=\"data row1 col9\" >0.918406</td>\n",
              "      <td id=\"T_743be_row1_col10\" class=\"data row1 col10\" >0.966875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_743be_row2_col0\" class=\"data row2 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row2_col1\" class=\"data row2 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row2_col2\" class=\"data row2 col2\" >1</td>\n",
              "      <td id=\"T_743be_row2_col3\" class=\"data row2 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row2_col4\" class=\"data row2 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row2_col5\" class=\"data row2 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row2_col6\" class=\"data row2 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row2_col7\" class=\"data row2 col7\" >0.815266</td>\n",
              "      <td id=\"T_743be_row2_col8\" class=\"data row2 col8\" >0.885672</td>\n",
              "      <td id=\"T_743be_row2_col9\" class=\"data row2 col9\" >0.849012</td>\n",
              "      <td id=\"T_743be_row2_col10\" class=\"data row2 col10\" >0.933467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_743be_row3_col0\" class=\"data row3 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row3_col1\" class=\"data row3 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row3_col2\" class=\"data row3 col2\" >2</td>\n",
              "      <td id=\"T_743be_row3_col3\" class=\"data row3 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row3_col4\" class=\"data row3 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row3_col5\" class=\"data row3 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row3_col6\" class=\"data row3 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row3_col7\" class=\"data row3 col7\" >0.925129</td>\n",
              "      <td id=\"T_743be_row3_col8\" class=\"data row3 col8\" >0.878268</td>\n",
              "      <td id=\"T_743be_row3_col9\" class=\"data row3 col9\" >0.901090</td>\n",
              "      <td id=\"T_743be_row3_col10\" class=\"data row3 col10\" >0.959309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_743be_row4_col0\" class=\"data row4 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row4_col1\" class=\"data row4 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row4_col2\" class=\"data row4 col2\" >3</td>\n",
              "      <td id=\"T_743be_row4_col3\" class=\"data row4 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row4_col4\" class=\"data row4 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row4_col5\" class=\"data row4 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row4_col6\" class=\"data row4 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row4_col7\" class=\"data row4 col7\" >0.972594</td>\n",
              "      <td id=\"T_743be_row4_col8\" class=\"data row4 col8\" >0.973835</td>\n",
              "      <td id=\"T_743be_row4_col9\" class=\"data row4 col9\" >0.973214</td>\n",
              "      <td id=\"T_743be_row4_col10\" class=\"data row4 col10\" >0.993093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_743be_row5_col0\" class=\"data row5 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row5_col1\" class=\"data row5 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row5_col2\" class=\"data row5 col2\" >4</td>\n",
              "      <td id=\"T_743be_row5_col3\" class=\"data row5 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row5_col4\" class=\"data row5 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row5_col5\" class=\"data row5 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row5_col6\" class=\"data row5 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row5_col7\" class=\"data row5 col7\" >0.609982</td>\n",
              "      <td id=\"T_743be_row5_col8\" class=\"data row5 col8\" >0.423620</td>\n",
              "      <td id=\"T_743be_row5_col9\" class=\"data row5 col9\" >0.500000</td>\n",
              "      <td id=\"T_743be_row5_col10\" class=\"data row5 col10\" >0.638492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_743be_row6_col0\" class=\"data row6 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row6_col1\" class=\"data row6 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row6_col2\" class=\"data row6 col2\" >5</td>\n",
              "      <td id=\"T_743be_row6_col3\" class=\"data row6 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row6_col4\" class=\"data row6 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row6_col5\" class=\"data row6 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row6_col6\" class=\"data row6 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row6_col7\" class=\"data row6 col7\" >0.908589</td>\n",
              "      <td id=\"T_743be_row6_col8\" class=\"data row6 col8\" >0.952105</td>\n",
              "      <td id=\"T_743be_row6_col9\" class=\"data row6 col9\" >0.929838</td>\n",
              "      <td id=\"T_743be_row6_col10\" class=\"data row6 col10\" >0.980374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_743be_row7_col0\" class=\"data row7 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row7_col1\" class=\"data row7 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row7_col2\" class=\"data row7 col2\" >6</td>\n",
              "      <td id=\"T_743be_row7_col3\" class=\"data row7 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row7_col4\" class=\"data row7 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row7_col5\" class=\"data row7 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row7_col6\" class=\"data row7 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row7_col7\" class=\"data row7 col7\" >0.796460</td>\n",
              "      <td id=\"T_743be_row7_col8\" class=\"data row7 col8\" >0.782609</td>\n",
              "      <td id=\"T_743be_row7_col9\" class=\"data row7 col9\" >0.789474</td>\n",
              "      <td id=\"T_743be_row7_col10\" class=\"data row7 col10\" >0.842687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_743be_row8_col0\" class=\"data row8 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row8_col1\" class=\"data row8 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row8_col2\" class=\"data row8 col2\" >7</td>\n",
              "      <td id=\"T_743be_row8_col3\" class=\"data row8 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row8_col4\" class=\"data row8 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row8_col5\" class=\"data row8 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row8_col6\" class=\"data row8 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row8_col7\" class=\"data row8 col7\" >0.871881</td>\n",
              "      <td id=\"T_743be_row8_col8\" class=\"data row8 col8\" >0.920940</td>\n",
              "      <td id=\"T_743be_row8_col9\" class=\"data row8 col9\" >0.895740</td>\n",
              "      <td id=\"T_743be_row8_col10\" class=\"data row8 col10\" >0.949267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_743be_row9_col0\" class=\"data row9 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row9_col1\" class=\"data row9 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row9_col2\" class=\"data row9 col2\" >8</td>\n",
              "      <td id=\"T_743be_row9_col3\" class=\"data row9 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row9_col4\" class=\"data row9 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row9_col5\" class=\"data row9 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row9_col6\" class=\"data row9 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row9_col7\" class=\"data row9 col7\" >0.775087</td>\n",
              "      <td id=\"T_743be_row9_col8\" class=\"data row9 col8\" >0.584856</td>\n",
              "      <td id=\"T_743be_row9_col9\" class=\"data row9 col9\" >0.666667</td>\n",
              "      <td id=\"T_743be_row9_col10\" class=\"data row9 col10\" >0.736485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "      <td id=\"T_743be_row10_col0\" class=\"data row10 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row10_col1\" class=\"data row10 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row10_col2\" class=\"data row10 col2\" >9</td>\n",
              "      <td id=\"T_743be_row10_col3\" class=\"data row10 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row10_col4\" class=\"data row10 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row10_col5\" class=\"data row10 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row10_col6\" class=\"data row10 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row10_col7\" class=\"data row10 col7\" >0.821813</td>\n",
              "      <td id=\"T_743be_row10_col8\" class=\"data row10 col8\" >0.826893</td>\n",
              "      <td id=\"T_743be_row10_col9\" class=\"data row10 col9\" >0.824345</td>\n",
              "      <td id=\"T_743be_row10_col10\" class=\"data row10 col10\" >0.905676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "      <td id=\"T_743be_row11_col0\" class=\"data row11 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row11_col1\" class=\"data row11 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row11_col2\" class=\"data row11 col2\" >10</td>\n",
              "      <td id=\"T_743be_row11_col3\" class=\"data row11 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row11_col4\" class=\"data row11 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row11_col5\" class=\"data row11 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row11_col6\" class=\"data row11 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row11_col7\" class=\"data row11 col7\" >0.982079</td>\n",
              "      <td id=\"T_743be_row11_col8\" class=\"data row11 col8\" >0.985169</td>\n",
              "      <td id=\"T_743be_row11_col9\" class=\"data row11 col9\" >0.983621</td>\n",
              "      <td id=\"T_743be_row11_col10\" class=\"data row11 col10\" >0.995450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "      <td id=\"T_743be_row12_col0\" class=\"data row12 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row12_col1\" class=\"data row12 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row12_col2\" class=\"data row12 col2\" >11</td>\n",
              "      <td id=\"T_743be_row12_col3\" class=\"data row12 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row12_col4\" class=\"data row12 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row12_col5\" class=\"data row12 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row12_col6\" class=\"data row12 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row12_col7\" class=\"data row12 col7\" >0.846094</td>\n",
              "      <td id=\"T_743be_row12_col8\" class=\"data row12 col8\" >0.586595</td>\n",
              "      <td id=\"T_743be_row12_col9\" class=\"data row12 col9\" >0.692844</td>\n",
              "      <td id=\"T_743be_row12_col10\" class=\"data row12 col10\" >0.811269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "      <td id=\"T_743be_row13_col0\" class=\"data row13 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row13_col1\" class=\"data row13 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row13_col2\" class=\"data row13 col2\" >12</td>\n",
              "      <td id=\"T_743be_row13_col3\" class=\"data row13 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row13_col4\" class=\"data row13 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row13_col5\" class=\"data row13 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row13_col6\" class=\"data row13 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row13_col7\" class=\"data row13 col7\" >0.706212</td>\n",
              "      <td id=\"T_743be_row13_col8\" class=\"data row13 col8\" >0.861463</td>\n",
              "      <td id=\"T_743be_row13_col9\" class=\"data row13 col9\" >0.776150</td>\n",
              "      <td id=\"T_743be_row13_col10\" class=\"data row13 col10\" >0.853682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "      <td id=\"T_743be_row14_col0\" class=\"data row14 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row14_col1\" class=\"data row14 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row14_col2\" class=\"data row14 col2\" >13</td>\n",
              "      <td id=\"T_743be_row14_col3\" class=\"data row14 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row14_col4\" class=\"data row14 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row14_col5\" class=\"data row14 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row14_col6\" class=\"data row14 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row14_col7\" class=\"data row14 col7\" >0.866485</td>\n",
              "      <td id=\"T_743be_row14_col8\" class=\"data row14 col8\" >0.801008</td>\n",
              "      <td id=\"T_743be_row14_col9\" class=\"data row14 col9\" >0.832461</td>\n",
              "      <td id=\"T_743be_row14_col10\" class=\"data row14 col10\" >0.899886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
              "      <td id=\"T_743be_row15_col0\" class=\"data row15 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row15_col1\" class=\"data row15 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row15_col2\" class=\"data row15 col2\" >14</td>\n",
              "      <td id=\"T_743be_row15_col3\" class=\"data row15 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row15_col4\" class=\"data row15 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row15_col5\" class=\"data row15 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row15_col6\" class=\"data row15 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row15_col7\" class=\"data row15 col7\" >0.926829</td>\n",
              "      <td id=\"T_743be_row15_col8\" class=\"data row15 col8\" >0.457831</td>\n",
              "      <td id=\"T_743be_row15_col9\" class=\"data row15 col9\" >0.612903</td>\n",
              "      <td id=\"T_743be_row15_col10\" class=\"data row15 col10\" >0.609403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
              "      <td id=\"T_743be_row16_col0\" class=\"data row16 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row16_col1\" class=\"data row16 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row16_col2\" class=\"data row16 col2\" >15</td>\n",
              "      <td id=\"T_743be_row16_col3\" class=\"data row16 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row16_col4\" class=\"data row16 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row16_col5\" class=\"data row16 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row16_col6\" class=\"data row16 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row16_col7\" class=\"data row16 col7\" >0.942943</td>\n",
              "      <td id=\"T_743be_row16_col8\" class=\"data row16 col8\" >0.926937</td>\n",
              "      <td id=\"T_743be_row16_col9\" class=\"data row16 col9\" >0.934872</td>\n",
              "      <td id=\"T_743be_row16_col10\" class=\"data row16 col10\" >0.977756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
              "      <td id=\"T_743be_row17_col0\" class=\"data row17 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_743be_row17_col1\" class=\"data row17 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row17_col2\" class=\"data row17 col2\" >16</td>\n",
              "      <td id=\"T_743be_row17_col3\" class=\"data row17 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row17_col4\" class=\"data row17 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row17_col5\" class=\"data row17 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row17_col6\" class=\"data row17 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row17_col7\" class=\"data row17 col7\" >0.111111</td>\n",
              "      <td id=\"T_743be_row17_col8\" class=\"data row17 col8\" >0.016949</td>\n",
              "      <td id=\"T_743be_row17_col9\" class=\"data row17 col9\" >0.029412</td>\n",
              "      <td id=\"T_743be_row17_col10\" class=\"data row17 col10\" >0.092294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
              "      <td id=\"T_743be_row18_col0\" class=\"data row18 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row18_col1\" class=\"data row18 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row18_col2\" class=\"data row18 col2\" >Macro Average</td>\n",
              "      <td id=\"T_743be_row18_col3\" class=\"data row18 col3\" >0.787014</td>\n",
              "      <td id=\"T_743be_row18_col4\" class=\"data row18 col4\" >0.332194</td>\n",
              "      <td id=\"T_743be_row18_col5\" class=\"data row18 col5\" >0.419534</td>\n",
              "      <td id=\"T_743be_row18_col6\" class=\"data row18 col6\" >0.528552</td>\n",
              "      <td id=\"T_743be_row18_col7\" class=\"data row18 col7\" >nan</td>\n",
              "      <td id=\"T_743be_row18_col8\" class=\"data row18 col8\" >nan</td>\n",
              "      <td id=\"T_743be_row18_col9\" class=\"data row18 col9\" >nan</td>\n",
              "      <td id=\"T_743be_row18_col10\" class=\"data row18 col10\" >nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
              "      <td id=\"T_743be_row19_col0\" class=\"data row19 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row19_col1\" class=\"data row19 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row19_col2\" class=\"data row19 col2\" >0</td>\n",
              "      <td id=\"T_743be_row19_col3\" class=\"data row19 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row19_col4\" class=\"data row19 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row19_col5\" class=\"data row19 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row19_col6\" class=\"data row19 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row19_col7\" class=\"data row19 col7\" >0.955665</td>\n",
              "      <td id=\"T_743be_row19_col8\" class=\"data row19 col8\" >0.207154</td>\n",
              "      <td id=\"T_743be_row19_col9\" class=\"data row19 col9\" >0.340500</td>\n",
              "      <td id=\"T_743be_row19_col10\" class=\"data row19 col10\" >0.607536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
              "      <td id=\"T_743be_row20_col0\" class=\"data row20 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row20_col1\" class=\"data row20 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row20_col2\" class=\"data row20 col2\" >1</td>\n",
              "      <td id=\"T_743be_row20_col3\" class=\"data row20 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row20_col4\" class=\"data row20 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row20_col5\" class=\"data row20 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row20_col6\" class=\"data row20 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row20_col7\" class=\"data row20 col7\" >0.835391</td>\n",
              "      <td id=\"T_743be_row20_col8\" class=\"data row20 col8\" >0.199215</td>\n",
              "      <td id=\"T_743be_row20_col9\" class=\"data row20 col9\" >0.321712</td>\n",
              "      <td id=\"T_743be_row20_col10\" class=\"data row20 col10\" >0.499591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
              "      <td id=\"T_743be_row21_col0\" class=\"data row21 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row21_col1\" class=\"data row21 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row21_col2\" class=\"data row21 col2\" >2</td>\n",
              "      <td id=\"T_743be_row21_col3\" class=\"data row21 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row21_col4\" class=\"data row21 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row21_col5\" class=\"data row21 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row21_col6\" class=\"data row21 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row21_col7\" class=\"data row21 col7\" >0.935933</td>\n",
              "      <td id=\"T_743be_row21_col8\" class=\"data row21 col8\" >0.274510</td>\n",
              "      <td id=\"T_743be_row21_col9\" class=\"data row21 col9\" >0.424510</td>\n",
              "      <td id=\"T_743be_row21_col10\" class=\"data row21 col10\" >0.612346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
              "      <td id=\"T_743be_row22_col0\" class=\"data row22 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row22_col1\" class=\"data row22 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row22_col2\" class=\"data row22 col2\" >3</td>\n",
              "      <td id=\"T_743be_row22_col3\" class=\"data row22 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row22_col4\" class=\"data row22 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row22_col5\" class=\"data row22 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row22_col6\" class=\"data row22 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row22_col7\" class=\"data row22 col7\" >0.984177</td>\n",
              "      <td id=\"T_743be_row22_col8\" class=\"data row22 col8\" >0.396937</td>\n",
              "      <td id=\"T_743be_row22_col9\" class=\"data row22 col9\" >0.565712</td>\n",
              "      <td id=\"T_743be_row22_col10\" class=\"data row22 col10\" >0.700174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
              "      <td id=\"T_743be_row23_col0\" class=\"data row23 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row23_col1\" class=\"data row23 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row23_col2\" class=\"data row23 col2\" >4</td>\n",
              "      <td id=\"T_743be_row23_col3\" class=\"data row23 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row23_col4\" class=\"data row23 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row23_col5\" class=\"data row23 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row23_col6\" class=\"data row23 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row23_col7\" class=\"data row23 col7\" >0.670051</td>\n",
              "      <td id=\"T_743be_row23_col8\" class=\"data row23 col8\" >0.169448</td>\n",
              "      <td id=\"T_743be_row23_col9\" class=\"data row23 col9\" >0.270492</td>\n",
              "      <td id=\"T_743be_row23_col10\" class=\"data row23 col10\" >0.251057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
              "      <td id=\"T_743be_row24_col0\" class=\"data row24 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row24_col1\" class=\"data row24 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row24_col2\" class=\"data row24 col2\" >5</td>\n",
              "      <td id=\"T_743be_row24_col3\" class=\"data row24 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row24_col4\" class=\"data row24 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row24_col5\" class=\"data row24 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row24_col6\" class=\"data row24 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row24_col7\" class=\"data row24 col7\" >0.961872</td>\n",
              "      <td id=\"T_743be_row24_col8\" class=\"data row24 col8\" >0.292105</td>\n",
              "      <td id=\"T_743be_row24_col9\" class=\"data row24 col9\" >0.448123</td>\n",
              "      <td id=\"T_743be_row24_col10\" class=\"data row24 col10\" >0.640033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
              "      <td id=\"T_743be_row25_col0\" class=\"data row25 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row25_col1\" class=\"data row25 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row25_col2\" class=\"data row25 col2\" >6</td>\n",
              "      <td id=\"T_743be_row25_col3\" class=\"data row25 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row25_col4\" class=\"data row25 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row25_col5\" class=\"data row25 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row25_col6\" class=\"data row25 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row25_col7\" class=\"data row25 col7\" >0.924528</td>\n",
              "      <td id=\"T_743be_row25_col8\" class=\"data row25 col8\" >0.426087</td>\n",
              "      <td id=\"T_743be_row25_col9\" class=\"data row25 col9\" >0.583333</td>\n",
              "      <td id=\"T_743be_row25_col10\" class=\"data row25 col10\" >0.601647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
              "      <td id=\"T_743be_row26_col0\" class=\"data row26 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row26_col1\" class=\"data row26 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row26_col2\" class=\"data row26 col2\" >7</td>\n",
              "      <td id=\"T_743be_row26_col3\" class=\"data row26 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row26_col4\" class=\"data row26 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row26_col5\" class=\"data row26 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row26_col6\" class=\"data row26 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row26_col7\" class=\"data row26 col7\" >0.236573</td>\n",
              "      <td id=\"T_743be_row26_col8\" class=\"data row26 col8\" >0.973647</td>\n",
              "      <td id=\"T_743be_row26_col9\" class=\"data row26 col9\" >0.380656</td>\n",
              "      <td id=\"T_743be_row26_col10\" class=\"data row26 col10\" >0.637029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
              "      <td id=\"T_743be_row27_col0\" class=\"data row27 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row27_col1\" class=\"data row27 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row27_col2\" class=\"data row27 col2\" >8</td>\n",
              "      <td id=\"T_743be_row27_col3\" class=\"data row27 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row27_col4\" class=\"data row27 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row27_col5\" class=\"data row27 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row27_col6\" class=\"data row27 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row27_col7\" class=\"data row27 col7\" >0.613333</td>\n",
              "      <td id=\"T_743be_row27_col8\" class=\"data row27 col8\" >0.240209</td>\n",
              "      <td id=\"T_743be_row27_col9\" class=\"data row27 col9\" >0.345216</td>\n",
              "      <td id=\"T_743be_row27_col10\" class=\"data row27 col10\" >0.329990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
              "      <td id=\"T_743be_row28_col0\" class=\"data row28 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row28_col1\" class=\"data row28 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row28_col2\" class=\"data row28 col2\" >9</td>\n",
              "      <td id=\"T_743be_row28_col3\" class=\"data row28 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row28_col4\" class=\"data row28 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row28_col5\" class=\"data row28 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row28_col6\" class=\"data row28 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row28_col7\" class=\"data row28 col7\" >0.955224</td>\n",
              "      <td id=\"T_743be_row28_col8\" class=\"data row28 col8\" >0.296754</td>\n",
              "      <td id=\"T_743be_row28_col9\" class=\"data row28 col9\" >0.452830</td>\n",
              "      <td id=\"T_743be_row28_col10\" class=\"data row28 col10\" >0.578153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
              "      <td id=\"T_743be_row29_col0\" class=\"data row29 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row29_col1\" class=\"data row29 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row29_col2\" class=\"data row29 col2\" >10</td>\n",
              "      <td id=\"T_743be_row29_col3\" class=\"data row29 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row29_col4\" class=\"data row29 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row29_col5\" class=\"data row29 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row29_col6\" class=\"data row29 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row29_col7\" class=\"data row29 col7\" >0.989418</td>\n",
              "      <td id=\"T_743be_row29_col8\" class=\"data row29 col8\" >0.504270</td>\n",
              "      <td id=\"T_743be_row29_col9\" class=\"data row29 col9\" >0.668056</td>\n",
              "      <td id=\"T_743be_row29_col10\" class=\"data row29 col10\" >0.779282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
              "      <td id=\"T_743be_row30_col0\" class=\"data row30 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row30_col1\" class=\"data row30 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row30_col2\" class=\"data row30 col2\" >11</td>\n",
              "      <td id=\"T_743be_row30_col3\" class=\"data row30 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row30_col4\" class=\"data row30 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row30_col5\" class=\"data row30 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row30_col6\" class=\"data row30 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row30_col7\" class=\"data row30 col7\" >0.805430</td>\n",
              "      <td id=\"T_743be_row30_col8\" class=\"data row30 col8\" >0.190885</td>\n",
              "      <td id=\"T_743be_row30_col9\" class=\"data row30 col9\" >0.308626</td>\n",
              "      <td id=\"T_743be_row30_col10\" class=\"data row30 col10\" >0.423616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
              "      <td id=\"T_743be_row31_col0\" class=\"data row31 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row31_col1\" class=\"data row31 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row31_col2\" class=\"data row31 col2\" >12</td>\n",
              "      <td id=\"T_743be_row31_col3\" class=\"data row31 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row31_col4\" class=\"data row31 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row31_col5\" class=\"data row31 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row31_col6\" class=\"data row31 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row31_col7\" class=\"data row31 col7\" >0.758146</td>\n",
              "      <td id=\"T_743be_row31_col8\" class=\"data row31 col8\" >0.537236</td>\n",
              "      <td id=\"T_743be_row31_col9\" class=\"data row31 col9\" >0.628854</td>\n",
              "      <td id=\"T_743be_row31_col10\" class=\"data row31 col10\" >0.694259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
              "      <td id=\"T_743be_row32_col0\" class=\"data row32 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row32_col1\" class=\"data row32 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row32_col2\" class=\"data row32 col2\" >13</td>\n",
              "      <td id=\"T_743be_row32_col3\" class=\"data row32 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row32_col4\" class=\"data row32 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row32_col5\" class=\"data row32 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row32_col6\" class=\"data row32 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row32_col7\" class=\"data row32 col7\" >0.944000</td>\n",
              "      <td id=\"T_743be_row32_col8\" class=\"data row32 col8\" >0.297229</td>\n",
              "      <td id=\"T_743be_row32_col9\" class=\"data row32 col9\" >0.452107</td>\n",
              "      <td id=\"T_743be_row32_col10\" class=\"data row32 col10\" >0.561892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
              "      <td id=\"T_743be_row33_col0\" class=\"data row33 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row33_col1\" class=\"data row33 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row33_col2\" class=\"data row33 col2\" >14</td>\n",
              "      <td id=\"T_743be_row33_col3\" class=\"data row33 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row33_col4\" class=\"data row33 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row33_col5\" class=\"data row33 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row33_col6\" class=\"data row33 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row33_col7\" class=\"data row33 col7\" >0.722222</td>\n",
              "      <td id=\"T_743be_row33_col8\" class=\"data row33 col8\" >0.313253</td>\n",
              "      <td id=\"T_743be_row33_col9\" class=\"data row33 col9\" >0.436975</td>\n",
              "      <td id=\"T_743be_row33_col10\" class=\"data row33 col10\" >0.353550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
              "      <td id=\"T_743be_row34_col0\" class=\"data row34 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row34_col1\" class=\"data row34 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row34_col2\" class=\"data row34 col2\" >15</td>\n",
              "      <td id=\"T_743be_row34_col3\" class=\"data row34 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row34_col4\" class=\"data row34 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row34_col5\" class=\"data row34 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row34_col6\" class=\"data row34 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row34_col7\" class=\"data row34 col7\" >0.969623</td>\n",
              "      <td id=\"T_743be_row34_col8\" class=\"data row34 col8\" >0.294465</td>\n",
              "      <td id=\"T_743be_row34_col9\" class=\"data row34 col9\" >0.451741</td>\n",
              "      <td id=\"T_743be_row34_col10\" class=\"data row34 col10\" >0.677901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
              "      <td id=\"T_743be_row35_col0\" class=\"data row35 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_743be_row35_col1\" class=\"data row35 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row35_col2\" class=\"data row35 col2\" >16</td>\n",
              "      <td id=\"T_743be_row35_col3\" class=\"data row35 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row35_col4\" class=\"data row35 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row35_col5\" class=\"data row35 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row35_col6\" class=\"data row35 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row35_col7\" class=\"data row35 col7\" >0.117647</td>\n",
              "      <td id=\"T_743be_row35_col8\" class=\"data row35 col8\" >0.033898</td>\n",
              "      <td id=\"T_743be_row35_col9\" class=\"data row35 col9\" >0.052632</td>\n",
              "      <td id=\"T_743be_row35_col10\" class=\"data row35 col10\" >0.037321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
              "      <td id=\"T_743be_row36_col0\" class=\"data row36 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row36_col1\" class=\"data row36 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row36_col2\" class=\"data row36 col2\" >Macro Average</td>\n",
              "      <td id=\"T_743be_row36_col3\" class=\"data row36 col3\" >0.786031</td>\n",
              "      <td id=\"T_743be_row36_col4\" class=\"data row36 col4\" >0.766755</td>\n",
              "      <td id=\"T_743be_row36_col5\" class=\"data row36 col5\" >0.739038</td>\n",
              "      <td id=\"T_743be_row36_col6\" class=\"data row36 col6\" >0.862416</td>\n",
              "      <td id=\"T_743be_row36_col7\" class=\"data row36 col7\" >nan</td>\n",
              "      <td id=\"T_743be_row36_col8\" class=\"data row36 col8\" >nan</td>\n",
              "      <td id=\"T_743be_row36_col9\" class=\"data row36 col9\" >nan</td>\n",
              "      <td id=\"T_743be_row36_col10\" class=\"data row36 col10\" >nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
              "      <td id=\"T_743be_row37_col0\" class=\"data row37 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row37_col1\" class=\"data row37 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row37_col2\" class=\"data row37 col2\" >0</td>\n",
              "      <td id=\"T_743be_row37_col3\" class=\"data row37 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row37_col4\" class=\"data row37 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row37_col5\" class=\"data row37 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row37_col6\" class=\"data row37 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row37_col7\" class=\"data row37 col7\" >0.000000</td>\n",
              "      <td id=\"T_743be_row37_col8\" class=\"data row37 col8\" >0.000000</td>\n",
              "      <td id=\"T_743be_row37_col9\" class=\"data row37 col9\" >0.000000</td>\n",
              "      <td id=\"T_743be_row37_col10\" class=\"data row37 col10\" >0.999789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
              "      <td id=\"T_743be_row38_col0\" class=\"data row38 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row38_col1\" class=\"data row38 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row38_col2\" class=\"data row38 col2\" >1</td>\n",
              "      <td id=\"T_743be_row38_col3\" class=\"data row38 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row38_col4\" class=\"data row38 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row38_col5\" class=\"data row38 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row38_col6\" class=\"data row38 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row38_col7\" class=\"data row38 col7\" >0.806084</td>\n",
              "      <td id=\"T_743be_row38_col8\" class=\"data row38 col8\" >0.924739</td>\n",
              "      <td id=\"T_743be_row38_col9\" class=\"data row38 col9\" >0.861345</td>\n",
              "      <td id=\"T_743be_row38_col10\" class=\"data row38 col10\" >0.906684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
              "      <td id=\"T_743be_row39_col0\" class=\"data row39 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row39_col1\" class=\"data row39 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row39_col2\" class=\"data row39 col2\" >2</td>\n",
              "      <td id=\"T_743be_row39_col3\" class=\"data row39 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row39_col4\" class=\"data row39 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row39_col5\" class=\"data row39 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row39_col6\" class=\"data row39 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row39_col7\" class=\"data row39 col7\" >0.926340</td>\n",
              "      <td id=\"T_743be_row39_col8\" class=\"data row39 col8\" >0.974975</td>\n",
              "      <td id=\"T_743be_row39_col9\" class=\"data row39 col9\" >0.950036</td>\n",
              "      <td id=\"T_743be_row39_col10\" class=\"data row39 col10\" >0.971623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
              "      <td id=\"T_743be_row40_col0\" class=\"data row40 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row40_col1\" class=\"data row40 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row40_col2\" class=\"data row40 col2\" >3</td>\n",
              "      <td id=\"T_743be_row40_col3\" class=\"data row40 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row40_col4\" class=\"data row40 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row40_col5\" class=\"data row40 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row40_col6\" class=\"data row40 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row40_col7\" class=\"data row40 col7\" >0.983397</td>\n",
              "      <td id=\"T_743be_row40_col8\" class=\"data row40 col8\" >0.982770</td>\n",
              "      <td id=\"T_743be_row40_col9\" class=\"data row40 col9\" >0.983083</td>\n",
              "      <td id=\"T_743be_row40_col10\" class=\"data row40 col10\" >0.985768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
              "      <td id=\"T_743be_row41_col0\" class=\"data row41 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row41_col1\" class=\"data row41 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row41_col2\" class=\"data row41 col2\" >4</td>\n",
              "      <td id=\"T_743be_row41_col3\" class=\"data row41 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row41_col4\" class=\"data row41 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row41_col5\" class=\"data row41 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row41_col6\" class=\"data row41 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row41_col7\" class=\"data row41 col7\" >0.905882</td>\n",
              "      <td id=\"T_743be_row41_col8\" class=\"data row41 col8\" >0.669565</td>\n",
              "      <td id=\"T_743be_row41_col9\" class=\"data row41 col9\" >0.770000</td>\n",
              "      <td id=\"T_743be_row41_col10\" class=\"data row41 col10\" >0.745403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
              "      <td id=\"T_743be_row42_col0\" class=\"data row42 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row42_col1\" class=\"data row42 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row42_col2\" class=\"data row42 col2\" >5</td>\n",
              "      <td id=\"T_743be_row42_col3\" class=\"data row42 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row42_col4\" class=\"data row42 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row42_col5\" class=\"data row42 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row42_col6\" class=\"data row42 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row42_col7\" class=\"data row42 col7\" >0.010869</td>\n",
              "      <td id=\"T_743be_row42_col8\" class=\"data row42 col8\" >0.982764</td>\n",
              "      <td id=\"T_743be_row42_col9\" class=\"data row42 col9\" >0.021499</td>\n",
              "      <td id=\"T_743be_row42_col10\" class=\"data row42 col10\" >0.939402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
              "      <td id=\"T_743be_row43_col0\" class=\"data row43 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row43_col1\" class=\"data row43 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row43_col2\" class=\"data row43 col2\" >6</td>\n",
              "      <td id=\"T_743be_row43_col3\" class=\"data row43 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row43_col4\" class=\"data row43 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row43_col5\" class=\"data row43 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row43_col6\" class=\"data row43 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row43_col7\" class=\"data row43 col7\" >0.861538</td>\n",
              "      <td id=\"T_743be_row43_col8\" class=\"data row43 col8\" >0.846348</td>\n",
              "      <td id=\"T_743be_row43_col9\" class=\"data row43 col9\" >0.853875</td>\n",
              "      <td id=\"T_743be_row43_col10\" class=\"data row43 col10\" >0.876091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
              "      <td id=\"T_743be_row44_col0\" class=\"data row44 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row44_col1\" class=\"data row44 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row44_col2\" class=\"data row44 col2\" >7</td>\n",
              "      <td id=\"T_743be_row44_col3\" class=\"data row44 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row44_col4\" class=\"data row44 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row44_col5\" class=\"data row44 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row44_col6\" class=\"data row44 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row44_col7\" class=\"data row44 col7\" >0.555556</td>\n",
              "      <td id=\"T_743be_row44_col8\" class=\"data row44 col8\" >0.169492</td>\n",
              "      <td id=\"T_743be_row44_col9\" class=\"data row44 col9\" >0.259740</td>\n",
              "      <td id=\"T_743be_row44_col10\" class=\"data row44 col10\" >0.164507</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
              "      <td id=\"T_743be_row45_col0\" class=\"data row45 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row45_col1\" class=\"data row45 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row45_col2\" class=\"data row45 col2\" >8</td>\n",
              "      <td id=\"T_743be_row45_col3\" class=\"data row45 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row45_col4\" class=\"data row45 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row45_col5\" class=\"data row45 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row45_col6\" class=\"data row45 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row45_col7\" class=\"data row45 col7\" >0.948538</td>\n",
              "      <td id=\"T_743be_row45_col8\" class=\"data row45 col8\" >0.897786</td>\n",
              "      <td id=\"T_743be_row45_col9\" class=\"data row45 col9\" >0.922464</td>\n",
              "      <td id=\"T_743be_row45_col10\" class=\"data row45 col10\" >0.910371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
              "      <td id=\"T_743be_row46_col0\" class=\"data row46 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row46_col1\" class=\"data row46 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row46_col2\" class=\"data row46 col2\" >9</td>\n",
              "      <td id=\"T_743be_row46_col3\" class=\"data row46 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row46_col4\" class=\"data row46 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row46_col5\" class=\"data row46 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row46_col6\" class=\"data row46 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row46_col7\" class=\"data row46 col7\" >0.987805</td>\n",
              "      <td id=\"T_743be_row46_col8\" class=\"data row46 col8\" >0.982921</td>\n",
              "      <td id=\"T_743be_row46_col9\" class=\"data row46 col9\" >0.985357</td>\n",
              "      <td id=\"T_743be_row46_col10\" class=\"data row46 col10\" >0.988136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
              "      <td id=\"T_743be_row47_col0\" class=\"data row47 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row47_col1\" class=\"data row47 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row47_col2\" class=\"data row47 col2\" >10</td>\n",
              "      <td id=\"T_743be_row47_col3\" class=\"data row47 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row47_col4\" class=\"data row47 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row47_col5\" class=\"data row47 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row47_col6\" class=\"data row47 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row47_col7\" class=\"data row47 col7\" >0.888889</td>\n",
              "      <td id=\"T_743be_row47_col8\" class=\"data row47 col8\" >0.605744</td>\n",
              "      <td id=\"T_743be_row47_col9\" class=\"data row47 col9\" >0.720497</td>\n",
              "      <td id=\"T_743be_row47_col10\" class=\"data row47 col10\" >0.678128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
              "      <td id=\"T_743be_row48_col0\" class=\"data row48 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row48_col1\" class=\"data row48 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row48_col2\" class=\"data row48 col2\" >11</td>\n",
              "      <td id=\"T_743be_row48_col3\" class=\"data row48 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row48_col4\" class=\"data row48 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row48_col5\" class=\"data row48 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row48_col6\" class=\"data row48 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row48_col7\" class=\"data row48 col7\" >0.946461</td>\n",
              "      <td id=\"T_743be_row48_col8\" class=\"data row48 col8\" >0.852124</td>\n",
              "      <td id=\"T_743be_row48_col9\" class=\"data row48 col9\" >0.896819</td>\n",
              "      <td id=\"T_743be_row48_col10\" class=\"data row48 col10\" >0.915791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
              "      <td id=\"T_743be_row49_col0\" class=\"data row49 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row49_col1\" class=\"data row49 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row49_col2\" class=\"data row49 col2\" >12</td>\n",
              "      <td id=\"T_743be_row49_col3\" class=\"data row49 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row49_col4\" class=\"data row49 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row49_col5\" class=\"data row49 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row49_col6\" class=\"data row49 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row49_col7\" class=\"data row49 col7\" >0.975288</td>\n",
              "      <td id=\"T_743be_row49_col8\" class=\"data row49 col8\" >0.914992</td>\n",
              "      <td id=\"T_743be_row49_col9\" class=\"data row49 col9\" >0.944179</td>\n",
              "      <td id=\"T_743be_row49_col10\" class=\"data row49 col10\" >0.945307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row50\" class=\"row_heading level0 row50\" >50</th>\n",
              "      <td id=\"T_743be_row50_col0\" class=\"data row50 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row50_col1\" class=\"data row50 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row50_col2\" class=\"data row50 col2\" >13</td>\n",
              "      <td id=\"T_743be_row50_col3\" class=\"data row50 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row50_col4\" class=\"data row50 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row50_col5\" class=\"data row50 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row50_col6\" class=\"data row50 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row50_col7\" class=\"data row50 col7\" >0.985934</td>\n",
              "      <td id=\"T_743be_row50_col8\" class=\"data row50 col8\" >0.989730</td>\n",
              "      <td id=\"T_743be_row50_col9\" class=\"data row50 col9\" >0.987828</td>\n",
              "      <td id=\"T_743be_row50_col10\" class=\"data row50 col10\" >0.993713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row51\" class=\"row_heading level0 row51\" >51</th>\n",
              "      <td id=\"T_743be_row51_col0\" class=\"data row51 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row51_col1\" class=\"data row51 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row51_col2\" class=\"data row51 col2\" >14</td>\n",
              "      <td id=\"T_743be_row51_col3\" class=\"data row51 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row51_col4\" class=\"data row51 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row51_col5\" class=\"data row51 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row51_col6\" class=\"data row51 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row51_col7\" class=\"data row51 col7\" >0.952163</td>\n",
              "      <td id=\"T_743be_row51_col8\" class=\"data row51 col8\" >0.984737</td>\n",
              "      <td id=\"T_743be_row51_col9\" class=\"data row51 col9\" >0.968176</td>\n",
              "      <td id=\"T_743be_row51_col10\" class=\"data row51 col10\" >0.988414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row52\" class=\"row_heading level0 row52\" >52</th>\n",
              "      <td id=\"T_743be_row52_col0\" class=\"data row52 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row52_col1\" class=\"data row52 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row52_col2\" class=\"data row52 col2\" >15</td>\n",
              "      <td id=\"T_743be_row52_col3\" class=\"data row52 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row52_col4\" class=\"data row52 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row52_col5\" class=\"data row52 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row52_col6\" class=\"data row52 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row52_col7\" class=\"data row52 col7\" >0.888535</td>\n",
              "      <td id=\"T_743be_row52_col8\" class=\"data row52 col8\" >0.893753</td>\n",
              "      <td id=\"T_743be_row52_col9\" class=\"data row52 col9\" >0.891137</td>\n",
              "      <td id=\"T_743be_row52_col10\" class=\"data row52 col10\" >0.920146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_743be_level0_row53\" class=\"row_heading level0 row53\" >53</th>\n",
              "      <td id=\"T_743be_row53_col0\" class=\"data row53 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_743be_row53_col1\" class=\"data row53 col1\" >Dev</td>\n",
              "      <td id=\"T_743be_row53_col2\" class=\"data row53 col2\" >16</td>\n",
              "      <td id=\"T_743be_row53_col3\" class=\"data row53 col3\" >nan</td>\n",
              "      <td id=\"T_743be_row53_col4\" class=\"data row53 col4\" >nan</td>\n",
              "      <td id=\"T_743be_row53_col5\" class=\"data row53 col5\" >nan</td>\n",
              "      <td id=\"T_743be_row53_col6\" class=\"data row53 col6\" >nan</td>\n",
              "      <td id=\"T_743be_row53_col7\" class=\"data row53 col7\" >0.765281</td>\n",
              "      <td id=\"T_743be_row53_col8\" class=\"data row53 col8\" >0.671314</td>\n",
              "      <td id=\"T_743be_row53_col9\" class=\"data row53 col9\" >0.715224</td>\n",
              "      <td id=\"T_743be_row53_col10\" class=\"data row53 col10\" >0.731798</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7689663bfb50>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_f7310 th {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_f7310 td {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_f7310 index {\n",
              "  display: none;\n",
              "}\n",
              "#T_f7310_row0_col0, #T_f7310_row0_col1, #T_f7310_row0_col2, #T_f7310_row0_col3, #T_f7310_row0_col4, #T_f7310_row0_col5, #T_f7310_row0_col6, #T_f7310_row0_col7, #T_f7310_row0_col8, #T_f7310_row0_col9, #T_f7310_row0_col10, #T_f7310_row18_col0, #T_f7310_row18_col1, #T_f7310_row18_col2, #T_f7310_row18_col3, #T_f7310_row18_col4, #T_f7310_row18_col5, #T_f7310_row18_col6, #T_f7310_row18_col7, #T_f7310_row18_col8, #T_f7310_row18_col9, #T_f7310_row18_col10, #T_f7310_row36_col0, #T_f7310_row36_col1, #T_f7310_row36_col2, #T_f7310_row36_col3, #T_f7310_row36_col4, #T_f7310_row36_col5, #T_f7310_row36_col6, #T_f7310_row36_col7, #T_f7310_row36_col8, #T_f7310_row36_col9, #T_f7310_row36_col10 {\n",
              "  border-top: 3px solid black;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_f7310\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_f7310_level0_col0\" class=\"col_heading level0 col0\" >Classifier</th>\n",
              "      <th id=\"T_f7310_level0_col1\" class=\"col_heading level0 col1\" >Subset</th>\n",
              "      <th id=\"T_f7310_level0_col2\" class=\"col_heading level0 col2\" >Class</th>\n",
              "      <th id=\"T_f7310_level0_col3\" class=\"col_heading level0 col3\" >Macro Precision</th>\n",
              "      <th id=\"T_f7310_level0_col4\" class=\"col_heading level0 col4\" >Macro Recall</th>\n",
              "      <th id=\"T_f7310_level0_col5\" class=\"col_heading level0 col5\" >Macro F1</th>\n",
              "      <th id=\"T_f7310_level0_col6\" class=\"col_heading level0 col6\" >Macro PR-AUC</th>\n",
              "      <th id=\"T_f7310_level0_col7\" class=\"col_heading level0 col7\" >Precision</th>\n",
              "      <th id=\"T_f7310_level0_col8\" class=\"col_heading level0 col8\" >Recall</th>\n",
              "      <th id=\"T_f7310_level0_col9\" class=\"col_heading level0 col9\" >F1</th>\n",
              "      <th id=\"T_f7310_level0_col10\" class=\"col_heading level0 col10\" >PR-AUC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_f7310_row0_col0\" class=\"data row0 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row0_col1\" class=\"data row0 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row0_col2\" class=\"data row0 col2\" >Macro Average</td>\n",
              "      <td id=\"T_f7310_row0_col3\" class=\"data row0 col3\" >0.848511</td>\n",
              "      <td id=\"T_f7310_row0_col4\" class=\"data row0 col4\" >0.765812</td>\n",
              "      <td id=\"T_f7310_row0_col5\" class=\"data row0 col5\" >0.787352</td>\n",
              "      <td id=\"T_f7310_row0_col6\" class=\"data row0 col6\" >0.837661</td>\n",
              "      <td id=\"T_f7310_row0_col7\" class=\"data row0 col7\" >nan</td>\n",
              "      <td id=\"T_f7310_row0_col8\" class=\"data row0 col8\" >nan</td>\n",
              "      <td id=\"T_f7310_row0_col9\" class=\"data row0 col9\" >nan</td>\n",
              "      <td id=\"T_f7310_row0_col10\" class=\"data row0 col10\" >nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_f7310_row1_col0\" class=\"data row1 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row1_col1\" class=\"data row1 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row1_col2\" class=\"data row1 col2\" >0</td>\n",
              "      <td id=\"T_f7310_row1_col3\" class=\"data row1 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row1_col4\" class=\"data row1 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row1_col5\" class=\"data row1 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row1_col6\" class=\"data row1 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row1_col7\" class=\"data row1 col7\" >0.924165</td>\n",
              "      <td id=\"T_f7310_row1_col8\" class=\"data row1 col8\" >0.910256</td>\n",
              "      <td id=\"T_f7310_row1_col9\" class=\"data row1 col9\" >0.917158</td>\n",
              "      <td id=\"T_f7310_row1_col10\" class=\"data row1 col10\" >0.959858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_f7310_row2_col0\" class=\"data row2 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row2_col1\" class=\"data row2 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row2_col2\" class=\"data row2 col2\" >1</td>\n",
              "      <td id=\"T_f7310_row2_col3\" class=\"data row2 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row2_col4\" class=\"data row2 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row2_col5\" class=\"data row2 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row2_col6\" class=\"data row2 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row2_col7\" class=\"data row2 col7\" >0.825730</td>\n",
              "      <td id=\"T_f7310_row2_col8\" class=\"data row2 col8\" >0.891626</td>\n",
              "      <td id=\"T_f7310_row2_col9\" class=\"data row2 col9\" >0.857414</td>\n",
              "      <td id=\"T_f7310_row2_col10\" class=\"data row2 col10\" >0.936331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_f7310_row3_col0\" class=\"data row3 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row3_col1\" class=\"data row3 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row3_col2\" class=\"data row3 col2\" >2</td>\n",
              "      <td id=\"T_f7310_row3_col3\" class=\"data row3 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row3_col4\" class=\"data row3 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row3_col5\" class=\"data row3 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row3_col6\" class=\"data row3 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row3_col7\" class=\"data row3 col7\" >0.943060</td>\n",
              "      <td id=\"T_f7310_row3_col8\" class=\"data row3 col8\" >0.896027</td>\n",
              "      <td id=\"T_f7310_row3_col9\" class=\"data row3 col9\" >0.918942</td>\n",
              "      <td id=\"T_f7310_row3_col10\" class=\"data row3 col10\" >0.963672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_f7310_row4_col0\" class=\"data row4 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row4_col1\" class=\"data row4 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row4_col2\" class=\"data row4 col2\" >3</td>\n",
              "      <td id=\"T_f7310_row4_col3\" class=\"data row4 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row4_col4\" class=\"data row4 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row4_col5\" class=\"data row4 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row4_col6\" class=\"data row4 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row4_col7\" class=\"data row4 col7\" >0.973497</td>\n",
              "      <td id=\"T_f7310_row4_col8\" class=\"data row4 col8\" >0.976021</td>\n",
              "      <td id=\"T_f7310_row4_col9\" class=\"data row4 col9\" >0.974757</td>\n",
              "      <td id=\"T_f7310_row4_col10\" class=\"data row4 col10\" >0.993321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_f7310_row5_col0\" class=\"data row5 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row5_col1\" class=\"data row5 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row5_col2\" class=\"data row5 col2\" >4</td>\n",
              "      <td id=\"T_f7310_row5_col3\" class=\"data row5 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row5_col4\" class=\"data row5 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row5_col5\" class=\"data row5 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row5_col6\" class=\"data row5 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row5_col7\" class=\"data row5 col7\" >0.623188</td>\n",
              "      <td id=\"T_f7310_row5_col8\" class=\"data row5 col8\" >0.467391</td>\n",
              "      <td id=\"T_f7310_row5_col9\" class=\"data row5 col9\" >0.534161</td>\n",
              "      <td id=\"T_f7310_row5_col10\" class=\"data row5 col10\" >0.649454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_f7310_row6_col0\" class=\"data row6 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row6_col1\" class=\"data row6 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row6_col2\" class=\"data row6 col2\" >5</td>\n",
              "      <td id=\"T_f7310_row6_col3\" class=\"data row6 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row6_col4\" class=\"data row6 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row6_col5\" class=\"data row6 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row6_col6\" class=\"data row6 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row6_col7\" class=\"data row6 col7\" >0.909467</td>\n",
              "      <td id=\"T_f7310_row6_col8\" class=\"data row6 col8\" >0.927215</td>\n",
              "      <td id=\"T_f7310_row6_col9\" class=\"data row6 col9\" >0.918255</td>\n",
              "      <td id=\"T_f7310_row6_col10\" class=\"data row6 col10\" >0.976275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_f7310_row7_col0\" class=\"data row7 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row7_col1\" class=\"data row7 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row7_col2\" class=\"data row7 col2\" >6</td>\n",
              "      <td id=\"T_f7310_row7_col3\" class=\"data row7 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row7_col4\" class=\"data row7 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row7_col5\" class=\"data row7 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row7_col6\" class=\"data row7 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row7_col7\" class=\"data row7 col7\" >0.883929</td>\n",
              "      <td id=\"T_f7310_row7_col8\" class=\"data row7 col8\" >0.818182</td>\n",
              "      <td id=\"T_f7310_row7_col9\" class=\"data row7 col9\" >0.849785</td>\n",
              "      <td id=\"T_f7310_row7_col10\" class=\"data row7 col10\" >0.902983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_f7310_row8_col0\" class=\"data row8 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row8_col1\" class=\"data row8 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row8_col2\" class=\"data row8 col2\" >7</td>\n",
              "      <td id=\"T_f7310_row8_col3\" class=\"data row8 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row8_col4\" class=\"data row8 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row8_col5\" class=\"data row8 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row8_col6\" class=\"data row8 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row8_col7\" class=\"data row8 col7\" >0.883264</td>\n",
              "      <td id=\"T_f7310_row8_col8\" class=\"data row8 col8\" >0.926752</td>\n",
              "      <td id=\"T_f7310_row8_col9\" class=\"data row8 col9\" >0.904486</td>\n",
              "      <td id=\"T_f7310_row8_col10\" class=\"data row8 col10\" >0.953391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_f7310_row9_col0\" class=\"data row9 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row9_col1\" class=\"data row9 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row9_col2\" class=\"data row9 col2\" >8</td>\n",
              "      <td id=\"T_f7310_row9_col3\" class=\"data row9 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row9_col4\" class=\"data row9 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row9_col5\" class=\"data row9 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row9_col6\" class=\"data row9 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row9_col7\" class=\"data row9 col7\" >0.776812</td>\n",
              "      <td id=\"T_f7310_row9_col8\" class=\"data row9 col8\" >0.494465</td>\n",
              "      <td id=\"T_f7310_row9_col9\" class=\"data row9 col9\" >0.604284</td>\n",
              "      <td id=\"T_f7310_row9_col10\" class=\"data row9 col10\" >0.681624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
              "      <td id=\"T_f7310_row10_col0\" class=\"data row10 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row10_col1\" class=\"data row10 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row10_col2\" class=\"data row10 col2\" >9</td>\n",
              "      <td id=\"T_f7310_row10_col3\" class=\"data row10 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row10_col4\" class=\"data row10 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row10_col5\" class=\"data row10 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row10_col6\" class=\"data row10 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row10_col7\" class=\"data row10 col7\" >0.835821</td>\n",
              "      <td id=\"T_f7310_row10_col8\" class=\"data row10 col8\" >0.862866</td>\n",
              "      <td id=\"T_f7310_row10_col9\" class=\"data row10 col9\" >0.849128</td>\n",
              "      <td id=\"T_f7310_row10_col10\" class=\"data row10 col10\" >0.923891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
              "      <td id=\"T_f7310_row11_col0\" class=\"data row11 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row11_col1\" class=\"data row11 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row11_col2\" class=\"data row11 col2\" >10</td>\n",
              "      <td id=\"T_f7310_row11_col3\" class=\"data row11 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row11_col4\" class=\"data row11 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row11_col5\" class=\"data row11 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row11_col6\" class=\"data row11 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row11_col7\" class=\"data row11 col7\" >0.980769</td>\n",
              "      <td id=\"T_f7310_row11_col8\" class=\"data row11 col8\" >0.988920</td>\n",
              "      <td id=\"T_f7310_row11_col9\" class=\"data row11 col9\" >0.984828</td>\n",
              "      <td id=\"T_f7310_row11_col10\" class=\"data row11 col10\" >0.995916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
              "      <td id=\"T_f7310_row12_col0\" class=\"data row12 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row12_col1\" class=\"data row12 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row12_col2\" class=\"data row12 col2\" >11</td>\n",
              "      <td id=\"T_f7310_row12_col3\" class=\"data row12 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row12_col4\" class=\"data row12 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row12_col5\" class=\"data row12 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row12_col6\" class=\"data row12 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row12_col7\" class=\"data row12 col7\" >0.866808</td>\n",
              "      <td id=\"T_f7310_row12_col8\" class=\"data row12 col8\" >0.592486</td>\n",
              "      <td id=\"T_f7310_row12_col9\" class=\"data row12 col9\" >0.703863</td>\n",
              "      <td id=\"T_f7310_row12_col10\" class=\"data row12 col10\" >0.813615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
              "      <td id=\"T_f7310_row13_col0\" class=\"data row13 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row13_col1\" class=\"data row13 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row13_col2\" class=\"data row13 col2\" >12</td>\n",
              "      <td id=\"T_f7310_row13_col3\" class=\"data row13 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row13_col4\" class=\"data row13 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row13_col5\" class=\"data row13 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row13_col6\" class=\"data row13 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row13_col7\" class=\"data row13 col7\" >0.681588</td>\n",
              "      <td id=\"T_f7310_row13_col8\" class=\"data row13 col8\" >0.870478</td>\n",
              "      <td id=\"T_f7310_row13_col9\" class=\"data row13 col9\" >0.764539</td>\n",
              "      <td id=\"T_f7310_row13_col10\" class=\"data row13 col10\" >0.844938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
              "      <td id=\"T_f7310_row14_col0\" class=\"data row14 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row14_col1\" class=\"data row14 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row14_col2\" class=\"data row14 col2\" >13</td>\n",
              "      <td id=\"T_f7310_row14_col3\" class=\"data row14 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row14_col4\" class=\"data row14 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row14_col5\" class=\"data row14 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row14_col6\" class=\"data row14 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row14_col7\" class=\"data row14 col7\" >0.892128</td>\n",
              "      <td id=\"T_f7310_row14_col8\" class=\"data row14 col8\" >0.796875</td>\n",
              "      <td id=\"T_f7310_row14_col9\" class=\"data row14 col9\" >0.841816</td>\n",
              "      <td id=\"T_f7310_row14_col10\" class=\"data row14 col10\" >0.908002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
              "      <td id=\"T_f7310_row15_col0\" class=\"data row15 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row15_col1\" class=\"data row15 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row15_col2\" class=\"data row15 col2\" >14</td>\n",
              "      <td id=\"T_f7310_row15_col3\" class=\"data row15 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row15_col4\" class=\"data row15 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row15_col5\" class=\"data row15 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row15_col6\" class=\"data row15 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row15_col7\" class=\"data row15 col7\" >0.972222</td>\n",
              "      <td id=\"T_f7310_row15_col8\" class=\"data row15 col8\" >0.642202</td>\n",
              "      <td id=\"T_f7310_row15_col9\" class=\"data row15 col9\" >0.773481</td>\n",
              "      <td id=\"T_f7310_row15_col10\" class=\"data row15 col10\" >0.710448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
              "      <td id=\"T_f7310_row16_col0\" class=\"data row16 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row16_col1\" class=\"data row16 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row16_col2\" class=\"data row16 col2\" >15</td>\n",
              "      <td id=\"T_f7310_row16_col3\" class=\"data row16 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row16_col4\" class=\"data row16 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row16_col5\" class=\"data row16 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row16_col6\" class=\"data row16 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row16_col7\" class=\"data row16 col7\" >0.952232</td>\n",
              "      <td id=\"T_f7310_row16_col8\" class=\"data row16 col8\" >0.933231</td>\n",
              "      <td id=\"T_f7310_row16_col9\" class=\"data row16 col9\" >0.942636</td>\n",
              "      <td id=\"T_f7310_row16_col10\" class=\"data row16 col10\" >0.983152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
              "      <td id=\"T_f7310_row17_col0\" class=\"data row17 col0\" >MLPClassifier</td>\n",
              "      <td id=\"T_f7310_row17_col1\" class=\"data row17 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row17_col2\" class=\"data row17 col2\" >16</td>\n",
              "      <td id=\"T_f7310_row17_col3\" class=\"data row17 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row17_col4\" class=\"data row17 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row17_col5\" class=\"data row17 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row17_col6\" class=\"data row17 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row17_col7\" class=\"data row17 col7\" >0.500000</td>\n",
              "      <td id=\"T_f7310_row17_col8\" class=\"data row17 col8\" >0.023810</td>\n",
              "      <td id=\"T_f7310_row17_col9\" class=\"data row17 col9\" >0.045455</td>\n",
              "      <td id=\"T_f7310_row17_col10\" class=\"data row17 col10\" >0.043361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
              "      <td id=\"T_f7310_row18_col0\" class=\"data row18 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row18_col1\" class=\"data row18 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row18_col2\" class=\"data row18 col2\" >Macro Average</td>\n",
              "      <td id=\"T_f7310_row18_col3\" class=\"data row18 col3\" >0.803940</td>\n",
              "      <td id=\"T_f7310_row18_col4\" class=\"data row18 col4\" >0.342006</td>\n",
              "      <td id=\"T_f7310_row18_col5\" class=\"data row18 col5\" >0.430657</td>\n",
              "      <td id=\"T_f7310_row18_col6\" class=\"data row18 col6\" >0.541903</td>\n",
              "      <td id=\"T_f7310_row18_col7\" class=\"data row18 col7\" >nan</td>\n",
              "      <td id=\"T_f7310_row18_col8\" class=\"data row18 col8\" >nan</td>\n",
              "      <td id=\"T_f7310_row18_col9\" class=\"data row18 col9\" >nan</td>\n",
              "      <td id=\"T_f7310_row18_col10\" class=\"data row18 col10\" >nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
              "      <td id=\"T_f7310_row19_col0\" class=\"data row19 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row19_col1\" class=\"data row19 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row19_col2\" class=\"data row19 col2\" >0</td>\n",
              "      <td id=\"T_f7310_row19_col3\" class=\"data row19 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row19_col4\" class=\"data row19 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row19_col5\" class=\"data row19 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row19_col6\" class=\"data row19 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row19_col7\" class=\"data row19 col7\" >0.950617</td>\n",
              "      <td id=\"T_f7310_row19_col8\" class=\"data row19 col8\" >0.214604</td>\n",
              "      <td id=\"T_f7310_row19_col9\" class=\"data row19 col9\" >0.350159</td>\n",
              "      <td id=\"T_f7310_row19_col10\" class=\"data row19 col10\" >0.609094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
              "      <td id=\"T_f7310_row20_col0\" class=\"data row20 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row20_col1\" class=\"data row20 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row20_col2\" class=\"data row20 col2\" >1</td>\n",
              "      <td id=\"T_f7310_row20_col3\" class=\"data row20 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row20_col4\" class=\"data row20 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row20_col5\" class=\"data row20 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row20_col6\" class=\"data row20 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row20_col7\" class=\"data row20 col7\" >0.905039</td>\n",
              "      <td id=\"T_f7310_row20_col8\" class=\"data row20 col8\" >0.230049</td>\n",
              "      <td id=\"T_f7310_row20_col9\" class=\"data row20 col9\" >0.366850</td>\n",
              "      <td id=\"T_f7310_row20_col10\" class=\"data row20 col10\" >0.541895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
              "      <td id=\"T_f7310_row21_col0\" class=\"data row21 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row21_col1\" class=\"data row21 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row21_col2\" class=\"data row21 col2\" >2</td>\n",
              "      <td id=\"T_f7310_row21_col3\" class=\"data row21 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row21_col4\" class=\"data row21 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row21_col5\" class=\"data row21 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row21_col6\" class=\"data row21 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row21_col7\" class=\"data row21 col7\" >0.961194</td>\n",
              "      <td id=\"T_f7310_row21_col8\" class=\"data row21 col8\" >0.272189</td>\n",
              "      <td id=\"T_f7310_row21_col9\" class=\"data row21 col9\" >0.424242</td>\n",
              "      <td id=\"T_f7310_row21_col10\" class=\"data row21 col10\" >0.622439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
              "      <td id=\"T_f7310_row22_col0\" class=\"data row22 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row22_col1\" class=\"data row22 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row22_col2\" class=\"data row22 col2\" >3</td>\n",
              "      <td id=\"T_f7310_row22_col3\" class=\"data row22 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row22_col4\" class=\"data row22 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row22_col5\" class=\"data row22 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row22_col6\" class=\"data row22 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row22_col7\" class=\"data row22 col7\" >0.991883</td>\n",
              "      <td id=\"T_f7310_row22_col8\" class=\"data row22 col8\" >0.395982</td>\n",
              "      <td id=\"T_f7310_row22_col9\" class=\"data row22 col9\" >0.566003</td>\n",
              "      <td id=\"T_f7310_row22_col10\" class=\"data row22 col10\" >0.705831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
              "      <td id=\"T_f7310_row23_col0\" class=\"data row23 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row23_col1\" class=\"data row23 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row23_col2\" class=\"data row23 col2\" >4</td>\n",
              "      <td id=\"T_f7310_row23_col3\" class=\"data row23 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row23_col4\" class=\"data row23 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row23_col5\" class=\"data row23 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row23_col6\" class=\"data row23 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row23_col7\" class=\"data row23 col7\" >0.593458</td>\n",
              "      <td id=\"T_f7310_row23_col8\" class=\"data row23 col8\" >0.172554</td>\n",
              "      <td id=\"T_f7310_row23_col9\" class=\"data row23 col9\" >0.267368</td>\n",
              "      <td id=\"T_f7310_row23_col10\" class=\"data row23 col10\" >0.233083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
              "      <td id=\"T_f7310_row24_col0\" class=\"data row24 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row24_col1\" class=\"data row24 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row24_col2\" class=\"data row24 col2\" >5</td>\n",
              "      <td id=\"T_f7310_row24_col3\" class=\"data row24 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row24_col4\" class=\"data row24 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row24_col5\" class=\"data row24 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row24_col6\" class=\"data row24 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row24_col7\" class=\"data row24 col7\" >0.967797</td>\n",
              "      <td id=\"T_f7310_row24_col8\" class=\"data row24 col8\" >0.301160</td>\n",
              "      <td id=\"T_f7310_row24_col9\" class=\"data row24 col9\" >0.459372</td>\n",
              "      <td id=\"T_f7310_row24_col10\" class=\"data row24 col10\" >0.640369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
              "      <td id=\"T_f7310_row25_col0\" class=\"data row25 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row25_col1\" class=\"data row25 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row25_col2\" class=\"data row25 col2\" >6</td>\n",
              "      <td id=\"T_f7310_row25_col3\" class=\"data row25 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row25_col4\" class=\"data row25 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row25_col5\" class=\"data row25 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row25_col6\" class=\"data row25 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row25_col7\" class=\"data row25 col7\" >0.969697</td>\n",
              "      <td id=\"T_f7310_row25_col8\" class=\"data row25 col8\" >0.528926</td>\n",
              "      <td id=\"T_f7310_row25_col9\" class=\"data row25 col9\" >0.684492</td>\n",
              "      <td id=\"T_f7310_row25_col10\" class=\"data row25 col10\" >0.686630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
              "      <td id=\"T_f7310_row26_col0\" class=\"data row26 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row26_col1\" class=\"data row26 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row26_col2\" class=\"data row26 col2\" >7</td>\n",
              "      <td id=\"T_f7310_row26_col3\" class=\"data row26 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row26_col4\" class=\"data row26 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row26_col5\" class=\"data row26 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row26_col6\" class=\"data row26 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row26_col7\" class=\"data row26 col7\" >0.237366</td>\n",
              "      <td id=\"T_f7310_row26_col8\" class=\"data row26 col8\" >0.977444</td>\n",
              "      <td id=\"T_f7310_row26_col9\" class=\"data row26 col9\" >0.381972</td>\n",
              "      <td id=\"T_f7310_row26_col10\" class=\"data row26 col10\" >0.646695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
              "      <td id=\"T_f7310_row27_col0\" class=\"data row27 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row27_col1\" class=\"data row27 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row27_col2\" class=\"data row27 col2\" >8</td>\n",
              "      <td id=\"T_f7310_row27_col3\" class=\"data row27 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row27_col4\" class=\"data row27 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row27_col5\" class=\"data row27 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row27_col6\" class=\"data row27 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row27_col7\" class=\"data row27 col7\" >0.603175</td>\n",
              "      <td id=\"T_f7310_row27_col8\" class=\"data row27 col8\" >0.210332</td>\n",
              "      <td id=\"T_f7310_row27_col9\" class=\"data row27 col9\" >0.311902</td>\n",
              "      <td id=\"T_f7310_row27_col10\" class=\"data row27 col10\" >0.397799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
              "      <td id=\"T_f7310_row28_col0\" class=\"data row28 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row28_col1\" class=\"data row28 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row28_col2\" class=\"data row28 col2\" >9</td>\n",
              "      <td id=\"T_f7310_row28_col3\" class=\"data row28 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row28_col4\" class=\"data row28 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row28_col5\" class=\"data row28 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row28_col6\" class=\"data row28 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row28_col7\" class=\"data row28 col7\" >0.954955</td>\n",
              "      <td id=\"T_f7310_row28_col8\" class=\"data row28 col8\" >0.326656</td>\n",
              "      <td id=\"T_f7310_row28_col9\" class=\"data row28 col9\" >0.486797</td>\n",
              "      <td id=\"T_f7310_row28_col10\" class=\"data row28 col10\" >0.604698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
              "      <td id=\"T_f7310_row29_col0\" class=\"data row29 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row29_col1\" class=\"data row29 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row29_col2\" class=\"data row29 col2\" >10</td>\n",
              "      <td id=\"T_f7310_row29_col3\" class=\"data row29 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row29_col4\" class=\"data row29 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row29_col5\" class=\"data row29 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row29_col6\" class=\"data row29 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row29_col7\" class=\"data row29 col7\" >0.991877</td>\n",
              "      <td id=\"T_f7310_row29_col8\" class=\"data row29 col8\" >0.507387</td>\n",
              "      <td id=\"T_f7310_row29_col9\" class=\"data row29 col9\" >0.671350</td>\n",
              "      <td id=\"T_f7310_row29_col10\" class=\"data row29 col10\" >0.781896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
              "      <td id=\"T_f7310_row30_col0\" class=\"data row30 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row30_col1\" class=\"data row30 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row30_col2\" class=\"data row30 col2\" >11</td>\n",
              "      <td id=\"T_f7310_row30_col3\" class=\"data row30 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row30_col4\" class=\"data row30 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row30_col5\" class=\"data row30 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row30_col6\" class=\"data row30 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row30_col7\" class=\"data row30 col7\" >0.800745</td>\n",
              "      <td id=\"T_f7310_row30_col8\" class=\"data row30 col8\" >0.207129</td>\n",
              "      <td id=\"T_f7310_row30_col9\" class=\"data row30 col9\" >0.329124</td>\n",
              "      <td id=\"T_f7310_row30_col10\" class=\"data row30 col10\" >0.442606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
              "      <td id=\"T_f7310_row31_col0\" class=\"data row31 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row31_col1\" class=\"data row31 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row31_col2\" class=\"data row31 col2\" >12</td>\n",
              "      <td id=\"T_f7310_row31_col3\" class=\"data row31 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row31_col4\" class=\"data row31 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row31_col5\" class=\"data row31 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row31_col6\" class=\"data row31 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row31_col7\" class=\"data row31 col7\" >0.710448</td>\n",
              "      <td id=\"T_f7310_row31_col8\" class=\"data row31 col8\" >0.538114</td>\n",
              "      <td id=\"T_f7310_row31_col9\" class=\"data row31 col9\" >0.612387</td>\n",
              "      <td id=\"T_f7310_row31_col10\" class=\"data row31 col10\" >0.665463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
              "      <td id=\"T_f7310_row32_col0\" class=\"data row32 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row32_col1\" class=\"data row32 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row32_col2\" class=\"data row32 col2\" >13</td>\n",
              "      <td id=\"T_f7310_row32_col3\" class=\"data row32 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row32_col4\" class=\"data row32 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row32_col5\" class=\"data row32 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row32_col6\" class=\"data row32 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row32_col7\" class=\"data row32 col7\" >0.955752</td>\n",
              "      <td id=\"T_f7310_row32_col8\" class=\"data row32 col8\" >0.281250</td>\n",
              "      <td id=\"T_f7310_row32_col9\" class=\"data row32 col9\" >0.434608</td>\n",
              "      <td id=\"T_f7310_row32_col10\" class=\"data row32 col10\" >0.560531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
              "      <td id=\"T_f7310_row33_col0\" class=\"data row33 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row33_col1\" class=\"data row33 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row33_col2\" class=\"data row33 col2\" >14</td>\n",
              "      <td id=\"T_f7310_row33_col3\" class=\"data row33 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row33_col4\" class=\"data row33 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row33_col5\" class=\"data row33 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row33_col6\" class=\"data row33 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row33_col7\" class=\"data row33 col7\" >0.878049</td>\n",
              "      <td id=\"T_f7310_row33_col8\" class=\"data row33 col8\" >0.330275</td>\n",
              "      <td id=\"T_f7310_row33_col9\" class=\"data row33 col9\" >0.480000</td>\n",
              "      <td id=\"T_f7310_row33_col10\" class=\"data row33 col10\" >0.393412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
              "      <td id=\"T_f7310_row34_col0\" class=\"data row34 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row34_col1\" class=\"data row34 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row34_col2\" class=\"data row34 col2\" >15</td>\n",
              "      <td id=\"T_f7310_row34_col3\" class=\"data row34 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row34_col4\" class=\"data row34 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row34_col5\" class=\"data row34 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row34_col6\" class=\"data row34 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row34_col7\" class=\"data row34 col7\" >0.944920</td>\n",
              "      <td id=\"T_f7310_row34_col8\" class=\"data row34 col8\" >0.296239</td>\n",
              "      <td id=\"T_f7310_row34_col9\" class=\"data row34 col9\" >0.451066</td>\n",
              "      <td id=\"T_f7310_row34_col10\" class=\"data row34 col10\" >0.662463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
              "      <td id=\"T_f7310_row35_col0\" class=\"data row35 col0\" >MostFrequentTagBaseline</td>\n",
              "      <td id=\"T_f7310_row35_col1\" class=\"data row35 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row35_col2\" class=\"data row35 col2\" >16</td>\n",
              "      <td id=\"T_f7310_row35_col3\" class=\"data row35 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row35_col4\" class=\"data row35 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row35_col5\" class=\"data row35 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row35_col6\" class=\"data row35 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row35_col7\" class=\"data row35 col7\" >0.250000</td>\n",
              "      <td id=\"T_f7310_row35_col8\" class=\"data row35 col8\" >0.023810</td>\n",
              "      <td id=\"T_f7310_row35_col9\" class=\"data row35 col9\" >0.043478</td>\n",
              "      <td id=\"T_f7310_row35_col10\" class=\"data row35 col10\" >0.017446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
              "      <td id=\"T_f7310_row36_col0\" class=\"data row36 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row36_col1\" class=\"data row36 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row36_col2\" class=\"data row36 col2\" >Macro Average</td>\n",
              "      <td id=\"T_f7310_row36_col3\" class=\"data row36 col3\" >0.756986</td>\n",
              "      <td id=\"T_f7310_row36_col4\" class=\"data row36 col4\" >0.772419</td>\n",
              "      <td id=\"T_f7310_row36_col5\" class=\"data row36 col5\" >0.734218</td>\n",
              "      <td id=\"T_f7310_row36_col6\" class=\"data row36 col6\" >0.856256</td>\n",
              "      <td id=\"T_f7310_row36_col7\" class=\"data row36 col7\" >nan</td>\n",
              "      <td id=\"T_f7310_row36_col8\" class=\"data row36 col8\" >nan</td>\n",
              "      <td id=\"T_f7310_row36_col9\" class=\"data row36 col9\" >nan</td>\n",
              "      <td id=\"T_f7310_row36_col10\" class=\"data row36 col10\" >nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
              "      <td id=\"T_f7310_row37_col0\" class=\"data row37 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row37_col1\" class=\"data row37 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row37_col2\" class=\"data row37 col2\" >0</td>\n",
              "      <td id=\"T_f7310_row37_col3\" class=\"data row37 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row37_col4\" class=\"data row37 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row37_col5\" class=\"data row37 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row37_col6\" class=\"data row37 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row37_col7\" class=\"data row37 col7\" >0.000000</td>\n",
              "      <td id=\"T_f7310_row37_col8\" class=\"data row37 col8\" >0.000000</td>\n",
              "      <td id=\"T_f7310_row37_col9\" class=\"data row37 col9\" >0.000000</td>\n",
              "      <td id=\"T_f7310_row37_col10\" class=\"data row37 col10\" >0.999767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
              "      <td id=\"T_f7310_row38_col0\" class=\"data row38 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row38_col1\" class=\"data row38 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row38_col2\" class=\"data row38 col2\" >1</td>\n",
              "      <td id=\"T_f7310_row38_col3\" class=\"data row38 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row38_col4\" class=\"data row38 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row38_col5\" class=\"data row38 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row38_col6\" class=\"data row38 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row38_col7\" class=\"data row38 col7\" >0.792340</td>\n",
              "      <td id=\"T_f7310_row38_col8\" class=\"data row38 col8\" >0.928208</td>\n",
              "      <td id=\"T_f7310_row38_col9\" class=\"data row38 col9\" >0.854909</td>\n",
              "      <td id=\"T_f7310_row38_col10\" class=\"data row38 col10\" >0.909961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
              "      <td id=\"T_f7310_row39_col0\" class=\"data row39 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row39_col1\" class=\"data row39 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row39_col2\" class=\"data row39 col2\" >2</td>\n",
              "      <td id=\"T_f7310_row39_col3\" class=\"data row39 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row39_col4\" class=\"data row39 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row39_col5\" class=\"data row39 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row39_col6\" class=\"data row39 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row39_col7\" class=\"data row39 col7\" >0.946860</td>\n",
              "      <td id=\"T_f7310_row39_col8\" class=\"data row39 col8\" >0.965517</td>\n",
              "      <td id=\"T_f7310_row39_col9\" class=\"data row39 col9\" >0.956098</td>\n",
              "      <td id=\"T_f7310_row39_col10\" class=\"data row39 col10\" >0.973605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
              "      <td id=\"T_f7310_row40_col0\" class=\"data row40 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row40_col1\" class=\"data row40 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row40_col2\" class=\"data row40 col2\" >3</td>\n",
              "      <td id=\"T_f7310_row40_col3\" class=\"data row40 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row40_col4\" class=\"data row40 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row40_col5\" class=\"data row40 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row40_col6\" class=\"data row40 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row40_col7\" class=\"data row40 col7\" >0.985046</td>\n",
              "      <td id=\"T_f7310_row40_col8\" class=\"data row40 col8\" >0.981854</td>\n",
              "      <td id=\"T_f7310_row40_col9\" class=\"data row40 col9\" >0.983447</td>\n",
              "      <td id=\"T_f7310_row40_col10\" class=\"data row40 col10\" >0.984585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
              "      <td id=\"T_f7310_row41_col0\" class=\"data row41 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row41_col1\" class=\"data row41 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row41_col2\" class=\"data row41 col2\" >4</td>\n",
              "      <td id=\"T_f7310_row41_col3\" class=\"data row41 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row41_col4\" class=\"data row41 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row41_col5\" class=\"data row41 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row41_col6\" class=\"data row41 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row41_col7\" class=\"data row41 col7\" >0.969697</td>\n",
              "      <td id=\"T_f7310_row41_col8\" class=\"data row41 col8\" >0.793388</td>\n",
              "      <td id=\"T_f7310_row41_col9\" class=\"data row41 col9\" >0.872727</td>\n",
              "      <td id=\"T_f7310_row41_col10\" class=\"data row41 col10\" >0.845547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
              "      <td id=\"T_f7310_row42_col0\" class=\"data row42 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row42_col1\" class=\"data row42 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row42_col2\" class=\"data row42 col2\" >5</td>\n",
              "      <td id=\"T_f7310_row42_col3\" class=\"data row42 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row42_col4\" class=\"data row42 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row42_col5\" class=\"data row42 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row42_col6\" class=\"data row42 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row42_col7\" class=\"data row42 col7\" >0.010480</td>\n",
              "      <td id=\"T_f7310_row42_col8\" class=\"data row42 col8\" >0.979974</td>\n",
              "      <td id=\"T_f7310_row42_col9\" class=\"data row42 col9\" >0.020738</td>\n",
              "      <td id=\"T_f7310_row42_col10\" class=\"data row42 col10\" >0.933735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
              "      <td id=\"T_f7310_row43_col0\" class=\"data row43 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row43_col1\" class=\"data row43 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row43_col2\" class=\"data row43 col2\" >6</td>\n",
              "      <td id=\"T_f7310_row43_col3\" class=\"data row43 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row43_col4\" class=\"data row43 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row43_col5\" class=\"data row43 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row43_col6\" class=\"data row43 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row43_col7\" class=\"data row43 col7\" >0.884718</td>\n",
              "      <td id=\"T_f7310_row43_col8\" class=\"data row43 col8\" >0.859375</td>\n",
              "      <td id=\"T_f7310_row43_col9\" class=\"data row43 col9\" >0.871863</td>\n",
              "      <td id=\"T_f7310_row43_col10\" class=\"data row43 col10\" >0.905067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
              "      <td id=\"T_f7310_row44_col0\" class=\"data row44 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row44_col1\" class=\"data row44 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row44_col2\" class=\"data row44 col2\" >7</td>\n",
              "      <td id=\"T_f7310_row44_col3\" class=\"data row44 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row44_col4\" class=\"data row44 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row44_col5\" class=\"data row44 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row44_col6\" class=\"data row44 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row44_col7\" class=\"data row44 col7\" >0.000000</td>\n",
              "      <td id=\"T_f7310_row44_col8\" class=\"data row44 col8\" >0.000000</td>\n",
              "      <td id=\"T_f7310_row44_col9\" class=\"data row44 col9\" >0.000000</td>\n",
              "      <td id=\"T_f7310_row44_col10\" class=\"data row44 col10\" >0.000769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
              "      <td id=\"T_f7310_row45_col0\" class=\"data row45 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row45_col1\" class=\"data row45 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row45_col2\" class=\"data row45 col2\" >8</td>\n",
              "      <td id=\"T_f7310_row45_col3\" class=\"data row45 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row45_col4\" class=\"data row45 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row45_col5\" class=\"data row45 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row45_col6\" class=\"data row45 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row45_col7\" class=\"data row45 col7\" >0.956346</td>\n",
              "      <td id=\"T_f7310_row45_col8\" class=\"data row45 col8\" >0.907905</td>\n",
              "      <td id=\"T_f7310_row45_col9\" class=\"data row45 col9\" >0.931496</td>\n",
              "      <td id=\"T_f7310_row45_col10\" class=\"data row45 col10\" >0.918364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
              "      <td id=\"T_f7310_row46_col0\" class=\"data row46 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row46_col1\" class=\"data row46 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row46_col2\" class=\"data row46 col2\" >9</td>\n",
              "      <td id=\"T_f7310_row46_col3\" class=\"data row46 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row46_col4\" class=\"data row46 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row46_col5\" class=\"data row46 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row46_col6\" class=\"data row46 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row46_col7\" class=\"data row46 col7\" >0.983380</td>\n",
              "      <td id=\"T_f7310_row46_col8\" class=\"data row46 col8\" >0.983380</td>\n",
              "      <td id=\"T_f7310_row46_col9\" class=\"data row46 col9\" >0.983380</td>\n",
              "      <td id=\"T_f7310_row46_col10\" class=\"data row46 col10\" >0.988621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row47\" class=\"row_heading level0 row47\" >47</th>\n",
              "      <td id=\"T_f7310_row47_col0\" class=\"data row47 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row47_col1\" class=\"data row47 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row47_col2\" class=\"data row47 col2\" >10</td>\n",
              "      <td id=\"T_f7310_row47_col3\" class=\"data row47 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row47_col4\" class=\"data row47 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row47_col5\" class=\"data row47 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row47_col6\" class=\"data row47 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row47_col7\" class=\"data row47 col7\" >0.852459</td>\n",
              "      <td id=\"T_f7310_row47_col8\" class=\"data row47 col8\" >0.479705</td>\n",
              "      <td id=\"T_f7310_row47_col9\" class=\"data row47 col9\" >0.613932</td>\n",
              "      <td id=\"T_f7310_row47_col10\" class=\"data row47 col10\" >0.577122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row48\" class=\"row_heading level0 row48\" >48</th>\n",
              "      <td id=\"T_f7310_row48_col0\" class=\"data row48 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row48_col1\" class=\"data row48 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row48_col2\" class=\"data row48 col2\" >11</td>\n",
              "      <td id=\"T_f7310_row48_col3\" class=\"data row48 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row48_col4\" class=\"data row48 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row48_col5\" class=\"data row48 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row48_col6\" class=\"data row48 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row48_col7\" class=\"data row48 col7\" >0.930335</td>\n",
              "      <td id=\"T_f7310_row48_col8\" class=\"data row48 col8\" >0.891801</td>\n",
              "      <td id=\"T_f7310_row48_col9\" class=\"data row48 col9\" >0.910660</td>\n",
              "      <td id=\"T_f7310_row48_col10\" class=\"data row48 col10\" >0.937003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row49\" class=\"row_heading level0 row49\" >49</th>\n",
              "      <td id=\"T_f7310_row49_col0\" class=\"data row49 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row49_col1\" class=\"data row49 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row49_col2\" class=\"data row49 col2\" >12</td>\n",
              "      <td id=\"T_f7310_row49_col3\" class=\"data row49 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row49_col4\" class=\"data row49 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row49_col5\" class=\"data row49 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row49_col6\" class=\"data row49 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row49_col7\" class=\"data row49 col7\" >0.976266</td>\n",
              "      <td id=\"T_f7310_row49_col8\" class=\"data row49 col8\" >0.950693</td>\n",
              "      <td id=\"T_f7310_row49_col9\" class=\"data row49 col9\" >0.963310</td>\n",
              "      <td id=\"T_f7310_row49_col10\" class=\"data row49 col10\" >0.961714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row50\" class=\"row_heading level0 row50\" >50</th>\n",
              "      <td id=\"T_f7310_row50_col0\" class=\"data row50 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row50_col1\" class=\"data row50 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row50_col2\" class=\"data row50 col2\" >13</td>\n",
              "      <td id=\"T_f7310_row50_col3\" class=\"data row50 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row50_col4\" class=\"data row50 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row50_col5\" class=\"data row50 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row50_col6\" class=\"data row50 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row50_col7\" class=\"data row50 col7\" >0.987805</td>\n",
              "      <td id=\"T_f7310_row50_col8\" class=\"data row50 col8\" >0.990489</td>\n",
              "      <td id=\"T_f7310_row50_col9\" class=\"data row50 col9\" >0.989145</td>\n",
              "      <td id=\"T_f7310_row50_col10\" class=\"data row50 col10\" >0.995577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row51\" class=\"row_heading level0 row51\" >51</th>\n",
              "      <td id=\"T_f7310_row51_col0\" class=\"data row51 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row51_col1\" class=\"data row51 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row51_col2\" class=\"data row51 col2\" >14</td>\n",
              "      <td id=\"T_f7310_row51_col3\" class=\"data row51 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row51_col4\" class=\"data row51 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row51_col5\" class=\"data row51 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row51_col6\" class=\"data row51 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row51_col7\" class=\"data row51 col7\" >0.957121</td>\n",
              "      <td id=\"T_f7310_row51_col8\" class=\"data row51 col8\" >0.988924</td>\n",
              "      <td id=\"T_f7310_row51_col9\" class=\"data row51 col9\" >0.972763</td>\n",
              "      <td id=\"T_f7310_row51_col10\" class=\"data row51 col10\" >0.991895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row52\" class=\"row_heading level0 row52\" >52</th>\n",
              "      <td id=\"T_f7310_row52_col0\" class=\"data row52 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row52_col1\" class=\"data row52 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row52_col2\" class=\"data row52 col2\" >15</td>\n",
              "      <td id=\"T_f7310_row52_col3\" class=\"data row52 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row52_col4\" class=\"data row52 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row52_col5\" class=\"data row52 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row52_col6\" class=\"data row52 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row52_col7\" class=\"data row52 col7\" >0.897982</td>\n",
              "      <td id=\"T_f7310_row52_col8\" class=\"data row52 col8\" >0.892977</td>\n",
              "      <td id=\"T_f7310_row52_col9\" class=\"data row52 col9\" >0.895472</td>\n",
              "      <td id=\"T_f7310_row52_col10\" class=\"data row52 col10\" >0.919696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_f7310_level0_row53\" class=\"row_heading level0 row53\" >53</th>\n",
              "      <td id=\"T_f7310_row53_col0\" class=\"data row53 col0\" >BiLSTM</td>\n",
              "      <td id=\"T_f7310_row53_col1\" class=\"data row53 col1\" >Test</td>\n",
              "      <td id=\"T_f7310_row53_col2\" class=\"data row53 col2\" >16</td>\n",
              "      <td id=\"T_f7310_row53_col3\" class=\"data row53 col3\" >nan</td>\n",
              "      <td id=\"T_f7310_row53_col4\" class=\"data row53 col4\" >nan</td>\n",
              "      <td id=\"T_f7310_row53_col5\" class=\"data row53 col5\" >nan</td>\n",
              "      <td id=\"T_f7310_row53_col6\" class=\"data row53 col6\" >nan</td>\n",
              "      <td id=\"T_f7310_row53_col7\" class=\"data row53 col7\" >0.758073</td>\n",
              "      <td id=\"T_f7310_row53_col8\" class=\"data row53 col8\" >0.667148</td>\n",
              "      <td id=\"T_f7310_row53_col9\" class=\"data row53 col9\" >0.709710</td>\n",
              "      <td id=\"T_f7310_row53_col10\" class=\"data row53 col10\" >0.713327</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7689fc540280>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "classes = np.unique(train_labels_flattened)\n",
        "\n",
        "# Wrap the trained BiLSTM model\n",
        "bilstm_adapter = BiLSTMAdapter(trained_model, device, batch_size=32)\n",
        "\n",
        "# Update the estimators dictionary\n",
        "estimators = {\n",
        "    'MLPClassifier': mlp,\n",
        "    'MostFrequentTagBaseline': baseline_model,\n",
        "    'BiLSTM': bilstm_adapter  # Use the adapter\n",
        "}\n",
        "datasets = {\n",
        "    'Train': ({\n",
        "        'MLPClassifier': train_features,\n",
        "        'MostFrequentTagBaseline': train_features,\n",
        "        'BiLSTM': X_train_padded\n",
        "    }, {\n",
        "        'MLPClassifier': train_labels_flattened,\n",
        "        'MostFrequentTagBaseline': train_labels_flattened,\n",
        "        'BiLSTM': train_labels\n",
        "    }),\n",
        "    'Dev': ({\n",
        "        'MLPClassifier': dev_features,\n",
        "        'MostFrequentTagBaseline': dev_features,\n",
        "        'BiLSTM': X_dev_padded\n",
        "    }, {\n",
        "        'MLPClassifier': dev_labels_flattened,\n",
        "        'MostFrequentTagBaseline': dev_labels_flattened,\n",
        "        'BiLSTM': dev_labels\n",
        "    }),\n",
        "    'Test': ({\n",
        "        'MLPClassifier': test_features,\n",
        "        'MostFrequentTagBaseline': test_features,\n",
        "        'BiLSTM': X_test_padded\n",
        "    }, {\n",
        "        'MLPClassifier': test_labels_flattened,\n",
        "        'MostFrequentTagBaseline': test_labels_flattened,\n",
        "        'BiLSTM': test_labels\n",
        "    })\n",
        "}\n",
        "\n",
        "\n",
        "# Call the Report function\n",
        "Report.display_metrics_scores(\n",
        "    estimators=estimators,\n",
        "    classes=classes,\n",
        "    datasets=datasets,\n",
        "    class_names=tag_to_int\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torch-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}